                                                                     File initialized May 8, 2025: 

This is the master folder for the "forecasts of opportunity using EOFs" research, examining stratosphere-troposphere dynamic coupling. 
There are multiple sub-folders 

As a note, after conducting EOF analysis, the following PCs were selected, accounting for 90% of the variance for each feature:

U-wind:
30 days = 10
20 days = 10
14 days = 10
###########

EHF:
30 days = 45
20 days = 45
14 days = 45
###########

GPH:
30 days = 10
20 days = 10
14 days = 10
####################################################
May 9th

I started the feature selection for each lead time.
So far, it is looking like the PCs do vary based on region, BUT it is not clear yet whether they also vary based on lead time. TBD. 

Need to re-run these, ignoring the first PC (spin-up of the vortex).
###################################################
May 12th

All finished on feature selection for now, me thinks. 
LEADTIME 14: 
##Europe
Index([1, 56, 0, 54, 9, 57, 55, 3, 26, 4], dtype='int64')
Accuracy, Validation: 70.32%
Accuracy, Training: 71.62%
Accuracy, Testing: 70.81%

##Nova
Index([1, 56, 0, 55, 2, 53, 54, 9, 4, 22], dtype='int64')
Accuracy, Validation: 80.75%
Accuracy, Training: 82.18%
Accuracy, Testing: 82.67%

##South
Index([1, 56, 54, 53, 55, 4, 3, 21, 14, 0], dtype='int64')
Accuracy, Validation: 68.74%
Accuracy, Training: 70.58%
Accuracy, Testing: 67.67%
###########################
LEADTIME 20:
##Europe
Index([1, 56, 54, 0, 17, 9, 57, 2, 16, 33], dtype='int64')
Accuracy, Validation: 68.36%
Accuracy, Training: 70.84%
Accuracy, Testing: 72.36%

##Nova
Index([1, 56, 0, 55, 54, 4, 2, 53, 57, 9], dtype='int64')
Accuracy, Validation: 79.51%
Accuracy, Training: 81.91%
Accuracy, Testing: 83.31%

##South
Index([1, 56, 54, 53, 57, 4, 16, 14, 17, 0], dtype='int64')
Accuracy, Validation: 67.90%
Accuracy, Training: 70.11%
Accuracy, Testing: 66.84%
###########################
LEADTIME 30:
Index([1, 56, 54, 49, 53, 2, 19, 55, 0, 18], dtype='int64')
Accuracy, Validation: 68.28%
Accuracy, Training: 70.94%
Accuracy, Testing: 71.34%


##Nova
Index([1, 56, 0, 54, 55, 4, 53, 57, 9, 2], dtype='int64')
Accuracy, Validation: 78.53%
Accuracy, Training: 80.45%
Accuracy, Testing: 80.74%

##South
Index([3, 0, 1, 53, 9, 56, 17, 13, 28, 54], dtype='int64')
Accuracy, Validation: 68.85%
Accuracy, Training: 69.35%
Accuracy, Testing: 67.25%

###################################################
May 27th 

I have returned ... finally. For better or for worse. 
I made a new graphcast job for this with the older version of tensorflow that is compatible with kerastuner. 

I see that I have mostly completed the feature selection, but I am going to go back and make sure this was done correctly. 
I did end up removing the first PC, but that has made the indexing confusing for no good reason lol. 
After this is completed, I will go timestep by timestep to make the figures of PCs. 

Then I will start with training a model for 14 days. Hopefully the hyperparameters found in tuning this model translate to other timesteps. 

(I finished the PC images today so I will start model tomorrow)

Comparing PCs from each area at leadtime
For U-wind at 14 days=
Europe: 3,2,5,6
Nova: 3,2,4,6
SEUS: 3,6,5,2

For U-wind at 20 days=
Europe: 3,2,4
Nova: 3,2,4,6
SEUS: 3,6,2

For U-wind at 30 days=
Europe: 3,4,2
Nova: 3,2,6,4
SEUS: 5,2,3
````````````````````````````````````
For EHF at 14 days=
Europe: 2, 19
Nova: 2, 15
SEUS: 14, 7

For EHF at 20 days=
Europe: 2, 10, 9, 26
Nova: 2
SEUS: 9, 7, 10

For EHF at 30 days=
Europe: 42, 12, 11 
Nova: 2
SEUS: 2, 10, 6, 21
````````````````````````````````````
For GPH at 14 days=
Europe: 5,3,6,4
Nova: 5,4,2,3
SEUS: 5,3,2,4

For GPH at 20 days=
Europe: 5,3,6
Nova: 5,4,3,2,6
SEUS: 5,3,2,4

For GPH at 30 days=
Europe: 5,3,2,4
Nova: 5,3,4,2,6
SEUS: 2,5,3 
###################################################

May 28th

I will start building the LSTM model today. I need to with one region and one lead time, then see if the model for that lead time translates to other regions. 
The question will be whether there is an observable change in the skill of the model between regions & changes to the foreasts of opportunity. 

Let's start with Europe at 14 days because ... of course. 

Models look good. I am going to have to conduct tuning by region, which sucks. 
I am starting with Europe at 14 days in its own model.

Tuning is occurring. I cannot run these at the same time. 
#######################################################

May 30th
I submitted my first manuscript this morning for the ellipse metrics (yay)
I have been otherwise working on the model hyperparameter tuning. 

I cannot tell if one architecture works for all methods. I am going to check using one of the tuned model architectures for the Nova Scotia region. 

The final model hyperparameters that worked for all regions at 14 days leadtime were: 
model = basic_LSTM(numb_int[0],numb_int[1], 32, 4, 24, 0.53, 0.26, 0.0016)
n1 = 32, 
n2 = 4, 
n3 = 24, 
rl1, rl2 = 0.53, 
rl3 = 0.26, 
lr = 0.0016

#######################################################

June 2nd. 

I am trying to ge the model to work a little better. i noticed an overfitting issue with the model during CV, so I am trying to intorduce a component that uses random selection of validation data into hyperparameter tuning in an effort to reduce this. Essentially this is just a loop for random selection+the tuning and then collectively examining the parameters. 

I am also looking into getting SHAP working for these models... obviously my model architecture has created issue because I have not shut off eager execution .... because I have no real reason to. 

######################################################

Jun 3rd. 

After some trouble shooting, it appears that I was incorrectly calculating the baseline for calculating BSS. I have fixed that. My model is NOT underperforming as much as I thought. 

I also tried an alternative method for getting hyperparameters for the model via k-fold cross validation. I may have to look into that for the 100 CVs of the model but I am running that as is for now to see what it does. I am still working with Europe at 14 days lead time. I reduced the forecast window to 10 days rather than 14 to increase my sample size marginally. The leadtime is still 14 days. That did NOT change. 

######################################################

June 4th.
Redoing the Solvers and feature selection.
I added in a new folder to designate ranges for the PCs and temps based on leadtime just to make my life a hair easier in the future. 
#149 for 14
#143 for 20
#133 for 30

for reference for doing the LSTM ... 
I am re-calculating the feature selection for a few reasons. 
1) to get more features. 
2) to get a look at the histograms by region and lead time. 

########################################################

June 5th. 

I finished redoing the feature selection for the updated dates/times last night. 
More or less, what is seen is that the model itself matters a small amount, but the predictands matter more. The region you are forecasting for is really important and the model actually does *better* at predicting here. 

I have a sneaking suspicion that this is being driven by the model itself. 
We already know that the model stuggles with differentiating between cold and neutral, middle ground events ... Maybe the reason why it excels so well in Canada is because there is more distinct positive temperature responses that are easier for the model to identify... so it struggles less with the neutral cases. 

14-days
#EUROPE
#Int64Index([1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13], dtype='int64')
#Accuracy, Validation: 67.84%
#Accuracy, Training: 67.95%
#Accuracy, Testing: 66.99%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 53, 22, 57, 9, 25, 11, 4, 54, 38, 17, 30, 36, 3, 14, 26], dtype='int64')
#Accuracy, Validation: 78.23%
#Accuracy, Training: 79.39%
#Accuracy, Testing: 80.41%

##SE US
#Int64Index([56, 1, 54, 18, 53, 3, 0, 10, 5, 19, 21, 14, 2, 57, 13, 60, 22, 26,25, 11], dtype='int64')
#Accuracy, Validation: 64.35%
#Accuracy, Training: 64.83%
#Accuracy, Testing: 62.45%
############################

20-days 
#EUROPE
#Int64Index([1, 56, 54, 33, 17, 2, 9, 57, 0, 31, 20, 16, 53, 19, 26, 36, 18, 21, 22, 5],dtype='int64')
#Accuracy, Validation: 65.59%
#Accuracy, Training: 67.08%
#Accuracy, Testing: 66.75%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 4, 57, 53, 25, 17, 22, 54, 13, 26, 38, 9, 12, 30, 14, 37],dtype='int64')
#Accuracy, Validation: 77.94%
#Accuracy, Training: 79.20%
#Accuracy, Testing: 80.59%

##SE US
#Int64Index([1, 56, 54, 5, 0, 14, 10, 16, 53, 18, 37, 13, 59, 19, 57, 3, 49, 17,25, 11],dtype='int64')
#Accuracy, Validation: 62.96%
#Accuracy, Training: 63.67%
#Accuracy, Testing: 60.22%
############################

30-days
#EUROPE
#IInt64Index([1, 56, 18, 2, 54, 53, 19, 49, 24, 48, 12, 8, 0, 25, 55, 17, 38, 60, 23, 5],dtype='int64')
#Accuracy, Validation: 64.38%
#Accuracy, Training: 65.52%
#Accuracy, Testing: 65.71%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 57, 4, 54, 22, 6, 53, 26, 25, 30, 9, 18, 13, 17, 3, 38],dtype='int64')
#Accuracy, Validation: 76.72%
#Accuracy, Training: 77.42%
#Accuracy, Testing: 77.12%

##SE US
#Int64Index([0, 3, 9, 53, 56, 13, 1, 49, 25, 17, 54, 7, 18, 19, 4, 59, 57, 5,14, 11],dtype='int64')
#Accuracy, Validation: 63.90%
#Accuracy, Training: 64.13%
#Accuracy, Testing: 61.35%

########################################################

June 10th
I have been working on doing the model testing to make sure that the model is actually, ya know, working correctly. 

########################################################

June 11th
After essentially checking on whether or not the model is struggling with the middle category (it is), I have been able to decide that I need to do the Controlled Abstention network.

########################################################

June 17th

After spending the better part of the last week working on integrating the CAN into my work, as well as tuning it, I have gotten to a point where it is functioning. I am now running CV in my silly manner with the intention of trying to duplicate the images seen in the regular cross entropy loss model that I have traditionally used (ala Tk2 LSTM SS file).

Also, yay, I finally took the time to learn how to initalize a connection here with Github so that my work is recorded in some form or another outside of this cloud environment. 

########################################################

June 18th
Ultimately decided I may not be using the abstention loss 100% correctly; so moving on to trying to use XAI on the cross entropy model 

I did run the CV on the CAN model once more just to see what that outcome looks like.

Started SHAP code.
########################################################

June 20th

Create beeswarm SHAP plots. Added in SHAP to CV loop of normal LSTM model. 
########################################################

June 23rd

Updated SHAP CV to fix plot creation. Made and saved these plots for Europe at 14 days. Will look into feature behaviors during events tomorrow by adding additional components for saving event indexes. 
I can check 90th percentile or 30th,70th, whatever perentile events for confidence and correctness, as well as physical significance. 
########################################################

June 24th

Hottest day of the year. :)
I decided to make the LSTM cross entropy model slightly more shallow by removing one of the LSTM layers ... I am rerunning this with the CV to observe whether this has a noticable impact on the overall performance/probability distributions/etc. 
I have already seen that this iteration has the model implementing early stopping slightly earlier than previous iterations. 

I also started today at plotting cross sections of U wind/PCs on days where they are "largest" ... if that makes sense. 
So, like, looking through time I choose the date where PC 3 of zonal-mean wind is greatest and plot the composite of the EOF on that day + the composite of the actual zonal-mean wind.

What is needed then is the following:
- import of PC and EOF daily values 
- import of daily values for the features themselves
- save indices where certain PCs are greatest (and orthogonal PCs are low amplitude)

Got it, kinda. Without the low amplitude things ... I will send these images to Andrea to see what her opinion is. 
########################################################

June 25th

I am going to go on a very brief sidequest at the gym-job tonight that involves going back to doing a Fully-connected NN using the cross sections of all three features as inputs and creating a merged model... Kinda like what I did for AGU. I realize I never actually tried this and I am curious what results it will yield, especially if I look at different forecast areas. These results are more interpretable by using LRP. 
This will all be in the "sidequest" folder. If this, somehow, yields a better model ... lol. Bye, Felicia. 

Back to the work at hand here, though. I have gotten to a point of slight confusion with the physical interpretability of the EOF-model.
It helps, I guess, to describe what I am doing or what I have done/the purpose it all serves ...

So essentially EOFs provide orthogonal eigenmode decompositions of a full field of my desired features. By using the timeseries of selections of these for within the model, I am essentially providing the model with the degree of the "presence" of a certain mode of variability and whether it is in its positive phase or negative phase. The model is not necessarily learning any specific dynamic patterns, but rather is able to observe which of the eigenmodes is most consistently related to cold anomalies or warm anomalies. 
Apparently, SHAP value sign is about model input direction rather than feature sign itself. 

The convention in which I need to think about this from here and forward is the following:
Red dots (big + PC) with Positive SHAP = Positive EOF phase and warm anomaly
Blue dots (big - PC) with Positive SHAP = Negative EOF phase and warm anomaly
Red dots with Negative SHAP = Positive EOF phase and cold anomaly
Blue dots with Negative SHAP = Negative EOF phase and cold anomaly
The question becomes whether this physically makes sense. 
I may benefit from plotting the top 5 of these EOF modes in +/- forms and sending these to Andrea for help in assessing whether they make physical sense for what they may be implying for the connections of these PCs to temperature outcomes. 

Composites during confident and correct events are only really useful for assessing whether there is a known feature of opportunity.
It will also probably benefit me to get the SHAP timeseries plots done so that I can modify the composite to represent the most "impactful" forecast days from the 14-day timeseries. 

Okay, so what needs to be done here? 
1) I am going to go on my little sidequest, idc...

2) Spend a good moment doing interpretation of the signs + SHAP values for this model. What corresponds?
3) Plot the EOF modes. Do these physically make sense with respect to the outcomes of SHAP?
4) Get timeseries of SHAP. What timestep is most robust for the model?
5) What do the composites of the three features show during instances of heightened forecast skill? Does this correspond to what we already know from literature or is there something new? 
If all this is good ... move on to next region. 

Will address the CAN at some point. 

(Little sidequest is interesting... almost too interesting lol.)

########################################################

June 26th/27th

Work on side quest. The goal here was to see whether there is any worth-while information beinf provided by this model compared to what is shown in my "lessons learned" paper on stratospheric polar vortex ellipse diagnostics. The model is not bad by any means. There is an appearance for forecasts of opportunity, up to 80% accuracy over the models 65%. The really important thing here are a few distinctions from the RF model code. I want to see a few things.

1) that the probability distributions of this model are improved, meaning that the task and data I am using here providde some kind of overall model improvement beyond what is shown in the "lessons learned."
2) that there is new information from this model that is not observed from the "lessons learned" models. It may be comparable in accuracy, but what do the results actually tell us? I think that the big thing here is that I should be looking to observe that THIS FCNN model provides increased accuracy at other lead times and for different regions. That will be the added benefit of this thought experiment. 

I am going to go ahead and try to do the "lessons learned" model again but with the addition of a few things: the probability distribution plots and attempts for other regions. 

I am going to look into quickly moving that data and files here so I can test and log on git. 

_________________
Okay, I learned a few things here. 
1) I may need to reconsider how I am choosing my output data as a target. I think the choice of calculating Oct-Mar 2m temp anomalies and then predicting relative to that is having a large impact on my results. I think I may want to just use the Nov-Mar 2m temp anomalies... But I am waiting to hear if logically this makes sense. Afterall, I am NOT forecasting for Oct at all, so why include it in the anomaly? 
2) I forgot to normalize for the FCNN. I am rerunning this now, this model will have to change. 

I think, really, what I need to consider is whether I need to change my output data. If I do, I will need to redo a LOT of stuff. 

##############################
July 7th, 2025

I am back from vacation. Hi. 
I needed to make a new environment woohoo.

Needed to install:
!pip install innvestigate
!pip install investigate

!pip install matplotlib
!pip install netCDF4
!pip install cartopy
!pip install shap
!pip install keras_tuner

Coming back to where I left it with the FCNN model. The sad and crushing reality is that I did, in fact, forget to normalize that data. AND it is now not tuning correctly. Shame. But I do not think that will really matter. I guess that was the point of this being a sidequest, it was just to test things for fun. 
I will be turning back to the physical interpretability of the EOF model. Because in the end, we want these more complex 2-dimensional features to provide more information.  

The real "use" here is trying to prove that, in some way, we gain some skill from looking at these more complex EOF features when compared to the results from the LL paper.
I think I am going to need to write out specifically what I want to check/compare for the purpose of my dissertation/expanding on the LL paper for the dissertation. (On paper ... not here. Just in case. lol.)

##############################
July 8th, 2025

I will be taking a brief hiatus here to work on LL images. 
Also, git didn't track my changes from last night. Boo. I figured out how to fix that. 

##############################
July 9th, 2025

Back again. Adding in saving indices for confident predictions so that I can analyze what is physically occurring. 
I will have to come back tomorrow and work on the code to do so... + the SHAP density timeseries plot so that I can look at the most IMPORTANT timestep for these physical plots. 

I also think I keep forgetting to do the global config in git so that my changes are tracked. 

##############################
July 10th, 2025

I plotted distribution SHAP plots for the 14 day EOF Europe model for this comparison. Later at the gym I will work on the composite plots for comparison. 
