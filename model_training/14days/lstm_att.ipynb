{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f609b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5069bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    follows these equations:\n",
    "    \n",
    "    (1) u_t = tanh(W h_t + b)\n",
    "    (2) \\alpha_t = \\frac{exp(u^T u)}{\\sum_t(exp(u_t^T u))}, this is the attention weight\n",
    "    (3) v_t = \\alpha_t * h_t, v in time t\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "                'W_regularizer': self.W_regularizer,\n",
    "                'u_regularizer': self.u_regularizer,\n",
    "                'b_regularizer': self.b_regularizer,\n",
    "                'W_constraint': self.W_constraint,\n",
    "                'u_constraint': self.u_constraint,\n",
    "                'b_constraint': self.b_constraint,\n",
    "                'bias': self.bias,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_W'.format(self.name),\n",
    "                                regularizer=self.W_regularizer,\n",
    "                                constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer='zero',\n",
    "                                    name='{}_b'.format(self.name),\n",
    "                                    regularizer=self.b_regularizer,\n",
    "                                    constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                initializer=self.init,\n",
    "                                name='{}_u'.format(self.name),\n",
    "                                regularizer=self.u_regularizer,\n",
    "                                constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero and this results in NaN's. \n",
    "        # Should add a small epsilon as the workaround\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        \n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "    \n",
    "class Addition(Layer):\n",
    "    \"\"\"\n",
    "    This layer is supposed to add of all activation weight.\n",
    "    We split this from AttentionWithContext to help us getting the activation weights\n",
    "    follows this equation:\n",
    "    (1) v = \\sum_t(\\alpha_t * h_t)\n",
    "    \n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(ntimestep, nfeature, **kwargs):\n",
    "    \n",
    "    regval = kwargs.get('regval', [0.0, 0.0])\n",
    "    recurr_dropout = kwargs.get('recurr_dropout', 0)\n",
    "    numlayer = kwargs.get('layers',2)\n",
    "    neurons = kwargs.get('neurons',[10,10])\n",
    "    numdenselayer = kwargs.get('dense_layers',1)\n",
    "    denseneurons = kwargs.get('dense_neurons',[10])\n",
    "    out_neurons = kwargs.get('out_neurons', 2)\n",
    "\n",
    "    input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "    layer1 = layers.LSTM(neurons[0], return_sequences=True, recurrent_dropout=recurr_dropout, kernel_regularizer=regularizers.l2(regval[0]))(input_tensor)\n",
    "    if numlayer >=2:\n",
    "        print('layer ' + str(numlayer))\n",
    "        for i in range(1, numlayer):\n",
    "            layer1 = layers.LSTM(neurons[i], return_sequences=True, recurrent_dropout=recurr_dropout, kernel_regularizer=regularizers.l2(regval[i]))(layer1)\n",
    "\n",
    "    layer1, alfa = AttentionWithContext()(layer1)\n",
    "    layer1 = Addition()(layer1)\n",
    "    \n",
    "    if numdenselayer > 0:\n",
    "        for i in range(numdenselayer):\n",
    "            layer1 = layers.Dense(denseneurons[i], activation=\"relu\")(layer1)\n",
    "            \n",
    "    output_tensor = layers.Dense(out_neurons, activation='softmax')(layer1)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.summary()                        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X, train_y, val_X, val_y, lr, callbacks_path, epochs, batch_size, class_weight):\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "    decay_rate = lr / epochs\n",
    "    momentum = 0.9\n",
    "    #opt = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum, nesterov=True)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    callbacks_list = [\n",
    "        #keras.callbacks.EarlyStopping(\n",
    "        #    monitor='val_loss',\n",
    "        #    patience=20,\n",
    "        #),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=callbacks_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "        )\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        train_X, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(val_X, val_y),\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=False,\n",
    "        shuffle=True,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
