{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d356f55",
   "metadata": {},
   "source": [
    "### LSTM Model Loop with Skill Score calculation\n",
    "\n",
    "File initially created 5/30/2025. \n",
    "\n",
    "I will start with a 14 day model. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "247331a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86acb365",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:31:44.928459: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-03 23:31:44.959565: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-03 23:31:44.959588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-03 23:31:44.960531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-03 23:31:44.965571: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 23:31:46.311884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 979 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2d:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "##import cell makes its appearance once again...\n",
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "#from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "tf.device('/physical_device:GPU:0')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#import investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5edbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from EOF_def import EOF_def\n",
    "\n",
    "from StatScores import BSS\n",
    "from StatScores import RAS_two\n",
    "from StatScores import PAS_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a3e5814",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in solvers for PC analysis to get the PCs for the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f12f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (8370, 10)\n",
      "<class 'numpy.ndarray'> (8370, 45)\n",
      "<class 'numpy.ndarray'> (8370, 10)\n"
     ]
    }
   ],
   "source": [
    "# load input solvers\n",
    "infile = open(\"../../eof_analysis/solvers/Usolver_14.p\",\"rb\",)\n",
    "Usolver = pickle.load(infile)  ##pv on an isentropic surface, 350\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_analysis/solvers/EHFsolver_14.p\",\"rb\",)\n",
    "EHFsolver = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_analysis/solvers/GPHsolver_14.p\",\"rb\",)\n",
    "GPHsolver = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()\n",
    "\n",
    "## U\n",
    "U_EOF_nw, U_EOF_nw2d, U_eigenv, U_VarEx, U_PC = EOF_def(Usolver, 10)\n",
    "## EHF\n",
    "EHF_EOF_nw, EHF_EOF_nw2d, EHF_eigenv, EHF_VarEx, EHF_PC = EOF_def(EHFsolver, 45)\n",
    "## GPH\n",
    "GPH_EOF_nw, GPH_EOF_nw2d, GPH_eigenv, GPH_VarEx, GPH_PC = EOF_def(GPHsolver, 10)\n",
    "\n",
    "##remove PC 1\n",
    "U_PC = U_PC[:, 1:]\n",
    "EHF_PC = EHF_PC[:, 1:]\n",
    "GPH_PC = GPH_PC[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78965e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create one array of PCs\n",
    "inputvar = np.concatenate((U_PC,EHF_PC,GPH_PC),axis=1) \n",
    "\n",
    "##make pandas dataframe for RF\n",
    "input = pd.DataFrame(inputvar)\n",
    "#input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83a2cbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pd datafram of selected feature columns\n",
    "#for all temp regions\n",
    "sele_ind_eur = input[[1, 56, 0, 54, 9, 57, 55, 3, 26, 4]]\n",
    "sele_ind_nova = input[[1, 56, 0, 55, 2, 53, 54, 9, 4, 22]]\n",
    "sele_ind_seus = input[[1, 56, 54, 53, 55, 4, 3, 21, 14, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f1d6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load output data\n",
    "infile = open(\"../../eof_data/eur_anomtemps.p\",\"rb\",)\n",
    "output_eur = pickle.load(infile) \n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_data/nova_anomtemps.p\",\"rb\",)\n",
    "output_nova = pickle.load(infile) \n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_data/seus_anomtemps.p\",\"rb\",)\n",
    "output_seus = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12f0f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load climo data\n",
    "infile = open(\"../../eof_data/eur_climoprob.p\",\"rb\",)\n",
    "climo_eur = pickle.load(infile) \n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_data/nova_climoprob.p\",\"rb\",)\n",
    "climo_nova = pickle.load(infile) \n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../eof_data/seus_climoprob.p\",\"rb\",)\n",
    "climo_seus = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a73fa",
   "metadata": {},
   "source": [
    "### This is where you specify the region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302eb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##CHANGE THIS\n",
    "sele_ind_df = sele_ind_eur\n",
    "output = output_eur\n",
    "climo = climo_eur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5dc6c2",
   "metadata": {},
   "source": [
    "Now reformat these arrays to match timeseries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f475b4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (62, 135, 10)\n",
      "climo:  (62, 139, 2)\n",
      "output:  (62, 139)\n"
     ]
    }
   ],
   "source": [
    "#turn input array into (# samples, 14 days, 10 features)\n",
    "sele_ind_data = sele_ind_df.values\n",
    "sele_ind_data1 = sele_ind_data.reshape(62,135,10)\n",
    "\n",
    "##need to change this based on lag \n",
    "## 14-days = [:, 47:]\n",
    "## 20-days = [:, 53:]\n",
    "## 30-days = [:, 63:]\n",
    "\n",
    "temp = output.reshape(62, 182)\n",
    "temp = temp[:, 43:]\n",
    "\n",
    "climo = climo.reshape(62, 182,2)\n",
    "climo = climo[:, 43:, :]\n",
    "#climo = climo.reshape(8370,2)\n",
    "\n",
    "print(\"input: \",sele_ind_data1.shape)\n",
    "print(\"climo: \",climo.shape)\n",
    "print(\"output: \",temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba25061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timeseries data arrays for PCs \n",
    "new_input =  np.empty((62,126,10,10))\n",
    "new_output = np.empty((62,126))\n",
    "new_climo = np.empty((62,126,2))\n",
    "\n",
    "for i in range(0,62):\n",
    "    for j in range(0,126):\n",
    "        #print(j)\n",
    "        new_input[i,j,:,:] = sele_ind_data1[i,j:j+10,:]\n",
    "        new_output[i,j] = temp[i,j]\n",
    "        new_climo[i,j] = climo[i,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc743f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (7812, 10, 10)\n",
      "climo:  (7812, 2)\n",
      "output:  (7812,)\n"
     ]
    }
   ],
   "source": [
    "new_input = np.array(new_input)\n",
    "new_input = new_input.reshape((62*126),10,10)\n",
    "\n",
    "new_climo = np.array(new_climo)\n",
    "new_climo = new_climo.reshape((62*126),2)\n",
    "\n",
    "new_output = np.array(new_output)\n",
    "new_output = new_output.flatten()\n",
    "\n",
    "print(\"input: \",new_input.shape)\n",
    "print(\"climo: \",new_climo.shape)\n",
    "print(\"output: \",new_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6af7a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing and validation size:  1367\n",
      "training size:  6445\n"
     ]
    }
   ],
   "source": [
    "frac_ind = round(len(new_output)*0.175)\n",
    "frac_end = round(len(new_output)-frac_ind)\n",
    "\n",
    "print(\"testing and validation size: \",frac_ind)\n",
    "print(\"training size: \",frac_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24a10015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7812, 2)\n"
     ]
    }
   ],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all = np.copy(new_input)\n",
    "Y_all = np.copy(new_output)\n",
    "\n",
    "##training data partition out\n",
    "X_tri = X_all[:frac_end,:]\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes = X_all[frac_end:,:]\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "#Convert the Y array into a categorical array. This means we will create one-hot vector labels for all of the inputs.\n",
    "# The one-hot vectors have an index for each possible output category (two in our case)\n",
    "# A \"1\" is put in the index corresponding to the category to which the sample belongs\n",
    "Y_all = keras.utils.to_categorical(Y_all)\n",
    "Y_tri = keras.utils.to_categorical(Y_tri)\n",
    "Y_tes= keras.utils.to_categorical(Y_tes)\n",
    "\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b6ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(X_all)) or np.any(np.isinf(X_all)):\n",
    "    print(\"NaN or Inf values found in X_all!\")\n",
    "\n",
    "if np.any(np.isnan(Y_all)) or np.any(np.isinf(Y_all)):\n",
    "    print(\"NaN or Inf values found in Y_all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca77ec5",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd046533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of lstm input:  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "##number of input nodes\n",
    "numb_int = X_all.shape[1:]\n",
    "#this defines the 2-d shape used by the lstm model.\n",
    "print(\"shape of lstm input: \",numb_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cbca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#CHANGE BASED ON NUMBER OF FOLDS FOR TIMESERIES SPLIT\n",
    "\n",
    "repeat = 5\n",
    "split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bccaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "##class weight creator for the instance where now I have a four dimensional output array \n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    Y_reshaped = Y.reshape(-1, Y.shape[-1])\n",
    "    weights = np.max(np.sum(Y_reshaped, axis=0)) / np.sum(Y_reshaped, axis=0)\n",
    "    for i in range(Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5e87b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##definition statement for ACC\n",
    "def calculate_accuracy(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28aac975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loss and ACC lists, only relevant w/o early stopping\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "cat_acc = []\n",
    "val_acc = [] \n",
    "\n",
    "##BSS Arrays, all of the skill scores have 200 rows\n",
    "#because that is how many cross-validations I will do for the model\n",
    "BSS_all= np.empty((repeat*split,))\n",
    "BSS_val= np.empty((repeat*split,))\n",
    "BSS_train= np.empty((repeat*split,))\n",
    "BSS_test= np.empty((repeat*split,))\n",
    "BSS_90= np.empty((repeat*split,))\n",
    "\n",
    "##RAS and PAS Arrays\n",
    "Prec_all= np.empty((repeat*split,2))\n",
    "Rec_all= np.empty((repeat*split,2))\n",
    "\n",
    "Prec_val= np.empty((repeat*split,2))\n",
    "Rec_val= np.empty((repeat*split,2))\n",
    "\n",
    "Prec_train= np.empty((repeat*split,2))\n",
    "Rec_train= np.empty((repeat*split,2))\n",
    "\n",
    "Prec_test= np.empty((repeat*split,2))\n",
    "Rec_test= np.empty((repeat*split,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04a58843",
   "metadata": {},
   "outputs": [],
   "source": [
    "##empty lists for ACC scores \n",
    "##Testing Accuracy Arrays\n",
    "fulltest_acc = []\n",
    "test90_acc = []\n",
    "\n",
    "##Training Accuracy Arrays\n",
    "fulltrain_acc = []\n",
    "train90_acc = []\n",
    "\n",
    "##Validation Accuracy Arrays\n",
    "fullval_acc = []\n",
    "val90_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b402c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = []\n",
    "p_train = []\n",
    "p_val = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd11d23",
   "metadata": {},
   "source": [
    "### Model Architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "521bea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "shuffle = True\n",
    "verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "429b478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINITION OF MODEL ITSELF\n",
    "def basic_LSTM(ntimestep, nfeature, n1, n2, n3, rl1, rl3, lr):  \n",
    "    input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "    \n",
    "    layer1 = layers.RNN(\n",
    "        layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= rl1)),\n",
    "        return_sequences=True)(input_tensor)\n",
    "    #layer1, alfa = AttentionWithContext()(layer1)\n",
    "    #layer1 = Addition()(layer1)\n",
    "    \n",
    "    layer2 = layers.RNN(\n",
    "        layers.LSTMCell(n2, activation='tanh', use_bias=True,\n",
    "                          kernel_initializer='glorot_uniform',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= rl1)))(layer1)\n",
    "    \n",
    "    layer3 = layers.Dense(n3, activation='relu',use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2=rl3))(layer2)\n",
    "\n",
    "    output_tensor = layers.Dense(2, activation='softmax',)(layer3)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
    "    #decay_rate = lr / epochs\n",
    "    #momentum = 0.9\n",
    "\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=[keras.metrics.categorical_accuracy],)\n",
    "                            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaf07b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Early Stopping Condition, if desired\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=4,restore_best_weights=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f419da01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPEAT  1\n",
      "Fold  1  in Repeat  1\n",
      "Epoch 1/30\n",
      "17/17 [==============================] - 2s 29ms/step - loss: 22.7712 - categorical_accuracy: 0.3070 - val_loss: 20.9501 - val_categorical_accuracy: 0.3547\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 19.9091 - categorical_accuracy: 0.3581 - val_loss: 18.3043 - val_categorical_accuracy: 0.4479\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 17.4400 - categorical_accuracy: 0.5191 - val_loss: 16.0331 - val_categorical_accuracy: 0.6304\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 15.3213 - categorical_accuracy: 0.6102 - val_loss: 14.0909 - val_categorical_accuracy: 0.6536\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 13.5064 - categorical_accuracy: 0.6549 - val_loss: 12.4259 - val_categorical_accuracy: 0.6629\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 11.9505 - categorical_accuracy: 0.6986 - val_loss: 11.0066 - val_categorical_accuracy: 0.6629\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 10.6191 - categorical_accuracy: 0.6949 - val_loss: 9.7845 - val_categorical_accuracy: 0.6601\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 9.4721 - categorical_accuracy: 0.7228 - val_loss: 8.7323 - val_categorical_accuracy: 0.6592\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 8.4883 - categorical_accuracy: 0.7135 - val_loss: 7.8325 - val_categorical_accuracy: 0.6713\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 7.6411 - categorical_accuracy: 0.7247 - val_loss: 7.0527 - val_categorical_accuracy: 0.6713\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 6.9174 - categorical_accuracy: 0.7237 - val_loss: 6.3683 - val_categorical_accuracy: 0.6713\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 6.2864 - categorical_accuracy: 0.7284 - val_loss: 5.8109 - val_categorical_accuracy: 0.6695\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 5.7395 - categorical_accuracy: 0.7321 - val_loss: 5.2970 - val_categorical_accuracy: 0.6704\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 5.2681 - categorical_accuracy: 0.7340 - val_loss: 4.8555 - val_categorical_accuracy: 0.6695\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 4.8585 - categorical_accuracy: 0.7442 - val_loss: 4.4933 - val_categorical_accuracy: 0.6657\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 4.5007 - categorical_accuracy: 0.7321 - val_loss: 4.1421 - val_categorical_accuracy: 0.6704\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 4.1910 - categorical_accuracy: 0.7423 - val_loss: 3.8627 - val_categorical_accuracy: 0.6685\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 3.9203 - categorical_accuracy: 0.7312 - val_loss: 3.6045 - val_categorical_accuracy: 0.6657\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 3.6768 - categorical_accuracy: 0.7340 - val_loss: 3.3889 - val_categorical_accuracy: 0.6648\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 3.4581 - categorical_accuracy: 0.7414 - val_loss: 3.1746 - val_categorical_accuracy: 0.6676\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 3.2692 - categorical_accuracy: 0.7395 - val_loss: 2.9994 - val_categorical_accuracy: 0.6713\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 3.0942 - categorical_accuracy: 0.7395 - val_loss: 2.8316 - val_categorical_accuracy: 0.6657\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.9445 - categorical_accuracy: 0.7507 - val_loss: 2.6977 - val_categorical_accuracy: 0.6685\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.8077 - categorical_accuracy: 0.7284 - val_loss: 2.5615 - val_categorical_accuracy: 0.6657\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.6783 - categorical_accuracy: 0.7442 - val_loss: 2.4532 - val_categorical_accuracy: 0.6667\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.5629 - categorical_accuracy: 0.7358 - val_loss: 2.3247 - val_categorical_accuracy: 0.6750\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 2.4594 - categorical_accuracy: 0.7423 - val_loss: 2.2349 - val_categorical_accuracy: 0.6695\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 2.3638 - categorical_accuracy: 0.7330 - val_loss: 2.1407 - val_categorical_accuracy: 0.6695\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.2870 - categorical_accuracy: 0.7237 - val_loss: 2.0402 - val_categorical_accuracy: 0.6713\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 2.2024 - categorical_accuracy: 0.7405 - val_loss: 2.0075 - val_categorical_accuracy: 0.6732\n",
      "245/245 [==============================] - 1s 3ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "43/43 [==============================] - 0s 3ms/step\n",
      "Fold  2  in Repeat  1\n",
      "Epoch 1/30\n",
      "34/34 [==============================] - 2s 20ms/step - loss: 21.2512 - categorical_accuracy: 0.6584 - val_loss: 18.1969 - val_categorical_accuracy: 0.6499\n",
      "Epoch 2/30\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 16.3053 - categorical_accuracy: 0.6598 - val_loss: 13.9988 - val_categorical_accuracy: 0.6601\n",
      "Epoch 3/30\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 12.6673 - categorical_accuracy: 0.6691 - val_loss: 10.9119 - val_categorical_accuracy: 0.6704\n",
      "Epoch 4/30\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 9.9916 - categorical_accuracy: 0.6668 - val_loss: 8.6399 - val_categorical_accuracy: 0.6769\n",
      "Epoch 5/30\n",
      "15/34 [============>.................] - ETA: 0s - loss: 8.4686 - categorical_accuracy: 0.6760"
     ]
    }
   ],
   "source": [
    "fold = 0\n",
    "for r in range(repeat):\n",
    "    print(\"REPEAT \",str(r+1))\n",
    "    tscv = TimeSeriesSplit(n_splits=split)\n",
    "    \n",
    "    for l, (train_index, val_index) in enumerate(tscv.split(X_tri)):\n",
    "        print(\"Fold \",str(l+1),\" in Repeat \",str(r+1))\n",
    "        \n",
    "        X_tr = X_tri[train_index]\n",
    "        Y_tr = Y_tri[train_index]\n",
    "        \n",
    "        X_val = X_tri[val_index]\n",
    "        Y_val = Y_tri[val_index]\n",
    "        \n",
    "        climo_train = new_climo[train_index]\n",
    "        climo_val = new_climo[val_index]\n",
    "        climo_test = new_climo[frac_end:]\n",
    "\n",
    "        # Get class weights\n",
    "        class_weight = class_weight_creator(Y_tr)\n",
    "        #initalize model\n",
    "        model = basic_LSTM(numb_int[0],numb_int[1], 64, 16, 16, 0.208, 0.177, 0.0006466)\n",
    "        model.fit(X_tr, Y_tr, validation_data=(X_val, Y_val),\n",
    "                  batch_size=batch_size, epochs=epochs, shuffle=shuffle,\n",
    "                  class_weight = class_weight, callbacks=[early_stopping])\n",
    "        \n",
    "        train_loss.append(model.history.history['loss'])\n",
    "        val_loss.append(model.history.history['val_loss'])\n",
    "        cat_acc.append(model.history.history['categorical_accuracy'])\n",
    "        val_acc.append(model.history.history['val_categorical_accuracy'])\n",
    "    \n",
    "        pred = model.predict(X_all)\n",
    "        pred_val = model.predict(X_val)\n",
    "        pred_train = model.predict(X_tr)\n",
    "        pred_test = model.predict(X_tes)\n",
    "\n",
    "        p_train.extend(pred_train)\n",
    "        p_val.extend(pred_val)\n",
    "        p_test.extend(pred_test)\n",
    "        ########################################################################################################\n",
    "        ##STATISTICS CALCULATINNG\n",
    "        pred_class = []\n",
    "        predval_class = []\n",
    "        predtr_class = []\n",
    "        predtest_class = []\n",
    "\n",
    "        ##BRIER SKILL SCORE\n",
    "        BSS_all[fold] = BSS(Y_all,pred)\n",
    "        BSS_val[fold] = BSS(Y_val,pred_val)\n",
    "        BSS_train[fold] = BSS(Y_tr,pred_train)\n",
    "        BSS_test[fold] = BSS(Y_tes,pred_test) \n",
    "    \n",
    "        ##RECALL ACCURACY SCORE    \n",
    "        RAS_two(fold, Rec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Rec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Rec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Rec_test, Y_tes, pred_test, predtest_class)\n",
    "        ##PRECISION ACCURACY SCORE     \n",
    "        PAS_two(fold, Prec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Prec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Prec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Prec_test, Y_tes, pred_test, predtest_class)\n",
    "        ########################################################################################################\n",
    "        def evaluate_90th(preds, labels, climo, acc_list, bss_arr):\n",
    "            q90 = np.percentile(preds, 90, axis=0)\n",
    "            high_conf_idx = [i for i, row in enumerate(preds) if (row[0] > q90[0]) or (row[1] > q90[1])]\n",
    "            preds_90 = preds[high_conf_idx]\n",
    "            labels_90 = labels[high_conf_idx]\n",
    "            climo_90 = climo[high_conf_idx]\n",
    "\n",
    "            acc_list.append(calculate_accuracy(labels_90, preds_90, threshold=0.5))\n",
    "            bss_arr[fold] = BSS(labels_90, preds_90)\n",
    "                \n",
    "        evaluate_90th(pred_train, Y_tr, climo_train, train90_acc, BSS_90)\n",
    "        fulltrain_acc.append(calculate_accuracy(Y_tr, pred_train, threshold=0.5))\n",
    "\n",
    "        evaluate_90th(pred_val, Y_val, climo_val, val90_acc, BSS_90)\n",
    "        fullval_acc.append(calculate_accuracy(Y_val, pred_val, threshold=0.5))\n",
    "\n",
    "        evaluate_90th(pred_test, Y_tes, climo_test, test90_acc, BSS_90)\n",
    "        fulltest_acc.append(calculate_accuracy(Y_tes, pred_test, threshold=0.5))\n",
    "\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892842a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Climatology predictions: use class frequencies\n",
    "climo_probs = np.mean(Y_tr, axis=0)  # e.g., [0.85, 0.15] for binary\n",
    "climo_pred = np.tile(climo_probs, (Y_tr.shape[0], 1))\n",
    "\n",
    "bss = BSS(Y_tr, model.predict(X_tr))  # Your model\n",
    "bss_climo = BSS(Y_tr, climo_pred)     # Should be 0 by definition\n",
    "\n",
    "print(f\"Train BSS: {bss:.4f} | Should be > 0 if model learns anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.3, 0.3,13)\n",
    "counts, edges, = np.histogram(BSS_90[:], bins=bins)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(7,7))\n",
    "\n",
    "ax1.bar(edges[:-1], counts, width=np.diff(edges), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_xlabel('BSS')\n",
    "ax1.set_ylabel('Number of Models')\n",
    "ax1.set_title('BSS for 90th Percentile Predictions')\n",
    "#plt.savefig(\"BSS90.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb894546",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.3, 0.3,13)\n",
    "counts1, edges1, = np.histogram(BSS_train[:], bins=bins)\n",
    "counts2, edges2, = np.histogram(BSS_val[:], bins=bins)\n",
    "counts3, edges3, = np.histogram(BSS_test[:], bins=bins)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "ax1.bar(edges1[:-1], counts1, width=np.diff(edges1), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_xlabel('BSS Train')\n",
    "ax1.set_ylabel('Number of Models')\n",
    "\n",
    "ax2.bar(edges2[:-1], counts2, width=np.diff(edges2), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax2.set_xlabel('BSS Val')\n",
    "ax2.set_ylabel('Number of Models')\n",
    "\n",
    "ax3.bar(edges3[:-1], counts3, width=np.diff(edges3), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax3.set_xlabel('BSS Test')\n",
    "ax3.set_ylabel('Number of Models')\n",
    "\n",
    "plt.suptitle('BSS for Predictions Across Datasets')\n",
    "#plt.savefig(\"BSS90.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70677885",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cc7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.array(p_test)\n",
    "p_test = p_test.reshape(((27340,2)))\n",
    "\n",
    "p_train = np.array(p_train)\n",
    "p_train = p_train.reshape(((101560,2)))\n",
    "\n",
    "p_val= np.array(p_val)\n",
    "p_val = p_val.reshape(((27340,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baed0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.2, 0.8, 37)  #10 bins from 0.4 to 1\n",
    "##bin the probabilities\n",
    "counts_neg_test, edges_neg_test = np.histogram(p_test[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(p_test[:,1], bins=bins)\n",
    "\n",
    "counts_neg_train, edges_neg_train = np.histogram(p_train[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(p_train[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(p_val[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(p_val[:,1], bins=bins)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative Training Predictions', fontsize=13)\n",
    "ax1.set_xlabel('Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive Training Predictions', fontsize=13)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax3.set_title('Negative Validation Predictions', fontsize=13)\n",
    "ax3.set_xlabel('Probability')\n",
    "ax3.set_ylabel('Count')\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax4.set_title('Positive Validation Predictions', fontsize=13)\n",
    "ax4.set_xlabel('Probability')\n",
    "ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax5.set_title('Negative Testing Predictions', fontsize=13)\n",
    "ax5.set_xlabel('Probability')\n",
    "ax5.set_ylabel('Count')\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax6.set_title('Positive Testing Predictions', fontsize=13)\n",
    "ax6.set_xlabel('Probability')\n",
    "ax6.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"probability_distrib.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426050c",
   "metadata": {},
   "source": [
    "### Model Curves. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce073e92",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "tloss = np.mean(train_loss, axis=0)\n",
    "vloss = np.mean(val_loss, axis=0)\n",
    "    \n",
    "cacc= np.mean(cat_acc, axis=0)\n",
    "vacc= np.mean(val_acc, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f11d3d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(tloss, label='Training loss')\n",
    "ax1.plot(vloss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cacc, label='Training ACC')\n",
    "ax2.plot(vacc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend\n",
    "\n",
    "#plt.savefig(\"EUR_ACC_14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2e1ca",
   "metadata": {},
   "source": [
    "### Skill Score Distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6313dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure(figsize=(7.5,7.5))\n",
    "font = 10\n",
    "#loop through each member\n",
    "ax2 = plt.subplot(2,1,1)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ticks = ['Full', 'Train', 'Val', 'Test'] #set tick numbers for dataset\n",
    "colors = ['lightcyan','peachpuff']\n",
    "ind = [2, 4, 6, 8]  # the x locations for the groups\n",
    "w = 0.25 #box-plot width\n",
    "labels = ['- Anom', '+ Anom '] #labels of quantiles\n",
    "\n",
    "\n",
    "##begin to go plot by plot ...\n",
    "#each plot has a separate plot function for each lead time. In these, the plots get each quantile plotted. \n",
    "ax2.set_title(\"Stat Scores for Predicting Europe Temp Anoms at 14-days Lead with Top 10 PCs\",fontsize = 11) \n",
    "\n",
    "##repeat the process\n",
    "a2_0 = ax2.boxplot([Rec_all[:,0],Rec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a2_5 = ax2.boxplot([Rec_train[:,0],Rec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a2_10 = ax2.boxplot([Rec_val[:,0],Rec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a2_14 = ax2.boxplot([Rec_test[:,0],Rec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax2.axhline(0.5, c='k', ls ='-.')\n",
    "ax2.set_xticks(ind, ticks)\n",
    "for bplot in (a2_0, a2_5, a2_10, a2_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a2_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax2.set_title('RAS for Temperature Region',fontsize = 14)\n",
    "ax2.set_ylim(-0.1,1.1)\n",
    "ax2.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax2.set_ylabel('RAS',fontsize = font)\n",
    "#ax2.set_xlabel('Data Set',fontsize = 14)\n",
    "#ax2.set_ylabel('Brier Skill Score')\n",
    "ax2.legend(loc = 'lower right', fontsize = 8)\n",
    "ax2.set_aspect('auto') ;\n",
    "\n",
    "a3_0 = ax3.boxplot([Prec_all[:,0],Prec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a3_5 = ax3.boxplot([Prec_train[:,0],Prec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a3_10 = ax3.boxplot([Prec_val[:,0],Prec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a3_14 = ax3.boxplot([Prec_test[:,0],Prec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax3.axhline(0.5, c='k', ls ='-.')\n",
    "ax3.set_xticks(ind, ticks)\n",
    "for bplot in (a3_0, a3_5, a3_10, a3_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a3_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax3.set_title('RAS Well')\n",
    "#ax3.set_title('PAS for Temperature Region',fontsize = 14)\n",
    "ax3.set_ylim(-0.1,1.1)\n",
    "ax3.set_ylabel('PAS',fontsize = font)\n",
    "ax3.set_xlabel('Data Set',fontsize = font)\n",
    "#ax3.set_ylabel('Recall Accuracy Score',fontsize = 14)\n",
    "ax3.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax3.legend(loc = 'lower right', fontsize = 8)\n",
    "ax3.set_aspect('auto') ;\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "#plt.savefig(\"EUR_StatScore_14.png\", bbox_inches='tight',dpi =150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b9c1d",
   "metadata": {},
   "source": [
    "### Percentile ACC Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d238a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.4, 1,13)  # 10 bins from 0.4 to 1\n",
    "##bin the testing 90th percentile accuracy data\n",
    "counts90, edges90 = np.histogram(test90_acc, bins=bins)\n",
    "countsfull, edgesfull = np.histogram(fulltest_acc, bins=bins)\n",
    "# Plot the full dataset\n",
    "#offset = 0.02  # Adjust this value if needed for better visibility\n",
    "plt.bar(edgesfull[:-1], countsfull, width=np.diff(edgesfull), \n",
    "        edgecolor='black', alpha=0.5, label='All Predictions', align='edge', color='blue')\n",
    "# Plot the 90th percentile\n",
    "plt.bar(edges90[:-1], counts90, width=np.diff(edges90), \n",
    "        edgecolor='black', alpha=0.5, label='>90th Percentile Predictions', align='edge', color = 'lightblue')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Number of Models')\n",
    "plt.legend()\n",
    "plt.title('Testing Prediction Accuracy Across 20 Models for Europe Temps')\n",
    "#plt.savefig(\"EUR14_ACCtesting.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
