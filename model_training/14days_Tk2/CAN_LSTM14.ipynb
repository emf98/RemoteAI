{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a0924f",
   "metadata": {},
   "source": [
    "### First file attempt at implementing Controlled Abstention Loss to my LSTM model.\n",
    "\n",
    "File initially created 6/11/2025. \n",
    "\n",
    "I will start with a 14 day model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e550a25",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-18 15:21:23.208400: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-18 15:21:23.239858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-18 15:21:23.239879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-18 15:21:23.240818: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-18 15:21:23.245910: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-18 15:21:24.591794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 990 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2d:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "##import cell makes its appearance once again...\n",
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "#from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "tf.device('/physical_device:GPU:0')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#import investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9697d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed212276",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This cell includes the new class and def functions from Barnes and Barnes 2021 relevant for creating a CAN. \n",
    "import metrics\n",
    "import abstentionloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4a8a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load input solvers\n",
    "infile = open(\"../../reduced_data/PCs/U_14.p\",\"rb\",)\n",
    "U_PC = pickle.load(infile)  ##pv on an isentropic surface, 350\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../reduced_data/PCs/EHF_14.p\",\"rb\",)\n",
    "EHF_PC = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../reduced_data/PCs/GPH_14.p\",\"rb\",)\n",
    "GPH_PC = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec37fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove PC 1\n",
    "U_PC = U_PC[:, 1:]\n",
    "EHF_PC = EHF_PC[:, 1:]\n",
    "GPH_PC = GPH_PC[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20e2f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_PC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92496ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load output data\n",
    "infile = open(\"../../reduced_data/temps/eur_14.p\",\"rb\",)\n",
    "output = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb562ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2dd8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create one array of PCs\n",
    "inputvar = np.concatenate((U_PC,EHF_PC,GPH_PC),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f61c9242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.791039</td>\n",
       "      <td>-1.727149</td>\n",
       "      <td>1.481165</td>\n",
       "      <td>-1.684007</td>\n",
       "      <td>0.329935</td>\n",
       "      <td>1.656387</td>\n",
       "      <td>-0.211460</td>\n",
       "      <td>1.453994</td>\n",
       "      <td>0.837609</td>\n",
       "      <td>0.195595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172799</td>\n",
       "      <td>1.564267</td>\n",
       "      <td>0.584225</td>\n",
       "      <td>-0.036226</td>\n",
       "      <td>0.870949</td>\n",
       "      <td>0.482494</td>\n",
       "      <td>0.197297</td>\n",
       "      <td>0.694999</td>\n",
       "      <td>-0.395420</td>\n",
       "      <td>-1.128227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.821552</td>\n",
       "      <td>-1.842438</td>\n",
       "      <td>1.416795</td>\n",
       "      <td>-1.725300</td>\n",
       "      <td>-0.155038</td>\n",
       "      <td>2.059544</td>\n",
       "      <td>0.081825</td>\n",
       "      <td>1.419269</td>\n",
       "      <td>0.147530</td>\n",
       "      <td>0.234372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.436175</td>\n",
       "      <td>1.686121</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.081523</td>\n",
       "      <td>0.887701</td>\n",
       "      <td>0.621646</td>\n",
       "      <td>0.388313</td>\n",
       "      <td>0.968120</td>\n",
       "      <td>-0.156091</td>\n",
       "      <td>-1.539525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.888491</td>\n",
       "      <td>-1.958864</td>\n",
       "      <td>1.626327</td>\n",
       "      <td>-1.405833</td>\n",
       "      <td>-0.030527</td>\n",
       "      <td>1.728328</td>\n",
       "      <td>0.536282</td>\n",
       "      <td>0.792897</td>\n",
       "      <td>-1.002197</td>\n",
       "      <td>0.268253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480251</td>\n",
       "      <td>1.535731</td>\n",
       "      <td>1.169094</td>\n",
       "      <td>0.118023</td>\n",
       "      <td>0.852785</td>\n",
       "      <td>0.684715</td>\n",
       "      <td>0.481477</td>\n",
       "      <td>0.804988</td>\n",
       "      <td>0.313862</td>\n",
       "      <td>-1.607084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.883083</td>\n",
       "      <td>-1.953125</td>\n",
       "      <td>1.878820</td>\n",
       "      <td>-0.966085</td>\n",
       "      <td>0.620002</td>\n",
       "      <td>1.275088</td>\n",
       "      <td>0.679312</td>\n",
       "      <td>1.133707</td>\n",
       "      <td>-0.758163</td>\n",
       "      <td>0.231512</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.880181</td>\n",
       "      <td>1.224010</td>\n",
       "      <td>1.292322</td>\n",
       "      <td>-0.031788</td>\n",
       "      <td>0.794642</td>\n",
       "      <td>0.805009</td>\n",
       "      <td>0.279682</td>\n",
       "      <td>0.627936</td>\n",
       "      <td>0.181622</td>\n",
       "      <td>-0.832041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.799762</td>\n",
       "      <td>-1.914598</td>\n",
       "      <td>1.794036</td>\n",
       "      <td>-0.578514</td>\n",
       "      <td>1.147515</td>\n",
       "      <td>0.969589</td>\n",
       "      <td>0.594968</td>\n",
       "      <td>1.942123</td>\n",
       "      <td>-0.141386</td>\n",
       "      <td>0.126057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636755</td>\n",
       "      <td>0.993273</td>\n",
       "      <td>1.244127</td>\n",
       "      <td>-0.186460</td>\n",
       "      <td>0.879478</td>\n",
       "      <td>1.131472</td>\n",
       "      <td>0.345422</td>\n",
       "      <td>0.342985</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>0.147311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9233</th>\n",
       "      <td>0.739117</td>\n",
       "      <td>0.344881</td>\n",
       "      <td>-1.039941</td>\n",
       "      <td>1.907440</td>\n",
       "      <td>-0.463413</td>\n",
       "      <td>1.368651</td>\n",
       "      <td>1.955351</td>\n",
       "      <td>-1.003432</td>\n",
       "      <td>-1.171885</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667313</td>\n",
       "      <td>0.159405</td>\n",
       "      <td>1.871430</td>\n",
       "      <td>1.036272</td>\n",
       "      <td>-0.931433</td>\n",
       "      <td>-0.594206</td>\n",
       "      <td>1.060108</td>\n",
       "      <td>0.489458</td>\n",
       "      <td>0.632401</td>\n",
       "      <td>1.478229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9234</th>\n",
       "      <td>0.708051</td>\n",
       "      <td>0.610486</td>\n",
       "      <td>-0.735913</td>\n",
       "      <td>1.811668</td>\n",
       "      <td>1.011904</td>\n",
       "      <td>0.905560</td>\n",
       "      <td>2.091698</td>\n",
       "      <td>-0.923073</td>\n",
       "      <td>-0.182186</td>\n",
       "      <td>0.100459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.677382</td>\n",
       "      <td>0.532989</td>\n",
       "      <td>1.945315</td>\n",
       "      <td>1.078350</td>\n",
       "      <td>-0.883960</td>\n",
       "      <td>-0.255287</td>\n",
       "      <td>1.054779</td>\n",
       "      <td>0.616516</td>\n",
       "      <td>0.933306</td>\n",
       "      <td>0.541883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>0.612392</td>\n",
       "      <td>0.813433</td>\n",
       "      <td>-0.491388</td>\n",
       "      <td>1.058656</td>\n",
       "      <td>2.158425</td>\n",
       "      <td>0.531745</td>\n",
       "      <td>2.458308</td>\n",
       "      <td>-0.798223</td>\n",
       "      <td>1.449475</td>\n",
       "      <td>0.138989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355340</td>\n",
       "      <td>1.044255</td>\n",
       "      <td>1.904598</td>\n",
       "      <td>0.936523</td>\n",
       "      <td>-1.013030</td>\n",
       "      <td>-0.031661</td>\n",
       "      <td>1.253826</td>\n",
       "      <td>0.375767</td>\n",
       "      <td>0.830058</td>\n",
       "      <td>-0.412274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>0.179745</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>-0.592462</td>\n",
       "      <td>-0.004740</td>\n",
       "      <td>2.092489</td>\n",
       "      <td>-0.249596</td>\n",
       "      <td>1.450899</td>\n",
       "      <td>-0.071044</td>\n",
       "      <td>2.113109</td>\n",
       "      <td>0.101430</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112332</td>\n",
       "      <td>1.295592</td>\n",
       "      <td>1.709359</td>\n",
       "      <td>0.596902</td>\n",
       "      <td>-1.099654</td>\n",
       "      <td>-0.021087</td>\n",
       "      <td>1.714048</td>\n",
       "      <td>0.279814</td>\n",
       "      <td>0.649183</td>\n",
       "      <td>-0.773683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>-0.171662</td>\n",
       "      <td>-0.141076</td>\n",
       "      <td>-0.920245</td>\n",
       "      <td>-0.283947</td>\n",
       "      <td>0.960485</td>\n",
       "      <td>-0.176553</td>\n",
       "      <td>0.644020</td>\n",
       "      <td>0.822152</td>\n",
       "      <td>2.009169</td>\n",
       "      <td>0.178951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076846</td>\n",
       "      <td>1.124540</td>\n",
       "      <td>1.518323</td>\n",
       "      <td>0.307120</td>\n",
       "      <td>-0.841747</td>\n",
       "      <td>-0.354376</td>\n",
       "      <td>1.852638</td>\n",
       "      <td>0.494849</td>\n",
       "      <td>0.489356</td>\n",
       "      <td>-0.597684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9238 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.791039 -1.727149  1.481165 -1.684007  0.329935  1.656387 -0.211460   \n",
       "1     0.821552 -1.842438  1.416795 -1.725300 -0.155038  2.059544  0.081825   \n",
       "2     0.888491 -1.958864  1.626327 -1.405833 -0.030527  1.728328  0.536282   \n",
       "3     0.883083 -1.953125  1.878820 -0.966085  0.620002  1.275088  0.679312   \n",
       "4     0.799762 -1.914598  1.794036 -0.578514  1.147515  0.969589  0.594968   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9233  0.739117  0.344881 -1.039941  1.907440 -0.463413  1.368651  1.955351   \n",
       "9234  0.708051  0.610486 -0.735913  1.811668  1.011904  0.905560  2.091698   \n",
       "9235  0.612392  0.813433 -0.491388  1.058656  2.158425  0.531745  2.458308   \n",
       "9236  0.179745  0.300776 -0.592462 -0.004740  2.092489 -0.249596  1.450899   \n",
       "9237 -0.171662 -0.141076 -0.920245 -0.283947  0.960485 -0.176553  0.644020   \n",
       "\n",
       "            7         8         9   ...        52        53        54  \\\n",
       "0     1.453994  0.837609  0.195595  ...  0.172799  1.564267  0.584225   \n",
       "1     1.419269  0.147530  0.234372  ... -0.436175  1.686121  0.834128   \n",
       "2     0.792897 -1.002197  0.268253  ...  0.480251  1.535731  1.169094   \n",
       "3     1.133707 -0.758163  0.231512  ... -0.880181  1.224010  1.292322   \n",
       "4     1.942123 -0.141386  0.126057  ... -0.636755  0.993273  1.244127   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9233 -1.003432 -1.171885 -0.032577  ...  0.667313  0.159405  1.871430   \n",
       "9234 -0.923073 -0.182186  0.100459  ...  0.677382  0.532989  1.945315   \n",
       "9235 -0.798223  1.449475  0.138989  ...  0.355340  1.044255  1.904598   \n",
       "9236 -0.071044  2.113109  0.101430  ... -0.112332  1.295592  1.709359   \n",
       "9237  0.822152  2.009169  0.178951  ...  0.076846  1.124540  1.518323   \n",
       "\n",
       "            55        56        57        58        59        60        61  \n",
       "0    -0.036226  0.870949  0.482494  0.197297  0.694999 -0.395420 -1.128227  \n",
       "1     0.081523  0.887701  0.621646  0.388313  0.968120 -0.156091 -1.539525  \n",
       "2     0.118023  0.852785  0.684715  0.481477  0.804988  0.313862 -1.607084  \n",
       "3    -0.031788  0.794642  0.805009  0.279682  0.627936  0.181622 -0.832041  \n",
       "4    -0.186460  0.879478  1.131472  0.345422  0.342985  0.390064  0.147311  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9233  1.036272 -0.931433 -0.594206  1.060108  0.489458  0.632401  1.478229  \n",
       "9234  1.078350 -0.883960 -0.255287  1.054779  0.616516  0.933306  0.541883  \n",
       "9235  0.936523 -1.013030 -0.031661  1.253826  0.375767  0.830058 -0.412274  \n",
       "9236  0.596902 -1.099654 -0.021087  1.714048  0.279814  0.649183 -0.773683  \n",
       "9237  0.307120 -0.841747 -0.354376  1.852638  0.494849  0.489356 -0.597684  \n",
       "\n",
       "[9238 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##make pandas dataframe for RF\n",
    "input = pd.DataFrame(inputvar)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "096a2a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 135)\n"
     ]
    }
   ],
   "source": [
    "##need to change this based on lag \n",
    "## 14-days = [:, 10:] 10 day window rather than 14\n",
    "## 20-days = [:, 16:]\n",
    "## 30-days = [:, 26:]\n",
    "\n",
    "# 139\n",
    "# 133\n",
    "# 123\n",
    "\n",
    "## 14-days = [:, 14:] 14 day window\n",
    "## 20-days = [:, 20:]\n",
    "## 30-days = [:, 30:]\n",
    "\n",
    "# 135\n",
    "# 129\n",
    "# 119\n",
    "\n",
    "lead = 135\n",
    "\n",
    "temp = output.reshape(62, 149)\n",
    "temp = temp[:, 14:]\n",
    "print(temp.shape)\n",
    "\n",
    "temp_flat = temp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a78d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected indices\n",
    "#EUROPE\n",
    "#input[[1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13]]\n",
    "\n",
    "##CANADA\n",
    "#input[[56, 1, 0, 55, 2, 53, 22, 57, 9, 25, 11, 4, 54, 38, 17, 30, 36, 3, 14, 26]]\n",
    "\n",
    "##SE US\n",
    "#input[[56, 1, 54, 18, 53, 3, 0, 10, 5, 19, 21, 14, 2, 57, 13, 60, 22, 26,25, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6cb3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pd datafram of selected feature columns.\n",
    "sele_ind_df = input[[1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13]]\n",
    "\n",
    "n_features = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "404ca951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>56</th>\n",
       "      <th>0</th>\n",
       "      <th>54</th>\n",
       "      <th>9</th>\n",
       "      <th>57</th>\n",
       "      <th>2</th>\n",
       "      <th>31</th>\n",
       "      <th>26</th>\n",
       "      <th>14</th>\n",
       "      <th>20</th>\n",
       "      <th>6</th>\n",
       "      <th>11</th>\n",
       "      <th>38</th>\n",
       "      <th>21</th>\n",
       "      <th>18</th>\n",
       "      <th>15</th>\n",
       "      <th>22</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.727149</td>\n",
       "      <td>0.870949</td>\n",
       "      <td>0.791039</td>\n",
       "      <td>0.584225</td>\n",
       "      <td>0.195595</td>\n",
       "      <td>0.482494</td>\n",
       "      <td>1.481165</td>\n",
       "      <td>-0.840203</td>\n",
       "      <td>-0.190102</td>\n",
       "      <td>0.057985</td>\n",
       "      <td>-0.773881</td>\n",
       "      <td>-0.211460</td>\n",
       "      <td>-0.443124</td>\n",
       "      <td>-0.130601</td>\n",
       "      <td>0.202801</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.187630</td>\n",
       "      <td>-0.032653</td>\n",
       "      <td>-0.019193</td>\n",
       "      <td>0.376977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.842438</td>\n",
       "      <td>0.887701</td>\n",
       "      <td>0.821552</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.234372</td>\n",
       "      <td>0.621646</td>\n",
       "      <td>1.416795</td>\n",
       "      <td>0.061981</td>\n",
       "      <td>0.047160</td>\n",
       "      <td>0.038753</td>\n",
       "      <td>-0.324938</td>\n",
       "      <td>0.081825</td>\n",
       "      <td>-0.373201</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.467937</td>\n",
       "      <td>-0.598541</td>\n",
       "      <td>-0.270955</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>-0.049397</td>\n",
       "      <td>0.386002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.958864</td>\n",
       "      <td>0.852785</td>\n",
       "      <td>0.888491</td>\n",
       "      <td>1.169094</td>\n",
       "      <td>0.268253</td>\n",
       "      <td>0.684715</td>\n",
       "      <td>1.626327</td>\n",
       "      <td>0.298515</td>\n",
       "      <td>-0.432859</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>-0.306709</td>\n",
       "      <td>0.536282</td>\n",
       "      <td>-0.265632</td>\n",
       "      <td>0.342450</td>\n",
       "      <td>0.137449</td>\n",
       "      <td>-0.366524</td>\n",
       "      <td>-0.253419</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.024585</td>\n",
       "      <td>0.317177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.953125</td>\n",
       "      <td>0.794642</td>\n",
       "      <td>0.883083</td>\n",
       "      <td>1.292322</td>\n",
       "      <td>0.231512</td>\n",
       "      <td>0.805009</td>\n",
       "      <td>1.878820</td>\n",
       "      <td>0.660462</td>\n",
       "      <td>-0.094418</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.679312</td>\n",
       "      <td>-0.231309</td>\n",
       "      <td>0.463792</td>\n",
       "      <td>0.329118</td>\n",
       "      <td>-0.513304</td>\n",
       "      <td>-0.236251</td>\n",
       "      <td>-0.282491</td>\n",
       "      <td>0.136366</td>\n",
       "      <td>0.271059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.914598</td>\n",
       "      <td>0.879478</td>\n",
       "      <td>0.799762</td>\n",
       "      <td>1.244127</td>\n",
       "      <td>0.126057</td>\n",
       "      <td>1.131472</td>\n",
       "      <td>1.794036</td>\n",
       "      <td>0.447582</td>\n",
       "      <td>-0.096808</td>\n",
       "      <td>-0.171956</td>\n",
       "      <td>0.254848</td>\n",
       "      <td>0.594968</td>\n",
       "      <td>-0.224626</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>-0.106193</td>\n",
       "      <td>-0.430760</td>\n",
       "      <td>-0.154636</td>\n",
       "      <td>-0.166812</td>\n",
       "      <td>0.192114</td>\n",
       "      <td>0.207816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9233</th>\n",
       "      <td>0.344881</td>\n",
       "      <td>-0.931433</td>\n",
       "      <td>0.739117</td>\n",
       "      <td>1.871430</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>-0.594206</td>\n",
       "      <td>-1.039941</td>\n",
       "      <td>-0.509073</td>\n",
       "      <td>-0.257606</td>\n",
       "      <td>-0.355304</td>\n",
       "      <td>-0.337902</td>\n",
       "      <td>1.955351</td>\n",
       "      <td>0.696732</td>\n",
       "      <td>-0.993664</td>\n",
       "      <td>-0.459643</td>\n",
       "      <td>-0.780476</td>\n",
       "      <td>0.085641</td>\n",
       "      <td>0.103523</td>\n",
       "      <td>1.020420</td>\n",
       "      <td>-0.827579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9234</th>\n",
       "      <td>0.610486</td>\n",
       "      <td>-0.883960</td>\n",
       "      <td>0.708051</td>\n",
       "      <td>1.945315</td>\n",
       "      <td>0.100459</td>\n",
       "      <td>-0.255287</td>\n",
       "      <td>-0.735913</td>\n",
       "      <td>-0.484002</td>\n",
       "      <td>0.104951</td>\n",
       "      <td>-0.734529</td>\n",
       "      <td>-0.387806</td>\n",
       "      <td>2.091698</td>\n",
       "      <td>0.528793</td>\n",
       "      <td>-0.712909</td>\n",
       "      <td>0.187002</td>\n",
       "      <td>-0.496463</td>\n",
       "      <td>-0.079321</td>\n",
       "      <td>0.201060</td>\n",
       "      <td>0.726277</td>\n",
       "      <td>-0.338853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9235</th>\n",
       "      <td>0.813433</td>\n",
       "      <td>-1.013030</td>\n",
       "      <td>0.612392</td>\n",
       "      <td>1.904598</td>\n",
       "      <td>0.138989</td>\n",
       "      <td>-0.031661</td>\n",
       "      <td>-0.491388</td>\n",
       "      <td>-0.967253</td>\n",
       "      <td>-0.227728</td>\n",
       "      <td>-0.252240</td>\n",
       "      <td>-0.915382</td>\n",
       "      <td>2.458308</td>\n",
       "      <td>0.276310</td>\n",
       "      <td>0.378725</td>\n",
       "      <td>0.115268</td>\n",
       "      <td>-0.112195</td>\n",
       "      <td>0.129463</td>\n",
       "      <td>-0.507578</td>\n",
       "      <td>0.114448</td>\n",
       "      <td>0.129632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9236</th>\n",
       "      <td>0.300776</td>\n",
       "      <td>-1.099654</td>\n",
       "      <td>0.179745</td>\n",
       "      <td>1.709359</td>\n",
       "      <td>0.101430</td>\n",
       "      <td>-0.021087</td>\n",
       "      <td>-0.592462</td>\n",
       "      <td>-1.556661</td>\n",
       "      <td>-0.148204</td>\n",
       "      <td>0.084979</td>\n",
       "      <td>-2.017825</td>\n",
       "      <td>1.450899</td>\n",
       "      <td>0.127205</td>\n",
       "      <td>0.196277</td>\n",
       "      <td>0.509032</td>\n",
       "      <td>0.827147</td>\n",
       "      <td>0.147253</td>\n",
       "      <td>-0.492190</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.058502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>-0.141076</td>\n",
       "      <td>-0.841747</td>\n",
       "      <td>-0.171662</td>\n",
       "      <td>1.518323</td>\n",
       "      <td>0.178951</td>\n",
       "      <td>-0.354376</td>\n",
       "      <td>-0.920245</td>\n",
       "      <td>-0.362053</td>\n",
       "      <td>0.017998</td>\n",
       "      <td>-0.229013</td>\n",
       "      <td>-2.150286</td>\n",
       "      <td>0.644020</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>0.382890</td>\n",
       "      <td>1.099226</td>\n",
       "      <td>0.677530</td>\n",
       "      <td>-0.054276</td>\n",
       "      <td>-0.581448</td>\n",
       "      <td>0.019743</td>\n",
       "      <td>-0.055451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9238 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            1         56        0         54        9         57        2   \\\n",
       "0    -1.727149  0.870949  0.791039  0.584225  0.195595  0.482494  1.481165   \n",
       "1    -1.842438  0.887701  0.821552  0.834128  0.234372  0.621646  1.416795   \n",
       "2    -1.958864  0.852785  0.888491  1.169094  0.268253  0.684715  1.626327   \n",
       "3    -1.953125  0.794642  0.883083  1.292322  0.231512  0.805009  1.878820   \n",
       "4    -1.914598  0.879478  0.799762  1.244127  0.126057  1.131472  1.794036   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9233  0.344881 -0.931433  0.739117  1.871430 -0.032577 -0.594206 -1.039941   \n",
       "9234  0.610486 -0.883960  0.708051  1.945315  0.100459 -0.255287 -0.735913   \n",
       "9235  0.813433 -1.013030  0.612392  1.904598  0.138989 -0.031661 -0.491388   \n",
       "9236  0.300776 -1.099654  0.179745  1.709359  0.101430 -0.021087 -0.592462   \n",
       "9237 -0.141076 -0.841747 -0.171662  1.518323  0.178951 -0.354376 -0.920245   \n",
       "\n",
       "            31        26        14        20        6         11        38  \\\n",
       "0    -0.840203 -0.190102  0.057985 -0.773881 -0.211460 -0.443124 -0.130601   \n",
       "1     0.061981  0.047160  0.038753 -0.324938  0.081825 -0.373201  0.321552   \n",
       "2     0.298515 -0.432859  0.009860 -0.306709  0.536282 -0.265632  0.342450   \n",
       "3     0.660462 -0.094418  0.000942  0.019133  0.679312 -0.231309  0.463792   \n",
       "4     0.447582 -0.096808 -0.171956  0.254848  0.594968 -0.224626  0.005776   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9233 -0.509073 -0.257606 -0.355304 -0.337902  1.955351  0.696732 -0.993664   \n",
       "9234 -0.484002  0.104951 -0.734529 -0.387806  2.091698  0.528793 -0.712909   \n",
       "9235 -0.967253 -0.227728 -0.252240 -0.915382  2.458308  0.276310  0.378725   \n",
       "9236 -1.556661 -0.148204  0.084979 -2.017825  1.450899  0.127205  0.196277   \n",
       "9237 -0.362053  0.017998 -0.229013 -2.150286  0.644020  0.027940  0.382890   \n",
       "\n",
       "            21        18        15        22        12        13  \n",
       "0     0.202801 -0.186997 -0.187630 -0.032653 -0.019193  0.376977  \n",
       "1     0.467937 -0.598541 -0.270955  0.027107 -0.049397  0.386002  \n",
       "2     0.137449 -0.366524 -0.253419  0.012183  0.024585  0.317177  \n",
       "3     0.329118 -0.513304 -0.236251 -0.282491  0.136366  0.271059  \n",
       "4    -0.106193 -0.430760 -0.154636 -0.166812  0.192114  0.207816  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "9233 -0.459643 -0.780476  0.085641  0.103523  1.020420 -0.827579  \n",
       "9234  0.187002 -0.496463 -0.079321  0.201060  0.726277 -0.338853  \n",
       "9235  0.115268 -0.112195  0.129463 -0.507578  0.114448  0.129632  \n",
       "9236  0.509032  0.827147  0.147253 -0.492190  0.008271  0.058502  \n",
       "9237  1.099226  0.677530 -0.054276 -0.581448  0.019743 -0.055451  \n",
       "\n",
       "[9238 rows x 20 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at new input array\n",
    "sele_ind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9b56d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn input array into (# samples, 14 days, 10 features)\n",
    "sele_ind_data = sele_ind_df.values\n",
    "sele_ind_data1 = sele_ind_data.reshape(62,149,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b914381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sele_ind_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d83c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timeseries data arrays for PCs \n",
    "#was 122 and 14 for the 14 day window. \n",
    "\n",
    "new_input =  np.empty((62,lead,14,n_features))\n",
    "new_output = np.empty((62,lead))\n",
    "\n",
    "for i in range(0,62):\n",
    "    for j in range(0,lead):\n",
    "        new_input[i,j,:,:] = sele_ind_data1[i,j:j+14,:]\n",
    "        new_output[i,j] = temp[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36f0ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 135, 14, 20)\n",
      "(62, 135)\n"
     ]
    }
   ],
   "source": [
    "new_input = np.array(new_input)\n",
    "new_output = np.array(new_output)\n",
    "print(new_input.shape)\n",
    "print(new_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f7265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7114"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_ind = round((62*lead)*0.15)\n",
    "frac_end = round((62*lead)-frac_ind)\n",
    "frac_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "875e1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372674e",
   "metadata": {},
   "source": [
    "### In the cells below, I am going to have to introduce the new abstention category... I believe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9593d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8370, 14, 20)\n",
      "(8370,)\n"
     ]
    }
   ],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all = np.copy(new_input.reshape((62*lead),14,n_features))\n",
    "Y_all = np.copy(new_output.flatten())\n",
    "\n",
    "print(X_all.shape)\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "827cb1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "(7114, 14, 20)\n",
      "(7114,)\n",
      " \n",
      "Testing\n",
      "(1256, 14, 20)\n",
      "(1256,)\n"
     ]
    }
   ],
   "source": [
    "##training data partition out\n",
    "X_tri = X_all[:frac_end,:]\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes = X_all[frac_end:,:]\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "print(\"Training\")\n",
    "print(X_tri.shape)\n",
    "print(Y_tri.shape)\n",
    "print(\" \")\n",
    "print(\"Testing\")\n",
    "print(X_tes.shape)\n",
    "print(Y_tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d768f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the Y array into a categorical array\n",
    "#Traditionally, I have done this using keras.utils.to_categorical ... but I am going to try to use another method\n",
    "#shown in the example file from Barnes and Barnes\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "def onehot_abstent(y_train, y_test, num_cats):\n",
    "    encoder = preprocessing.OneHotEncoder(categories='auto', sparse_output=False)\n",
    "    encoder.fit(np.arange(num_cats).reshape(-1,1))\n",
    "    \n",
    "    onehotlabels_train = encoder.transform(np.array(y_train).reshape(-1, 1))\n",
    "    onehotlabels_test = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    onehotlabels_train = np.concatenate([onehotlabels_train, np.zeros((onehotlabels_train.shape[0], 1))], axis=1)\n",
    "    onehotlabels_test = np.concatenate([onehotlabels_test, np.zeros((onehotlabels_test.shape[0], 1))], axis=1)\n",
    "    \n",
    "    return onehotlabels_train, onehotlabels_test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90d20799",
   "metadata": {},
   "outputs": [],
   "source": [
    "##one hot encode and add on the abstention category\n",
    "Y_tri, Y_tes = onehot_abstent(Y_tri, Y_tes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "288249bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7114, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Y_tri.shape)\n",
    "Y_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e11ba768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y_tes.shape\n",
    "Y_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d64ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(Y_tri)) or np.any(np.isinf(Y_tri)):\n",
    "    print(\"NaN or Inf values found in Y_tri!\")\n",
    "\n",
    "if np.any(np.isnan(Y_tes)) or np.any(np.isinf(Y_tes)):\n",
    "    print(\"NaN or Inf values found in Y_tes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6acfcf",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd827004",
   "metadata": {},
   "outputs": [],
   "source": [
    "##number of input nodes\n",
    "numb_int = X_all.shape[1:]\n",
    "\n",
    "##fraction of training data\n",
    "X_validation = X_tri[0:frac_ind]\n",
    "Y_validation = Y_tri[0:frac_ind]\n",
    "        \n",
    "X_train = X_tri[frac_ind:len(X_tri)]\n",
    "Y_train = Y_tri[frac_ind:len(Y_tri)]\n",
    "\n",
    "X_test = X_tes\n",
    "Y_test = Y_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98824dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5858, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc6e1dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 20)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numb_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12342f6",
   "metadata": {},
   "source": [
    "### Model Architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aecd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These definition statements are from Barnes and Barnes 2021\n",
    "def make_model(ntimestep, nfeature, n1, n2, n3, rl1, rl3, lr, alpha_init, setpoint, spinup_epochs, nupd,\n",
    "              loss_str = 'NotWrongLoss', updater_str = 'Colorado'):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    if(loss_str == 'DNN'):\n",
    "        input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "\n",
    "        layer1 = layers.RNN(\n",
    "            layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                            dropout=0.4, \n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(l2=rl1)),\n",
    "            return_sequences=False)(input_tensor)\n",
    "\n",
    "        output_tensor = layers.Dense(2, activation='softmax')(layer1)\n",
    "\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model, None  \n",
    "    \n",
    "    else:\n",
    "        input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "    \n",
    "        layer1 = layers.RNN(\n",
    "            layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                              dropout=0.2, \n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              kernel_regularizer=keras.regularizers.l2(l2= rl1)),\n",
    "            return_sequences=False)(input_tensor)\n",
    "\n",
    "        #layer2 = layers.RNN(\n",
    "            #layers.LSTMCell(n2, activation='tanh', use_bias=True,\n",
    "                             #kernel_initializer='glorot_uniform',\n",
    "                             #kernel_regularizer=keras.regularizers.l2(l2= rl1)))(layer1)\n",
    "\n",
    "        layer3 = layers.Dense(n3, activation='relu',use_bias=True,\n",
    "                             kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                              kernel_regularizer=keras.regularizers.l2(l2=rl3))(layer1)\n",
    "        ### change the output tensor to reflect the inclusion of the abstention category.\n",
    "        output_tensor = layers.Dense(3, activation='softmax',)(layer3)\n",
    "\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "        updater = getattr(abstentionloss, updater_str)(setpoint=setpoint, \n",
    "                                                       alpha_init=alpha_init, \n",
    "                                                       length=nupd)\n",
    "        loss_function = getattr(abstentionloss, loss_str)(updater=updater,\n",
    "                                                          spinup_epochs=spinup_epochs)\n",
    "        model.compile(\n",
    "            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr),#tf.keras.optimizers.SGD(lr=lr, momentum=0.9, nesterov=True),\n",
    "            loss = loss_function,\n",
    "            metrics=[keras.metrics.categorical_accuracy,\n",
    "                alpha_value,\n",
    "                metrics.AbstentionFraction(abstain=2),\n",
    "                metrics.PredictionLoss(abstain=2),\n",
    "                metrics.PredictionAccuracy(abstain=2)\n",
    "            ]\n",
    "        )        \n",
    "        \n",
    "    # model.summary()\n",
    "        \n",
    "    return model, loss_function\n",
    "\n",
    "#---------------------------------------------------\n",
    "def alpha_value(y_true,y_pred):\n",
    "    return loss_function.updater.alpha\n",
    "\n",
    "#---------------------------------------------------\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < lr_epoch_bound:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.1)\n",
    "\n",
    "#---------------------------------------------------\n",
    "class EarlyStoppingCAN(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingCAN, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as zero.\n",
    "        self.best = 0.\n",
    "        self.best_epoch = np.Inf\n",
    "        # initialize best_weights to non-trained model\n",
    "        self.best_weights = self.model.get_weights()\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_prediction_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            abstention_error = np.abs(logs.get(\"val_abstention_fraction\") - setpoint)\n",
    "            if np.less(abstention_error,.1):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "                # Record the best weights if current results is better (greater).\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Early stopping, setting to best_epoch = \" + str(self.best_epoch + 1))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d0b27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training a CAN...\n"
     ]
    }
   ],
   "source": [
    "#---------------------\n",
    "######\n",
    "batch_size = 64 #The number of samples the network sees before it backpropagates (batch size)\n",
    "epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "# Set parameters\n",
    "LOSS = 'NotWrongLoss' #or DNN ClassWeight_\n",
    "UPDATER = 'Washington'\n",
    "NLABEL = 3 #number of categories\n",
    "NUPD = 50\n",
    "SPINUP_EPOCHS = 0\n",
    "setpoint = 0.275\n",
    "alpha_init = 1\n",
    "BATCH_SIZE = batch_size\n",
    "N_EPOCHS = epochs\n",
    "#---------------------\n",
    "NETWORK_SEED = 0\n",
    "network_type = 'CAN' # 'CAN' or 'DNN'\n",
    "#---------------------\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "# Get the model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# define the model, loss function and callbacks\n",
    "if(network_type=='DNN'):\n",
    "    print('--> Training a DNN...')    \n",
    "    model, loss_function = make_model(ntimestep = numb_int[0],\n",
    "                                      nfeature = numb_int[1],\n",
    "                                      n1 = 6, \n",
    "                                      rl1 = 0.077,  \n",
    "                                      lr = 0.015, #0.0006466\n",
    "                                      alpha_init = alpha_init,\n",
    "                                      setpoint=setpoint, \n",
    "                                      spinup_epochs=SPINUP_EPOCHS,\n",
    "                                      nupd=NUPD,\n",
    "                                      loss_str = LOSS, \n",
    "                                      updater_str=UPDATER)\n",
    "    callbacks = []\n",
    "    \n",
    "if(network_type=='CAN'):\n",
    "    print('--> Training a CAN...')    \n",
    "    es_can_callback = EarlyStoppingCAN(patience=40)            \n",
    "    model, loss_function = make_model(ntimestep = numb_int[0],\n",
    "                                      nfeature = numb_int[1],\n",
    "                                      n1 = 6,\n",
    "                                      n2= 4,\n",
    "                                      n3 = 6,\n",
    "                                      rl1 = 0.06,\n",
    "                                      rl3 = 0.077,\n",
    "                                      lr = 0.0177, #0.0006466\n",
    "                                      alpha_init = alpha_init,\n",
    "                                      setpoint=setpoint, \n",
    "                                      spinup_epochs=SPINUP_EPOCHS,\n",
    "                                      nupd=NUPD,\n",
    "                                      loss_str = LOSS, \n",
    "                                      updater_str=UPDATER)\n",
    "    callbacks = [abstentionloss.AlphaUpdaterCallback(), es_can_callback]         \n",
    "    \n",
    "\n",
    "                                      #n1 = 6,\n",
    "                                      #n2= 4,\n",
    "                                      #n3 = 6,\n",
    "                                      #rl1 = 0.06,\n",
    "                                      #rl3 = 0.077,\n",
    "                                      #lr = 0.01755, #0.0006466\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "# Remake onehotencoding for whether CAN or DNN\n",
    "hotlabels = Y_train[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n",
    "hotlabels_val = Y_validation[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631a3838",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_data=(X_validation, Y_validation),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=N_EPOCHS,\n",
    "    shuffle=True,\n",
    "    verbose=0,\n",
    "    class_weight = {0 : 1.0, 1 : 1.84, 2 : 0.4},\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a36f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this cell is for verifying that the model is kinda working\n",
    "history = model.history.history\n",
    "\n",
    "for key in history:\n",
    "    print(f\"{key}: {history[key][-1]}\")\n",
    "\n",
    "##check to see how much the validation is abstaining on the abstain class\n",
    "preds_val = model.predict(X_validation)\n",
    "abstain_class = 2\n",
    "pred_labels_val = np.argmax(preds_val, axis=1)\n",
    "abstain_rate_val = np.mean(pred_labels_val == abstain_class)\n",
    "\n",
    "print(\"Manual validation abstention rate: \", [abstain_rate_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dceacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstain_rate_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_all)\n",
    "pred_val = model.predict(X_validation)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Look at the optimization history\n",
    "\n",
    "epochs = range(len(history['alpha_value']))\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(epochs, history['alpha_value'], label='Train α')\n",
    "plt.plot(epochs, history['val_alpha_value'], label='Val α')\n",
    "plt.legend(); plt.title(\"Alpha (Penalty)\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Alpha\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(epochs, history['abstention_fraction'], label='Train')\n",
    "plt.plot(epochs, history['val_abstention_fraction'], label='Val')\n",
    "plt.legend(); plt.title(\"Abstention Fraction\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Fraction\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.plot(epochs, history['prediction_accuracy'], label='Train')\n",
    "plt.plot(epochs, history['val_prediction_accuracy'], label='Val')\n",
    "plt.legend(); plt.title(\"Prediction Accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60beb3a4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "train_loss=model.history.history['loss']\n",
    "val_loss=model.history.history['val_loss']\n",
    "    \n",
    "cat_acc=model.history.history['categorical_accuracy']\n",
    "val_acc=model.history.history['val_categorical_accuracy']\n",
    "        \n",
    "pred = model.predict(X_all)\n",
    "pred_val = model.predict(X_validation)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Look at the optimization history\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_loss, label='Training loss')\n",
    "ax1.plot(val_loss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cat_acc, label='Training ACC')\n",
    "ax2.plot(val_acc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend\n",
    "#plt.savefig(\"SEUS_onearch.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_val_pred = model.predict(X_validation)\n",
    "y_val_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true = np.argmax(Y_validation, axis=1)\n",
    "\n",
    "print(classification_report(y_val_true, y_val_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example variables:\n",
    "# y_all: shape (n_samples, 2) — one-hot encoded true labels\n",
    "# pred_probs: shape (n_samples, 2) — predicted probabilities for each class\n",
    "\n",
    "# Convert one-hot labels to 1D integer labels (0 or 1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Extract predicted probability for the positive class (class 1)\n",
    "y_prob = pred_test[:,1]\n",
    "\n",
    "# Create climatology baseline: constant probability = mean positive class rate\n",
    "p_climatology = np.full_like(y_true, y_true.mean(), dtype=float)\n",
    "\n",
    "# Compute Brier Scores\n",
    "bs_model = brier_score_loss(y_true, y_prob)\n",
    "bs_climo = brier_score_loss(y_true, p_climatology)\n",
    "\n",
    "# Compute Brier Skill Score\n",
    "bss = 1 - (bs_model / bs_climo)\n",
    "\n",
    "print(f\"Brier Score (Model): {bs_model:.4f}\")\n",
    "print(f\"Brier Score (Climatology): {bs_climo:.4f}\")\n",
    "print(f\"Brier Skill Score: {bss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels for negative class\n",
    "y_true_neg = 1 - y_true  # flip 0 <-> 1\n",
    "\n",
    "# Predicted probabilities for negative class\n",
    "y_prob_neg = pred_test[:, 0]\n",
    "\n",
    "# Calculate Brier Score and BSS for negative class\n",
    "bs_model_neg = brier_score_loss(y_true_neg, y_prob_neg)\n",
    "bs_climo_neg = brier_score_loss(y_true_neg, np.full_like(y_true_neg, y_true_neg.mean()))\n",
    "bss_neg = 1 - (bs_model_neg / bs_climo_neg)\n",
    "\n",
    "print(f\"Brier Score (Model): {bs_model_neg:.4f}\")\n",
    "print(f\"Brier Score (Climatology): {bs_climo_neg:.4f}\")\n",
    "print(f\"Brier Skill Score: {bss_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.2, 0.7, 37)  #10 bins from 0.4 to 1\n",
    "##bin the probabilities\n",
    "counts_neg_test, edges_neg_test = np.histogram(pred_test[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(pred_test[:,1], bins=bins)\n",
    "counts_abs_test, edges_abs_test = np.histogram(pred_test[:,2], bins=bins)\n",
    "\n",
    "counts_neg_train, edges_neg_train = np.histogram(pred_train[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(pred_train[:,1], bins=bins)\n",
    "counts_abs_train, edges_abs_train = np.histogram(pred_train[:,2], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(pred_val[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(pred_val[:,1], bins=bins)\n",
    "counts_abs_val, edges_abs_val = np.histogram(pred_val[:,2], bins=bins)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10,10))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative Training Predictions', fontsize=13)\n",
    "ax1.set_xlabel('Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive Training Predictions', fontsize=13)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_abs_train[:-1], counts_abs_train, width=np.diff(edges_abs_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax3.set_title('Abstention Training Predictions', fontsize=13)\n",
    "ax3.set_xlabel('Probability')\n",
    "ax3.set_ylabel('Count')\n",
    "\n",
    "ax4.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax4.set_title('Negative Validation Predictions', fontsize=13)\n",
    "ax4.set_xlabel('Probability')\n",
    "ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax5.set_title('Positive Validation Predictions', fontsize=13)\n",
    "ax5.set_xlabel('Probability')\n",
    "ax5.set_ylabel('Count')\n",
    "\n",
    "ax6.bar(edges_abs_val[:-1], counts_abs_val, width=np.diff(edges_abs_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax6.set_title('Abstention Validation Predictions', fontsize=13)\n",
    "ax6.set_xlabel('Probability')\n",
    "ax6.set_ylabel('Count')\n",
    "\n",
    "ax7.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax7.set_title('Negative Testing Predictions', fontsize=13)\n",
    "ax7.set_xlabel('Probability')\n",
    "ax7.set_ylabel('Count')\n",
    "\n",
    "ax8.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax8.set_title('Positive Testing Predictions', fontsize=13)\n",
    "ax8.set_xlabel('Probability')\n",
    "ax8.set_ylabel('Count')\n",
    "\n",
    "ax9.bar(edges_abs_test[:-1], counts_abs_test, width=np.diff(edges_abs_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax9.set_title('Abstention Testing Predictions', fontsize=13)\n",
    "ax9.set_xlabel('Probability')\n",
    "ax9.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ACC\n",
    "##definition statement for ACC\n",
    "def calculate_accuracy(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "##TESTING\n",
    "q90 = np.percentile(pred_test,90,axis=0) ##90th percentile of test\n",
    "##90th percentile acc\n",
    "great90 = [i for i, row in enumerate(pred_test) if (row[0] > q90[0]) or (row[1] > q90[1])]\n",
    "# Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "test90 = pred_test[great90]\n",
    "test90_norm = Y_tes[great90]\n",
    "test90_acc = calculate_accuracy(test90_norm, test90, threshold=0.5)\n",
    "##full model\n",
    "fulltest_acc = calculate_accuracy(Y_tes, pred_test, threshold=0.5)\n",
    "    \n",
    "##BSS\n",
    "y_true = np.argmax(test90_norm, axis=1)\n",
    "y_prob = test90[:,1]\n",
    "\n",
    "p_climatology = np.full_like(y_true, y_true.mean(), dtype=float)\n",
    "bs_model = brier_score_loss(y_true, y_prob)\n",
    "bs_climo = brier_score_loss(y_true, p_climatology)\n",
    "BSS90 = 1 - (bs_model / bs_climo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BSS 90th Percentile: \",BSS90)\n",
    "print(\"90th Percentile ACC: \",test90_acc)\n",
    "print(\"Model ACC: \",fulltest_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
