{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "819fd830",
   "metadata": {},
   "source": [
    "### CAN Model Loop with Skill Score calculation\n",
    "\n",
    "File created on 6/17/2025 for the controlled abstention model. \n",
    "\n",
    "I will start with a 14 day model. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9588507",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f58b31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 18:15:14.400821: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 18:15:14.452172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9373] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 18:15:14.452199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 18:15:14.453492: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1534] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 18:15:14.461112: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 18:15:16.492698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1926] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 978 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2d:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "##import cell makes its appearance once again...\n",
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "#from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "tf.device('/physical_device:GPU:0')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "#import investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f611ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StatScores import BSS\n",
    "from StatScores import RAS_two\n",
    "from StatScores import PAS_two\n",
    "\n",
    "##This cell includes the new class and def functions from Barnes and Barnes 2021 relevant for creating a CAN. \n",
    "import metrics\n",
    "import abstentionloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f054012",
   "metadata": {},
   "source": [
    "### Inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa8d4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in solvers for PC analysis to get the PCs for the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8271eeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load input solvers\n",
    "infile = open(\"../../reduced_data/PCs/U_14.p\",\"rb\",)\n",
    "U_PC = pickle.load(infile)  ##pv on an isentropic surface, 350\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../reduced_data/PCs/EHF_14.p\",\"rb\",)\n",
    "EHF_PC = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../../reduced_data/PCs/GPH_14.p\",\"rb\",)\n",
    "GPH_PC = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "infile.close()\n",
    "\n",
    "##remove PC 1\n",
    "U_PC = U_PC[:, 1:]\n",
    "EHF_PC = EHF_PC[:, 1:]\n",
    "GPH_PC = GPH_PC[:, 1:]\n",
    "\n",
    "U_PC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "415b4b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create one array of PCs\n",
    "inputvar = np.concatenate((U_PC,EHF_PC,GPH_PC),axis=1) \n",
    "\n",
    "##make pandas dataframe for RF\n",
    "input = pd.DataFrame(inputvar)\n",
    "#input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64804469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected indices\n",
    "#EUROPE\n",
    "#input[[1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13]]\n",
    "\n",
    "##CANADA\n",
    "#input[[56, 1, 0, 55, 2, 53, 22, 57, 9, 25, 11, 4, 54, 38, 17, 30, 36, 3, 14, 26]]\n",
    "\n",
    "##SE US\n",
    "#input[[56, 1, 54, 18, 53, 3, 0, 10, 5, 19, 21, 14, 2, 57, 13, 60, 22, 26,25, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5f32ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create pd datafram of selected feature columns.\n",
    "sele_ind_df = input[[1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13]]\n",
    "\n",
    "n_features = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f9ffcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn input array into (# samples, 14 days, 10 features)\n",
    "sele_ind_data = sele_ind_df.values\n",
    "sele_ind_data1 = sele_ind_data.reshape(62,149,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9564b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sele_ind_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ebdff",
   "metadata": {},
   "source": [
    "### Outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4bb405f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load output data\n",
    "infile = open(\"../../reduced_data/temps/eur_14.p\",\"rb\",)\n",
    "output = pickle.load(infile) \n",
    "infile.close()\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85cab160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load climo data\n",
    "infile = open(\"../../reduced_data/climo/eur_climo14.p\",\"rb\",)\n",
    "climo = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81b1e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (62, 149, 20)\n",
      "climo:  (62, 135, 2)\n",
      "output:  (62, 135)\n"
     ]
    }
   ],
   "source": [
    "##need to change this based on lag \n",
    "## 14-days = [:, 10:] 10 day window rather than 14\n",
    "## 20-days = [:, 16:]\n",
    "## 30-days = [:, 26:]\n",
    "\n",
    "# 139\n",
    "# 133\n",
    "# 123\n",
    "\n",
    "## 14-days = [:, 14:] 14 day window\n",
    "## 20-days = [:, 20:]\n",
    "## 30-days = [:, 30:]\n",
    "\n",
    "# 135\n",
    "# 129\n",
    "# 119\n",
    "\n",
    "lead = 135\n",
    "\n",
    "temp = output.reshape(62, 149)\n",
    "temp = temp[:, 14:]\n",
    "\n",
    "climo = climo.reshape(62, 149,2)\n",
    "climo = climo[:, 14:, :]\n",
    "#climo = climo.reshape(8370,2)\n",
    "\n",
    "print(\"input: \",sele_ind_data1.shape)\n",
    "print(\"climo: \",climo.shape)\n",
    "print(\"output: \",temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2e7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create timeseries data arrays for PCs \n",
    "#was 122 and 14 for the 14 day window. \n",
    "\n",
    "new_input =  np.empty((62,lead,14,n_features))\n",
    "new_output = np.empty((62,lead))\n",
    "new_climo = np.empty((62,lead,2))\n",
    "\n",
    "for i in range(0,62):\n",
    "    for j in range(0,lead):\n",
    "        new_input[i,j,:,:] = sele_ind_data1[i,j:j+14,:]\n",
    "        new_output[i,j] = temp[i,j]\n",
    "        new_climo[i,j] = climo[i,j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d88b79f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  (62, 135, 14, 20)\n",
      "climo:  (8370, 2)\n",
      "output:  (62, 135)\n"
     ]
    }
   ],
   "source": [
    "new_input = np.array(new_input)\n",
    "new_climo = np.array(new_climo.reshape(62*lead,2))\n",
    "new_output = np.array(new_output)\n",
    "\n",
    "print(\"input: \",new_input.shape)\n",
    "print(\"climo: \",new_climo.shape)\n",
    "print(\"output: \",new_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5b31894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing and validation size:  1256\n",
      "training size:  7114\n"
     ]
    }
   ],
   "source": [
    "frac_ind = round((62*lead)*0.15)\n",
    "frac_end = round((62*lead)-frac_ind)\n",
    "\n",
    "print(\"testing and validation size: \",frac_ind)\n",
    "print(\"training size: \",frac_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b623d0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8370, 14, 20)\n",
      "(8370,)\n"
     ]
    }
   ],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all = np.copy(new_input.reshape((62*lead),14,n_features))\n",
    "Y_all = np.copy(new_output.flatten())\n",
    "\n",
    "print(X_all.shape)\n",
    "print(Y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "533a3816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "(7114, 14, 20)\n",
      "(7114,)\n",
      " \n",
      "Testing\n",
      "(1256, 14, 20)\n",
      "(1256,)\n"
     ]
    }
   ],
   "source": [
    "##training data partition out\n",
    "X_tri = X_all[:frac_end,:]\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes = X_all[frac_end:,:]\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "print(\"Training\")\n",
    "print(X_tri.shape)\n",
    "print(Y_tri.shape)\n",
    "print(\" \")\n",
    "print(\"Testing\")\n",
    "print(X_tes.shape)\n",
    "print(Y_tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "688748ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the Y array into a categorical array\n",
    "#Traditionally, I have done this using keras.utils.to_categorical ... but I am going to try to use another method\n",
    "#shown in the example file from Barnes and Barnes\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "\n",
    "def onehot_abstent(y_all, y_train, y_test, num_cats):\n",
    "    encoder = preprocessing.OneHotEncoder(categories='auto', sparse_output=False)\n",
    "    encoder.fit(np.arange(num_cats).reshape(-1,1))\n",
    "    \n",
    "    onehotlabels_all = encoder.transform(np.array(y_all).reshape(-1, 1))\n",
    "    onehotlabels_train = encoder.transform(np.array(y_train).reshape(-1, 1))\n",
    "    onehotlabels_test = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "    \n",
    "    onehotlabels_all = np.concatenate([onehotlabels_all, np.zeros((onehotlabels_all.shape[0], 1))], axis=1)\n",
    "    onehotlabels_train = np.concatenate([onehotlabels_train, np.zeros((onehotlabels_train.shape[0], 1))], axis=1)\n",
    "    onehotlabels_test = np.concatenate([onehotlabels_test, np.zeros((onehotlabels_test.shape[0], 1))], axis=1)\n",
    "    \n",
    "    return onehotlabels_all, onehotlabels_train, onehotlabels_test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcb41a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##one hot encode and add on the abstention category\n",
    "Y_all, Y_tri, Y_tes = onehot_abstent(Y_all, Y_tri, Y_tes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c34eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8370, 3)\n",
      "(7114, 3)\n",
      "(1256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(Y_all.shape)\n",
    "print(Y_tri.shape)\n",
    "print(Y_tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cda8311",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(Y_tri)) or np.any(np.isinf(Y_tri)):\n",
    "    print(\"NaN or Inf values found in Y_tri!\")\n",
    "\n",
    "if np.any(np.isnan(Y_tes)) or np.any(np.isinf(Y_tes)):\n",
    "    print(\"NaN or Inf values found in Y_tes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404610f",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "991bb19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of lstm input:  (14, 20)\n"
     ]
    }
   ],
   "source": [
    "##number of input nodes\n",
    "numb_int = X_all.shape[1:]\n",
    "#this defines the 2-d shape used by the lstm model.\n",
    "print(\"shape of lstm input: \",numb_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e94cf714",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#CHANGE BASED ON NUMBER OF FOLDS FOR TIMESERIES SPLIT\n",
    "#repeat = 2\n",
    "#split = 7\n",
    "#n = repeat*split\n",
    "\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f7ac6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##definition statement for ACC\n",
    "def calculate_accuracy(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afcc08d",
   "metadata": {},
   "source": [
    "All empty lists in next three cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b498c8c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##Loss and ACC lists, only relevant w/o early stopping\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "cat_acc = []\n",
    "val_acc = [] \n",
    "\n",
    "##BSS Arrays, all of the skill scores have 200 rows\n",
    "#because that is how many cross-validations I will do for the model\n",
    "BSS_all= np.empty((n,))\n",
    "BSS_val= np.empty((n,))\n",
    "BSS_train= np.empty((n,))\n",
    "BSS_test= np.empty((n,))\n",
    "BSS_90= np.empty((n,))\n",
    "\n",
    "BSS_avg= np.empty((n,))\n",
    "BSS_30= np.empty((n,))\n",
    "BSS_70= np.empty((n,))\n",
    "\n",
    "##RAS and PAS Arrays\n",
    "Prec_all= np.empty((n,2))\n",
    "Rec_all= np.empty((n,2))\n",
    "\n",
    "Prec_val= np.empty((n,2))\n",
    "Rec_val= np.empty((n,2))\n",
    "\n",
    "Prec_train= np.empty((n,2))\n",
    "Rec_train= np.empty((n,2))\n",
    "\n",
    "Prec_test= np.empty((n,2))\n",
    "Rec_test= np.empty((n,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5e5df45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##empty lists for ACC scores \n",
    "##Testing Accuracy Arrays\n",
    "fulltest_acc = []\n",
    "test90_acc = []\n",
    "\n",
    "##Training Accuracy Arrays\n",
    "fulltrain_acc = []\n",
    "train90_acc = []\n",
    "\n",
    "##Validation Accuracy Arrays\n",
    "fullval_acc = []\n",
    "val90_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6844e279",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p_test = []\n",
    "p_train = []\n",
    "p_val = []\n",
    "\n",
    "abs_test = np.empty((n,))\n",
    "abs_train = np.empty((n,))\n",
    "abs_val = np.empty((n,))\n",
    "\n",
    "p_mid = []\n",
    "p_90 = []\n",
    "p_30 = []\n",
    "p_70 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b678b9",
   "metadata": {},
   "source": [
    "### Model Architecture. \n",
    "\n",
    "Start with model definition cells from Barnes and Barnes 2021. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f930a13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#These definition statements are from Barnes and Barnes 2021\n",
    "#the \"make model\" cell contains the DNN for testing whether the model learns and the CAN for abstention\n",
    "def make_model(ntimestep, nfeature, n1, n2, n3, rl1, rl3, lr, alpha_init, setpoint, spinup_epochs, nupd,\n",
    "              loss_str = 'NotWrongLoss', updater_str = 'Colorado'):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    if(loss_str == 'DNN'):\n",
    "        input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "\n",
    "        layer1 = layers.RNN(\n",
    "            layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                            dropout=0.4, \n",
    "                            kernel_initializer='glorot_uniform',\n",
    "                            kernel_regularizer=keras.regularizers.l2(l2=rl1)),\n",
    "            return_sequences=False)(input_tensor)\n",
    "\n",
    "        output_tensor = layers.Dense(2, activation='softmax')(layer1)\n",
    "\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model, None  \n",
    "    \n",
    "    else:\n",
    "        input_tensor = Input(shape=(ntimestep, nfeature))\n",
    "    \n",
    "        layer1 = layers.RNN(\n",
    "            layers.LSTMCell(n1, activation='tanh', use_bias=True,\n",
    "                              dropout=0.2, \n",
    "                              kernel_initializer='glorot_uniform',\n",
    "                              kernel_regularizer=keras.regularizers.l2(l2= rl1)),\n",
    "            return_sequences=False)(input_tensor)\n",
    "\n",
    "        #layer2 = layers.RNN(\n",
    "            #layers.LSTMCell(n2, activation='tanh', use_bias=True,\n",
    "                             #kernel_initializer='glorot_uniform',\n",
    "                             #kernel_regularizer=keras.regularizers.l2(l2= rl1)))(layer1)\n",
    "\n",
    "        layer3 = layers.Dense(n3, activation='relu',use_bias=True,\n",
    "                             kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                              kernel_regularizer=keras.regularizers.l2(l2=rl3))(layer1)\n",
    "        ### change the output tensor to reflect the inclusion of the abstention category.\n",
    "        output_tensor = layers.Dense(3, activation='softmax',)(layer3)\n",
    "\n",
    "        model = Model(input_tensor, output_tensor)\n",
    "        updater = getattr(abstentionloss, updater_str)(setpoint=setpoint, \n",
    "                                                       alpha_init=alpha_init, \n",
    "                                                       length=nupd)\n",
    "        loss_function = getattr(abstentionloss, loss_str)(updater=updater,\n",
    "                                                          spinup_epochs=spinup_epochs)\n",
    "        model.compile(\n",
    "            optimizer= tf.keras.optimizers.legacy.Adam(learning_rate=lr),#tf.keras.optimizers.SGD(lr=lr, momentum=0.9, nesterov=True),\n",
    "            loss = loss_function,\n",
    "            metrics=[keras.metrics.categorical_accuracy,\n",
    "                alpha_value,\n",
    "                metrics.AbstentionFraction(abstain=2),\n",
    "                metrics.PredictionLoss(abstain=2),\n",
    "                metrics.PredictionAccuracy(abstain=2)\n",
    "            ]\n",
    "        )        \n",
    "        \n",
    "    # model.summary()\n",
    "        \n",
    "    return model, loss_function\n",
    "\n",
    "#---------------------------------------------------\n",
    "def alpha_value(y_true,y_pred):\n",
    "    return loss_function.updater.alpha\n",
    "\n",
    "#---------------------------------------------------\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < lr_epoch_bound:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*tf.math.exp(-0.1)\n",
    "\n",
    "#---------------------------------------------------\n",
    "class EarlyStoppingCAN(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingCAN, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as zero.\n",
    "        self.best = 0.\n",
    "        self.best_epoch = np.Inf\n",
    "        # initialize best_weights to non-trained model\n",
    "        self.best_weights = self.model.get_weights()\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"val_prediction_accuracy\")\n",
    "        if np.greater(current, self.best):\n",
    "            abstention_error = np.abs(logs.get(\"val_abstention_fraction\") - setpoint)\n",
    "            if np.less(abstention_error,.1):\n",
    "                self.best = current\n",
    "                self.wait = 0\n",
    "                # Record the best weights if current results is better (greater).\n",
    "                self.best_weights = self.model.get_weights()\n",
    "                self.best_epoch = epoch\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Early stopping, setting to best_epoch = \" + str(self.best_epoch + 1))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b78959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "CV  0\n",
      "--> Training a CAN...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Early stopping, setting to best_epoch = 1\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  1\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "BAD MODEL ITERATION\n",
      "#######################################\n",
      "CV  2\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  3\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  4\n",
      "--> Training a CAN...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Early stopping, setting to best_epoch = 6\n",
      "262/262 [==============================] - 1s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "184/184 [==============================] - 0s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "#######################################\n",
      "CV  5\n",
      "--> Training a CAN...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Early stopping, setting to best_epoch = 8\n",
      "262/262 [==============================] - 1s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "184/184 [==============================] - 0s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "#######################################\n",
      "CV  6\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "BAD MODEL ITERATION\n",
      "#######################################\n",
      "CV  7\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  8\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "#######################################\n",
      "CV  9\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  10\n",
      "--> Training a CAN...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Early stopping, setting to best_epoch = 5\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  11\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "BAD MODEL ITERATION\n",
      "#######################################\n",
      "CV  12\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  13\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  14\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  15\n",
      "--> Training a CAN...\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "184/184 [==============================] - 1s 3ms/step\n",
      "40/40 [==============================] - 0s 3ms/step\n",
      "#######################################\n",
      "CV  16\n",
      "--> Training a CAN...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Early stopping, setting to best_epoch = 8\n",
      "262/262 [==============================] - 1s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "184/184 [==============================] - 0s 2ms/step\n",
      "40/40 [==============================] - 0s 2ms/step\n",
      "#######################################\n",
      "CV  17\n",
      "--> Training a CAN...\n"
     ]
    }
   ],
   "source": [
    "## create loop for cross validation\n",
    "model_fail = 0\n",
    "for l in range(0,n):\n",
    "    print(\"#######################################\")\n",
    "    print(\"CV \",l)\n",
    "    ##randomly choose a fraction of events for validation and training\n",
    "    start = random.randrange(len(X_tri[:,0])-frac_ind)\n",
    "    end = start+(frac_ind)\n",
    "\n",
    "    #choose random 15% window of training data to be used for validation\n",
    "    X_val = X_tri[start:end]\n",
    "    Y_val = Y_tri[start:end]\n",
    "\n",
    "    #remaining training data\n",
    "    X_train1 = X_tri[0:start]\n",
    "    Y_train1 = Y_tri[0:start]\n",
    "    X_train2 = X_tri[end:]\n",
    "    Y_train2 = Y_tri[end:]\n",
    "    ##concatenate all of these\n",
    "    X_tr = np.concatenate((X_train1,X_train2))\n",
    "    Y_tr = np.concatenate((Y_train1,Y_train2))\n",
    "    \n",
    "    \n",
    "    ##same partitioning for climo data\n",
    "    climo_train1 = new_climo[0:start,:]\n",
    "    climo_train2 = new_climo[end:frac_end,:]\n",
    "    \n",
    "    climo_train = np.concatenate((climo_train1,climo_train2))\n",
    "    \n",
    "    ##pause, calculate climo probabilities\n",
    "    climo_val = new_climo[start:end,:]\n",
    "    climo_test = new_climo[frac_end:,:]\n",
    "    #####################################\n",
    "    ##INSERT LOOP FOR CAN MODEL INITALIZATION from Barnes and Barnes 2021\n",
    "    batch_size = 64 #The number of samples the network sees before it backpropagates (batch size)\n",
    "    epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n",
    "    shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "    verbose = 0 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "    # Set parameters\n",
    "    LOSS = 'ClassWeight_NotWrongLoss' #or DNN\n",
    "    UPDATER = 'Washington'\n",
    "    NLABEL = 3 #number of categories\n",
    "    NUPD = 50\n",
    "    SPINUP_EPOCHS = 0\n",
    "    setpoint = 0.275\n",
    "    alpha_init = 1\n",
    "    BATCH_SIZE = batch_size\n",
    "    N_EPOCHS = epochs\n",
    "    #---------------------\n",
    "    NETWORK_SEED = 0\n",
    "    network_type = 'CAN' # 'CAN' or 'DNN'\n",
    "    #---------------------\n",
    "\n",
    "\n",
    "    #-------------------------------\n",
    "    # Get the model\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # define the model, loss function and callbacks\n",
    "    if(network_type=='DNN'):\n",
    "        print('--> Training a DNN...')    \n",
    "        model, loss_function = make_model(ntimestep = numb_int[0],\n",
    "                                          nfeature = numb_int[1],\n",
    "                                          n1 = 6, \n",
    "                                          rl1 = 0.077,  \n",
    "                                          lr = 0.015, #0.0006466\n",
    "                                          alpha_init = alpha_init,\n",
    "                                          setpoint=setpoint, \n",
    "                                          spinup_epochs=SPINUP_EPOCHS,\n",
    "                                          nupd=NUPD,\n",
    "                                          loss_str = LOSS, \n",
    "                                          updater_str=UPDATER)\n",
    "        callbacks = []\n",
    "\n",
    "    if(network_type=='CAN'):\n",
    "        print('--> Training a CAN...')    \n",
    "        es_can_callback = EarlyStoppingCAN(patience=40)            \n",
    "        model, loss_function = make_model(ntimestep = numb_int[0],\n",
    "                                          nfeature = numb_int[1],\n",
    "                                          n1 = 6,\n",
    "                                          n2= 4,\n",
    "                                          n3 = 6,\n",
    "                                          rl1 = 0.06,\n",
    "                                          rl3 = 0.077,\n",
    "                                          lr = 0.017, #0.0006466\n",
    "                                          alpha_init = alpha_init,\n",
    "                                          setpoint=setpoint, \n",
    "                                          spinup_epochs=SPINUP_EPOCHS,\n",
    "                                          nupd=NUPD,\n",
    "                                          loss_str = LOSS, \n",
    "                                          updater_str=UPDATER)\n",
    "        callbacks = [abstentionloss.AlphaUpdaterCallback(), es_can_callback]         \n",
    "\n",
    "\n",
    "                                          #n1 = 6,\n",
    "                                          #n2= 4,\n",
    "                                          #n3 = 6,\n",
    "                                          #rl1 = 0.06,\n",
    "                                          #rl3 = 0.077,\n",
    "                                          #lr = 0.01755, #0.0006466\n",
    "\n",
    "\n",
    "    #-------------------------------\n",
    "    # Remake onehotencoding for whether CAN or DNN\n",
    "    #hotlabels = Y_train[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n",
    "    #hotlabels_val = Y_validation[:,:model.output_shape[-1]] # strip off abstention class if using the DNN\n",
    "    \n",
    "    #-------------------------------\n",
    "    # Train the model\n",
    "    model.fit(X_tr,Y_tr,validation_data=(X_val, Y_val),batch_size=BATCH_SIZE,epochs=N_EPOCHS,shuffle=True,verbose=verbose,callbacks=callbacks)\n",
    "    \n",
    "    ########### SAVE PREDICTED PROBABILITIES\n",
    "    pred = model.predict(X_all)\n",
    "    pred_val = model.predict(X_val)\n",
    "    pred_train = model.predict(X_tr)\n",
    "    pred_test = model.predict(X_tes)\n",
    "    \n",
    "    p_train.extend(pred_train)\n",
    "    p_val.extend(pred_val)\n",
    "    p_test.extend(pred_test)\n",
    "    \n",
    "    #SAVE ABSTENTION RATE\n",
    "    abstain_class = 2\n",
    "    pred_labels_val = np.argmax(pred_val, axis=1)\n",
    "    abstain_rate_val = np.mean(pred_labels_val == abstain_class)\n",
    "    abs_val[l] = abstain_rate_val\n",
    "    \n",
    "    pred_labels_test = np.argmax(pred_test, axis=1)\n",
    "    abstain_rate_test = np.mean(pred_labels_test == abstain_class)\n",
    "    abs_test[l] = abstain_rate_test\n",
    "    \n",
    "    pred_labels_train = np.argmax(pred_train, axis=1)\n",
    "    abstain_rate_train = np.mean(pred_labels_train == abstain_class)\n",
    "    abs_train[l] = abstain_rate_train\n",
    "    \n",
    "    if abstain_rate_val == 0.0:\n",
    "        print(\"BAD MODEL ITERATION\")\n",
    "        model_fail = model_fail+1\n",
    "        continue\n",
    "    elif abstain_rate_val != 0.0:\n",
    "        ########################################################################################################\n",
    "        ##STATISTICS CALCULATINNG\n",
    "        pred_class = []\n",
    "        predval_class = []\n",
    "        predtr_class = []\n",
    "        predtest_class = []\n",
    "\n",
    "        ##BRIER SKILL SCORE\n",
    "        BSS_all[l] = BSS(Y_all,pred)\n",
    "        BSS_val[l] = BSS(Y_val,pred_val)\n",
    "        BSS_train[l] = BSS(Y_tr,pred_train)\n",
    "        BSS_test[l] = BSS(Y_tes,pred_test) \n",
    "\n",
    "        ##RECALL ACCURACY SCORE    \n",
    "        RAS_two(l, Rec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Rec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Rec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Rec_test, Y_tes, pred_test, predtest_class)\n",
    "        ##PRECISION ACCURACY SCORE     \n",
    "        PAS_two(l, Prec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Prec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Prec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Prec_test, Y_tes, pred_test, predtest_class)\n",
    "        ########################################################################################################\n",
    "        ##ACC\n",
    "        ##TESTING\n",
    "        q90 = np.percentile(pred_test,90,axis=0) ##90th percentile of test\n",
    "        ##90th percentile acc\n",
    "        great90 = [i for i, row in enumerate(pred_test) if (row[0] > q90[0]) or (row[1] > q90[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test90 = pred_test[great90]\n",
    "        test90_norm = Y_tes[great90]\n",
    "        test90_acc.append(calculate_accuracy(test90_norm, test90, threshold=0.5))\n",
    "        ##full model\n",
    "        fulltest_acc.append(calculate_accuracy(Y_tes, pred_test, threshold=0.5))\n",
    "        ##Greater than 90 BSS\n",
    "        climo_90 = climo_test[great90]\n",
    "\n",
    "        BSS_90[l] = BSS(test90_norm,test90)\n",
    "        p_90.extend(test90)\n",
    "\n",
    "        ##############################3\n",
    "\n",
    "        #AVERAGE QUANT\n",
    "        qmin = np.min(pred_test,axis=0)\n",
    "        q30 = np.percentile(pred_test,30,axis=0) ##30th percentile of test\n",
    "        q70 = np.percentile(pred_test,70,axis=0) ##70th percentile of test\n",
    "        qmax = np.max(pred_test,axis=0)\n",
    "\n",
    "        ##mid percentile acc\n",
    "        mid = [i for i, row in enumerate(pred_test) if (row[0] >= q30[0] and row[0] <= q70[0] ) or (row[1] >= q30[1] and row[1] <= q70[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_mid = pred_test[mid]\n",
    "        test_mid_norm = Y_tes[mid]\n",
    "\n",
    "        BSS_avg[l] = BSS(test_mid_norm,test_mid)\n",
    "        p_mid.extend(test_mid)\n",
    "\n",
    "        #0-30\n",
    "        minimum = [i for i, row in enumerate(pred_test) if (row[0] >= qmin[0] and row[0] < q30[0] ) or (row[1] >= qmin[1] and row[1] < q30[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_minimum = pred_test[minimum]\n",
    "        test_minimum_norm = Y_tes[minimum]\n",
    "\n",
    "        BSS_30[l] = BSS(test_minimum_norm,test_minimum)\n",
    "        p_30.extend(test_minimum)\n",
    "\n",
    "        #70-100\n",
    "        maximum = [i for i, row in enumerate(pred_test) if (row[0] > q70[0] and row[0] <= qmax[0] ) or (row[1] > q70[1] and row[1] <= qmax[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_maximum = pred_test[maximum]\n",
    "        test_maximum_norm = Y_tes[maximum]\n",
    "\n",
    "        BSS_70[l] = BSS(test_maximum_norm,test_maximum)\n",
    "        p_70.extend(test_maximum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fail "
   ]
  },
  {
   "cell_type": "raw",
   "id": "97972fca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fold = 0\n",
    "for r in range(repeat):\n",
    "    print(\"REPEAT \",str(r+1))\n",
    "    tscv = TimeSeriesSplit(n_splits=split)\n",
    "    \n",
    "    for l, (train_index, val_index) in enumerate(tscv.split(X_tri)):\n",
    "        print(\"Fold \",str(l+1),\" in Repeat \",str(r+1))\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        X_tr = X_tri[train_index]\n",
    "        Y_tr = Y_tri[train_index]\n",
    "        \n",
    "        X_val = X_tri[val_index]\n",
    "        Y_val = Y_tri[val_index]\n",
    "        \n",
    "        climo_train = new_climo[train_index]\n",
    "        climo_val = new_climo[val_index]\n",
    "        climo_test = new_climo[frac_end:]\n",
    "\n",
    "        # Get class weights\n",
    "        ##PERSONAL MODIFICATION\n",
    "        class_weight = class_weight_creator(Y_tr)\n",
    "        \n",
    "        #initalize model\n",
    "        model = basic_LSTM(numb_int[0],numb_int[1], 64, 16, 16, 0.208, 0.177, 0.0006466)\n",
    "        model.fit(X_tr, Y_tr, validation_data=(X_val, Y_val),\n",
    "                  batch_size=batch_size, epochs=epochs, shuffle=shuffle,\n",
    "                  class_weight = class_weight, callbacks=[early_stopping])\n",
    "        \n",
    "        train_loss.append(model.history.history['loss'])\n",
    "        val_loss.append(model.history.history['val_loss'])\n",
    "        cat_acc.append(model.history.history['categorical_accuracy'])\n",
    "        val_acc.append(model.history.history['val_categorical_accuracy'])\n",
    "    \n",
    "        pred = model.predict(X_all)\n",
    "        pred_val = model.predict(X_val)\n",
    "        pred_train = model.predict(X_tr)\n",
    "        pred_test = model.predict(X_tes)\n",
    "\n",
    "        p_train.extend(pred_train)\n",
    "        p_val.extend(pred_val)\n",
    "        p_test.extend(pred_test)\n",
    "        ########################################################################################################\n",
    "        ##STATISTICS CALCULATINNG\n",
    "        pred_class = []\n",
    "        predval_class = []\n",
    "        predtr_class = []\n",
    "        predtest_class = []\n",
    "\n",
    "        ##BRIER SKILL SCORE\n",
    "        BSS_all[fold] = BSS(Y_all,pred)\n",
    "        BSS_val[fold] = BSS(Y_val,pred_val)\n",
    "        BSS_train[fold] = BSS(Y_tr,pred_train)\n",
    "        BSS_test[fold] = BSS(Y_tes,pred_test) \n",
    "    \n",
    "        ##RECALL ACCURACY SCORE    \n",
    "        RAS_two(fold, Rec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Rec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Rec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Rec_test, Y_tes, pred_test, predtest_class)\n",
    "        ##PRECISION ACCURACY SCORE     \n",
    "        PAS_two(fold, Prec_all, new_climo, Y_all, pred, pred_class,\n",
    "                climo_val, Prec_val, Y_val, pred_val, predval_class,\n",
    "                climo_train, Prec_train, Y_tr, pred_train, predtr_class,\n",
    "                climo_test, Prec_test, Y_tes, pred_test, predtest_class)\n",
    "        ########################################################################################################\n",
    "        def evaluate_90th(preds, labels, climo, acc_list, bss_arr):\n",
    "            q90 = np.percentile(preds, 90, axis=0)\n",
    "            high_conf_idx = [i for i, row in enumerate(preds) if (row[0] > q90[0]) or (row[1] > q90[1])]\n",
    "            preds_90 = preds[high_conf_idx]\n",
    "            labels_90 = labels[high_conf_idx]\n",
    "            climo_90 = climo[high_conf_idx]\n",
    "\n",
    "            acc_list.append(calculate_accuracy(labels_90, preds_90, threshold=0.5))\n",
    "            bss_arr[fold] = BSS(labels_90, preds_90)\n",
    "                \n",
    "        evaluate_90th(pred_train, Y_tr, climo_train, train90_acc, BSS_90)\n",
    "        fulltrain_acc.append(calculate_accuracy(Y_tr, pred_train, threshold=0.5))\n",
    "\n",
    "        evaluate_90th(pred_val, Y_val, climo_val, val90_acc, BSS_90)\n",
    "        fullval_acc.append(calculate_accuracy(Y_val, pred_val, threshold=0.5))\n",
    "\n",
    "        evaluate_90th(pred_test, Y_tes, climo_test, test90_acc, BSS_90)\n",
    "        fulltest_acc.append(calculate_accuracy(Y_tes, pred_test, threshold=0.5))\n",
    "\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767735e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.3, 0.3,13)\n",
    "counts, edges, = np.histogram(BSS_90[:], bins=bins)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(9,7))\n",
    "\n",
    "ax1.bar(edges[:-1], counts, width=np.diff(edges), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS of 90th Percentile',fontsize =13)\n",
    "ax1.set_ylabel('Number of Models',fontsize =13)\n",
    "plt.tick_params(labelsize = 12)\n",
    "ax1.set_title('BSS for 90th Percentile Predictions of Testing Data in 100 CAN CVs',fontsize =15)\n",
    "#plt.savefig(\"BSS90_eur14.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.3, 0.3,13)\n",
    "fs = 11\n",
    "\n",
    "counts1, edges1, = np.histogram(BSS_30[:], bins=bins)\n",
    "counts2, edges2, = np.histogram(BSS_avg[:], bins=bins)\n",
    "counts3, edges3, = np.histogram(BSS_70[:], bins=bins)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "ax1.bar(edges1[:-1], counts1, width=np.diff(edges1), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS 0-30th Percentiles', fontsize=fs+1)\n",
    "ax1.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges2[:-1], counts2, width=np.diff(edges2), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax2.set_xlabel('BSS Middle Percentiles', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax3.bar(edges3[:-1], counts3, width=np.diff(edges3), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax3.set_xlabel('BSS 70-100th Percentiles', fontsize=fs+1)\n",
    "#ax3.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "plt.suptitle('BSS of Predictions in Various Percentiles of Testing Data in 100 CAN CVs for Predicting +14 day European Temp Anomalies', fontsize=fs+3, x=0.525,y=0.95)\n",
    "#plt.savefig(\"BSSpercentile_eur14.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-0.3, 0.3,13)\n",
    "fs = 11\n",
    "\n",
    "counts1, edges1, = np.histogram(BSS_train[:], bins=bins)\n",
    "counts2, edges2, = np.histogram(BSS_val[:], bins=bins)\n",
    "counts3, edges3, = np.histogram(BSS_test[:], bins=bins)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "ax1.bar(edges1[:-1], counts1, width=np.diff(edges1), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS Training Data', fontsize=fs+1)\n",
    "ax1.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges2[:-1], counts2, width=np.diff(edges2), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax2.set_xlabel('BSS Validation Data', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax3.bar(edges3[:-1], counts3, width=np.diff(edges3), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax3.set_xlabel('BSS Testing Data', fontsize=fs+1)\n",
    "#ax3.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "plt.suptitle('BSS of Predictions Across All Datasets in 100 CAN CVs for Predicting +14 day European Temp Anomalies', fontsize=fs+3, x=0.525,y=0.95)\n",
    "#plt.savefig(\"BSSdatasets_eur14.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = np.array(p_test)\n",
    "p_test = p_test.reshape(((len(p_test),2)))\n",
    "\n",
    "p_train = np.array(p_train)\n",
    "p_train = p_train.reshape(((len(p_train),2)))\n",
    "\n",
    "p_val= np.array(p_val)\n",
    "p_val = p_val.reshape(((len(p_val),2)))\n",
    "\n",
    "p_mid= np.array(p_mid)\n",
    "p_mid = p_mid.reshape(((len(p_mid),2)))\n",
    "\n",
    "p_30= np.array(p_30)\n",
    "p_30 = p_30.reshape(((len(p_30),2)))\n",
    "\n",
    "p_70= np.array(p_70)\n",
    "p_70 = p_70.reshape(((len(p_70),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642af581",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.2, 0.8, 37)  #10 bins from 0.4 to 1\n",
    "fs = 11\n",
    "##bin the probabilities\n",
    "counts_neg_train, edges_neg_train = np.histogram(p_train[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(p_train[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(p_val[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(p_val[:,1], bins=bins)\n",
    "\n",
    "counts_neg_test, edges_neg_test = np.histogram(p_test[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(p_test[:,1], bins=bins)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(11,11))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative', fontsize=fs+2)\n",
    "ax1.set_xlabel('Probability in Training Data', fontsize=fs+1)\n",
    "ax1.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive', fontsize=fs+2)\n",
    "ax2.set_xlabel('Probability in Training Data', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax3.set_title('Negative Validation Predictions', fontsize=fs)\n",
    "ax3.set_xlabel('Probability in Validation Data', fontsize=fs+1)\n",
    "ax3.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax4.set_title('Positive Validation Predictions', fontsize=fs)\n",
    "ax4.set_xlabel('Probability in Validation Data', fontsize=fs+1)\n",
    "#ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax5.set_title('Negative Testing Predictions', fontsize=fs)\n",
    "ax5.set_xlabel('Probability in Testing Data', fontsize=fs+1)\n",
    "ax5.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax6.set_title('Positive Testing Predictions', fontsize=fs)\n",
    "ax6.set_xlabel('Probability in Testing Data', fontsize=fs+1)\n",
    "#ax6.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle(\"Probability Distributions of Various Datasets Across 100 CAN CVs for Predicting +14 day European Temp Anomalies\", fontsize=fs+4, x=0.525,y=0.99)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"probability_distrib_eur14.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.2, 0.8, 37)  #10 bins from 0.4 to 1\n",
    "fs = 11\n",
    "##bin the probabilities\n",
    "counts_neg_train, edges_neg_train = np.histogram(p_30[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(p_30[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(p_mid[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(p_mid[:,1], bins=bins)\n",
    "\n",
    "counts_neg_test, edges_neg_test = np.histogram(p_70[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(p_70[:,1], bins=bins)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(11,11))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative', fontsize=fs+2)\n",
    "ax1.set_xlabel('Probability in 0-30th Percentiles', fontsize=fs+1)\n",
    "ax1.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive', fontsize=fs+2)\n",
    "ax2.set_xlabel('Probability in 0-30th Percentiles', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax3.set_title('Negative Validation Predictions', fontsize=fs)\n",
    "ax3.set_xlabel('Probability in Middle Percentiles', fontsize=fs+1)\n",
    "ax3.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax4.set_title('Positive Validation Predictions', fontsize=fs)\n",
    "ax4.set_xlabel('Probability in Middle Percentiles', fontsize=fs+1)\n",
    "#ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax5.set_title('Negative Testing Predictions', fontsize=fs)\n",
    "ax5.set_xlabel('Probability in 70-100th Percentiles', fontsize=fs+1)\n",
    "ax5.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax6.set_title('Positive Testing Predictions', fontsize=fs)\n",
    "ax6.set_xlabel('Probability in 70-100th Percentiles', fontsize=fs+1)\n",
    "#ax6.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle(\"Probability Distributions for Percentiles of Testing Data Across 100 CAN CVs, +14 day European Temp Anomalies\", fontsize=fs+4, x=0.525,y=0.99)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"percentile_distrib_eur14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924361eb",
   "metadata": {},
   "source": [
    "### Model Curves. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e853bff5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "tloss = np.mean(train_loss, axis=0)\n",
    "vloss = np.mean(val_loss, axis=0)\n",
    "    \n",
    "cacc= np.mean(cat_acc, axis=0)\n",
    "vacc= np.mean(val_acc, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d6e5fef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(tloss, label='Training loss')\n",
    "ax1.plot(vloss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cacc, label='Training ACC')\n",
    "ax2.plot(vacc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend\n",
    "\n",
    "#plt.savefig(\"EUR_ACC_14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502793fb",
   "metadata": {},
   "source": [
    "### Skill Score Distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f366b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "font = 12\n",
    "#loop through each member\n",
    "ax2 = plt.subplot(2,1,1)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ticks = ['Full', 'Train', 'Val', 'Test'] #set tick numbers for dataset\n",
    "colors = ['lightcyan','peachpuff']\n",
    "ind = [2, 4, 6, 8]  # the x locations for the groups\n",
    "w = 0.25 #box-plot width\n",
    "labels = ['- Anom', '+ Anom '] #labels of quantiles\n",
    "\n",
    "\n",
    "##begin to go plot by plot ...\n",
    "#each plot has a separate plot function for each lead time. In these, the plots get each quantile plotted. \n",
    "ax2.set_title(\"Stat Scores for Predicting Europe Temp Anoms at 14-days Lead with Top 20 PCs in CAN\",fontsize = 14) \n",
    "\n",
    "##repeat the process\n",
    "a2_0 = ax2.boxplot([Rec_all[:,0],Rec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a2_5 = ax2.boxplot([Rec_train[:,0],Rec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a2_10 = ax2.boxplot([Rec_val[:,0],Rec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a2_14 = ax2.boxplot([Rec_test[:,0],Rec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax2.axhline(0.5, c='k', ls ='-.')\n",
    "ax2.set_xticks(ind, ticks)\n",
    "for bplot in (a2_0, a2_5, a2_10, a2_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a2_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax2.set_title('RAS for Temperature Region',fontsize = 14)\n",
    "ax2.set_ylim(-0.1,1.1)\n",
    "ax2.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax2.set_ylabel('RAS',fontsize = font)\n",
    "#ax2.set_xlabel('Data Set',fontsize = 14)\n",
    "#ax2.set_ylabel('Brier Skill Score')\n",
    "ax2.legend(loc = 'lower right', fontsize = 11)\n",
    "ax2.set_aspect('auto') ;\n",
    "\n",
    "a3_0 = ax3.boxplot([Prec_all[:,0],Prec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a3_5 = ax3.boxplot([Prec_train[:,0],Prec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a3_10 = ax3.boxplot([Prec_val[:,0],Prec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a3_14 = ax3.boxplot([Prec_test[:,0],Prec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax3.axhline(0.5, c='k', ls ='-.')\n",
    "ax3.set_xticks(ind, ticks)\n",
    "for bplot in (a3_0, a3_5, a3_10, a3_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a3_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax3.set_title('RAS Well')\n",
    "#ax3.set_title('PAS for Temperature Region',fontsize = 14)\n",
    "ax3.set_ylim(-0.1,1.1)\n",
    "ax3.set_ylabel('PAS',fontsize = font)\n",
    "ax3.set_xlabel('Data Set',fontsize = font)\n",
    "#ax3.set_ylabel('Recall Accuracy Score',fontsize = 14)\n",
    "ax3.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax3.legend(loc = 'lower right', fontsize = 11)\n",
    "ax3.set_aspect('auto') ;\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "#plt.savefig(\"EUR_StatScore_14.png\", bbox_inches='tight',dpi =150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d4256",
   "metadata": {},
   "source": [
    "### Percentile ACC Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0.4, 1,13)  # 10 bins from 0.4 to 1\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "##bin the testing 90th percentile accuracy data\n",
    "counts90, edges90 = np.histogram(test90_acc, bins=bins)\n",
    "countsfull, edgesfull = np.histogram(fulltest_acc, bins=bins)\n",
    "# Plot the full dataset\n",
    "#offset = 0.02  # Adjust this value if needed for better visibility\n",
    "plt.bar(edgesfull[:-1], countsfull, width=np.diff(edgesfull), \n",
    "        edgecolor='black', alpha=0.5, label='All Predictions', align='edge', color='blue')\n",
    "# Plot the 90th percentile\n",
    "plt.bar(edges90[:-1], counts90, width=np.diff(edges90), \n",
    "        edgecolor='black', alpha=0.5, label='>90th Percentile Predictions', align='edge', color = 'lightblue')\n",
    "plt.xlabel('Accuracy', fontsize =13)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.ylabel('Number of Models', fontsize =13)\n",
    "plt.legend(fontsize = 11)\n",
    "plt.title('Testing Prediction Accuracy Across 100 CAN Models for +14 day Europe Temp Anomalies', fontsize =15)\n",
    "#plt.savefig(\"EUR14_ACCtesting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49438c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
