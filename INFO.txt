                                                                     File initialized May 8, 2025: 

This is the master folder for the "forecasts of opportunity using EOFs" research, examining stratosphere-troposphere dynamic coupling. 
There are multiple sub-folders 

As a note, after conducting EOF analysis, the following PCs were selected, accounting for 90% of the variance for each feature:

U-wind:
30 days = 10
20 days = 10
14 days = 10
###########

EHF:
30 days = 45
20 days = 45
14 days = 45
###########

GPH:
30 days = 10
20 days = 10
14 days = 10
####################################################
May 9th

I started the feature selection for each lead time.
So far, it is looking like the PCs do vary based on region, BUT it is not clear yet whether they also vary based on lead time. TBD. 

Need to re-run these, ignoring the first PC (spin-up of the vortex).
###################################################
May 12th

All finished on feature selection for now, me thinks. 
LEADTIME 14: 
##Europe
Index([1, 56, 0, 54, 9, 57, 55, 3, 26, 4], dtype='int64')
Accuracy, Validation: 70.32%
Accuracy, Training: 71.62%
Accuracy, Testing: 70.81%

##Nova
Index([1, 56, 0, 55, 2, 53, 54, 9, 4, 22], dtype='int64')
Accuracy, Validation: 80.75%
Accuracy, Training: 82.18%
Accuracy, Testing: 82.67%

##South
Index([1, 56, 54, 53, 55, 4, 3, 21, 14, 0], dtype='int64')
Accuracy, Validation: 68.74%
Accuracy, Training: 70.58%
Accuracy, Testing: 67.67%
###########################
LEADTIME 20:
##Europe
Index([1, 56, 54, 0, 17, 9, 57, 2, 16, 33], dtype='int64')
Accuracy, Validation: 68.36%
Accuracy, Training: 70.84%
Accuracy, Testing: 72.36%

##Nova
Index([1, 56, 0, 55, 54, 4, 2, 53, 57, 9], dtype='int64')
Accuracy, Validation: 79.51%
Accuracy, Training: 81.91%
Accuracy, Testing: 83.31%

##South
Index([1, 56, 54, 53, 57, 4, 16, 14, 17, 0], dtype='int64')
Accuracy, Validation: 67.90%
Accuracy, Training: 70.11%
Accuracy, Testing: 66.84%
###########################
LEADTIME 30:
Index([1, 56, 54, 49, 53, 2, 19, 55, 0, 18], dtype='int64')
Accuracy, Validation: 68.28%
Accuracy, Training: 70.94%
Accuracy, Testing: 71.34%


##Nova
Index([1, 56, 0, 54, 55, 4, 53, 57, 9, 2], dtype='int64')
Accuracy, Validation: 78.53%
Accuracy, Training: 80.45%
Accuracy, Testing: 80.74%

##South
Index([3, 0, 1, 53, 9, 56, 17, 13, 28, 54], dtype='int64')
Accuracy, Validation: 68.85%
Accuracy, Training: 69.35%
Accuracy, Testing: 67.25%

###################################################
May 27th 

I have returned ... finally. For better or for worse. 
I made a new graphcast job for this with the older version of tensorflow that is compatible with kerastuner. 

I see that I have mostly completed the feature selection, but I am going to go back and make sure this was done correctly. 
I did end up removing the first PC, but that has made the indexing confusing for no good reason lol. 
After this is completed, I will go timestep by timestep to make the figures of PCs. 

Then I will start with training a model for 14 days. Hopefully the hyperparameters found in tuning this model translate to other timesteps. 

(I finished the PC images today so I will start model tomorrow)

Comparing PCs from each area at leadtime
For U-wind at 14 days=
Europe: 3,2,5,6
Nova: 3,2,4,6
SEUS: 3,6,5,2

For U-wind at 20 days=
Europe: 3,2,4
Nova: 3,2,4,6
SEUS: 3,6,2

For U-wind at 30 days=
Europe: 3,4,2
Nova: 3,2,6,4
SEUS: 5,2,3
````````````````````````````````````
For EHF at 14 days=
Europe: 2, 19
Nova: 2, 15
SEUS: 14, 7

For EHF at 20 days=
Europe: 2, 10, 9, 26
Nova: 2
SEUS: 9, 7, 10

For EHF at 30 days=
Europe: 42, 12, 11 
Nova: 2
SEUS: 2, 10, 6, 21
````````````````````````````````````
For GPH at 14 days=
Europe: 5,3,6,4
Nova: 5,4,2,3
SEUS: 5,3,2,4

For GPH at 20 days=
Europe: 5,3,6
Nova: 5,4,3,2,6
SEUS: 5,3,2,4

For GPH at 30 days=
Europe: 5,3,2,4
Nova: 5,3,4,2,6
SEUS: 2,5,3 
###################################################

May 28th

I will start building the LSTM model today. I need to with one region and one lead time, then see if the model for that lead time translates to other regions. 
The question will be whether there is an observable change in the skill of the model between regions & changes to the foreasts of opportunity. 

Let's start with Europe at 14 days because ... of course. 

Models look good. I am going to have to conduct tuning by region, which sucks. 
I am starting with Europe at 14 days in its own model.

Tuning is occurring. I cannot run these at the same time. 
#######################################################

May 30th
I submitted my first manuscript this morning for the ellipse metrics (yay)
I have been otherwise working on the model hyperparameter tuning. 

I cannot tell if one architecture works for all methods. I am going to check using one of the tuned model architectures for the Nova Scotia region. 

The final model hyperparameters that worked for all regions at 14 days leadtime were: 
model = basic_LSTM(numb_int[0],numb_int[1], 32, 4, 24, 0.53, 0.26, 0.0016)
n1 = 32, 
n2 = 4, 
n3 = 24, 
rl1, rl2 = 0.53, 
rl3 = 0.26, 
lr = 0.0016

#######################################################

June 2nd. 

I am trying to ge the model to work a little better. i noticed an overfitting issue with the model during CV, so I am trying to intorduce a component that uses random selection of validation data into hyperparameter tuning in an effort to reduce this. Essentially this is just a loop for random selection+the tuning and then collectively examining the parameters. 

I am also looking into getting SHAP working for these models... obviously my model architecture has created issue because I have not shut off eager execution .... because I have no real reason to. 

######################################################

Jun 3rd. 

After some trouble shooting, it appears that I was incorrectly calculating the baseline for calculating BSS. I have fixed that. My model is NOT underperforming as much as I thought. 

I also tried an alternative method for getting hyperparameters for the model via k-fold cross validation. I may have to look into that for the 100 CVs of the model but I am running that as is for now to see what it does. I am still working with Europe at 14 days lead time. I reduced the forecast window to 10 days rather than 14 to increase my sample size marginally. The leadtime is still 14 days. That did NOT change. 

######################################################

June 4th.
Redoing the Solvers and feature selection.
I added in a new folder to designate ranges for the PCs and temps based on leadtime just to make my life a hair easier in the future. 
#149 for 14
#143 for 20
#133 for 30

for reference for doing the LSTM ... 
I am re-calculating the feature selection for a few reasons. 
1) to get more features. 
2) to get a look at the histograms by region and lead time. 

########################################################

June 5th. 

I finished redoing the feature selection for the updated dates/times last night. 
More or less, what is seen is that the model itself matters a small amount, but the predictands matter more. The region you are forecasting for is really important and the model actually does *better* at predicting here. 

I have a sneaking suspicion that this is being driven by the model itself. 
We already know that the model stuggles with differentiating between cold and neutral, middle ground events ... Maybe the reason why it excels so well in Canada is because there is more distinct positive temperature responses that are easier for the model to identify... so it struggles less with the neutral cases. 

14-days
#EUROPE
#Int64Index([1, 56, 0, 54, 9, 57, 2, 31, 26, 14, 20, 6, 11, 38, 21, 18, 15, 22, 12, 13], dtype='int64')
#Accuracy, Validation: 67.84%
#Accuracy, Training: 67.95%
#Accuracy, Testing: 66.99%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 53, 22, 57, 9, 25, 11, 4, 54, 38, 17, 30, 36, 3, 14, 26], dtype='int64')
#Accuracy, Validation: 78.23%
#Accuracy, Training: 79.39%
#Accuracy, Testing: 80.41%

##SE US
#Int64Index([56, 1, 54, 18, 53, 3, 0, 10, 5, 19, 21, 14, 2, 57, 13, 60, 22, 26,25, 11], dtype='int64')
#Accuracy, Validation: 64.35%
#Accuracy, Training: 64.83%
#Accuracy, Testing: 62.45%
############################

20-days 
#EUROPE
#Int64Index([1, 56, 54, 33, 17, 2, 9, 57, 0, 31, 20, 16, 53, 19, 26, 36, 18, 21, 22, 5],dtype='int64')
#Accuracy, Validation: 65.59%
#Accuracy, Training: 67.08%
#Accuracy, Testing: 66.75%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 4, 57, 53, 25, 17, 22, 54, 13, 26, 38, 9, 12, 30, 14, 37],dtype='int64')
#Accuracy, Validation: 77.94%
#Accuracy, Training: 79.20%
#Accuracy, Testing: 80.59%

##SE US
#Int64Index([1, 56, 54, 5, 0, 14, 10, 16, 53, 18, 37, 13, 59, 19, 57, 3, 49, 17,25, 11],dtype='int64')
#Accuracy, Validation: 62.96%
#Accuracy, Training: 63.67%
#Accuracy, Testing: 60.22%
############################

30-days
#EUROPE
#IInt64Index([1, 56, 18, 2, 54, 53, 19, 49, 24, 48, 12, 8, 0, 25, 55, 17, 38, 60, 23, 5],dtype='int64')
#Accuracy, Validation: 64.38%
#Accuracy, Training: 65.52%
#Accuracy, Testing: 65.71%

##CANADA
#Int64Index([56, 1, 0, 55, 2, 57, 4, 54, 22, 6, 53, 26, 25, 30, 9, 18, 13, 17, 3, 38],dtype='int64')
#Accuracy, Validation: 76.72%
#Accuracy, Training: 77.42%
#Accuracy, Testing: 77.12%

##SE US
#Int64Index([0, 3, 9, 53, 56, 13, 1, 49, 25, 17, 54, 7, 18, 19, 4, 59, 57, 5,14, 11],dtype='int64')
#Accuracy, Validation: 63.90%
#Accuracy, Training: 64.13%
#Accuracy, Testing: 61.35%

########################################################

June 10th
I have been working on doing the model testing to make sure that the model is actually, ya know, working correctly. 

########################################################

June 11th
After essentially checking on whether or not the model is struggling with the middle category (it is), I have been able to decide that I need to do the Controlled Abstention network.

########################################################

June 17th

After spending the better part of the last week working on integrating the CAN into my work, as well as tuning it, I have gotten to a point where it is functioning. I am now running CV in my silly manner with the intention of trying to duplicate the images seen in the regular cross entropy loss model that I have traditionally used (ala Tk2 LSTM SS file).

Also, yay, I finally took the time to learn how to initalize a connection here with Github so that my work is recorded in some form or another outside of this cloud environment. 

########################################################

June 18th
Ultimately decided I may not be using the abstention loss 100% correctly; so moving on to trying to use XAI on the cross entropy model 

I did run the CV on the CAN model once more just to see what that outcome looks like.

Started SHAP code.
########################################################

June 20th

Create beeswarm SHAP plots. Added in SHAP to CV loop of normal LSTM model. 
########################################################

June 23rd

Updated SHAP CV to fix plot creation. Made and saved these plots for Europe at 14 days. Will look into feature behaviors during events tomorrow by adding additional components for saving event indexes. 
I can check 90th percentile or 30th,70th, whatever perentile events for confidence and correctness, as well as physical significance. 

