{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4e4657",
   "metadata": {},
   "source": [
    "### Modified PVU/GPH FCNN model file from November of 2024 to meet the needs of my current work. \n",
    "\n",
    "This will be a three-input merged model of flattened vertical cross sections for Zonal-mean wind, GPH, and Zonal-mean meridional eddy heat flux. \n",
    "\n",
    "File initalized on June 25, 2025. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9089138",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "!pip install innvestigate\n",
    "!pip install investigate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38df94c",
   "metadata": {},
   "source": [
    "First two cells are for imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f02ae73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams #For changing text properties\n",
    "from cartopy import crs as ccrs #Useful for plotting maps\n",
    "import cartopy.util #Requires separate import\n",
    "from cartopy.util import add_cyclic_point\n",
    "import cartopy.feature as cf\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import matplotlib.path as mpath\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1d1f46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:00:49.333957: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-25 23:00:49.336010: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-25 23:00:49.362943: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-25 23:00:49.362964: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-25 23:00:49.362984: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-25 23:00:49.368326: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-25 23:00:49.368697: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "##import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "#necessary for this model because I need to use LRP for XAI. \n",
    "#import innvestigate\n",
    "import investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c613735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb9f7e",
   "metadata": {},
   "source": [
    "#### Upload data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e413cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load input data\n",
    "infile = open(\"../eof_data/vertanom_u.p\",\"rb\",)\n",
    "u_input = pickle.load(infile)  ##vertical U cross section\n",
    "#flat_u_14 = u_input[:62, 19:168, :, :].reshape((62 * 149, 37, 36))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../eof_data/vertanom_ehf.p\",\"rb\",)\n",
    "ehf_input = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "#flat_EHF_14 = ehf_input[:62, 19:168, :, :].reshape((62 * 149, 37, 180))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../eof_data/vertanom_gph.p\",\"rb\",)\n",
    "gph_input = pickle.load(infile)  ##GPH vertical cross section along longitudes\n",
    "#flat_GPH_14= gph_input[:62, 19:168, :, :].reshape((62 * 149, 37, 180))\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd017617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Wind shape:  (62, 149, 37, 36)\n",
      "EHF shape:  (62, 149, 37, 180)\n",
      "GPH shape:  (62, 149, 37, 180)\n"
     ]
    }
   ],
   "source": [
    "u_14 = u_input[:62, 19:168, :, :]\n",
    "EHF_14 = ehf_input[:62, 19:168, :, :]\n",
    "GPH_14= gph_input[:62, 19:168, :, :]\n",
    "\n",
    "print(\"U Wind shape: \",u_14.shape)\n",
    "print(\"EHF shape: \",EHF_14.shape)\n",
    "print(\"GPH shape: \",GPH_14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc739d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load output data\n",
    "infile = open(\"../reduced_data/temps/eur_14.p\",\"rb\",)\n",
    "output = pickle.load(infile) \n",
    "output = output.reshape(62, 149)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a802a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 149)\n"
     ]
    }
   ],
   "source": [
    "temp = output[:,:]\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3c443",
   "metadata": {},
   "source": [
    "The next two cells are only relevant for lead times >14 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb82c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead = 149"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aec98254",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "input_u =  np.empty((62,lead,37,36))\n",
    "input_ehf =  np.empty((62,lead,37,180))\n",
    "input_gph =  np.empty((62,lead,37,180))\n",
    "\n",
    "temp_n = np.empty((62,lead))\n",
    "\n",
    "for i in range(0,62):\n",
    "    for j in range(0,lead):\n",
    "        input_u[i,j,:,:] = u_14[i,j,:,:]\n",
    "        temp_n[i,j] = temp[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "063092cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be commented out if not needed. \n",
    "input_u =  u_14\n",
    "input_ehf =  EHF_14\n",
    "input_gph =  GPH_14\n",
    "temp_n = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d57a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 149, 37, 36)\n",
      "(62, 149, 37, 180)\n",
      "(62, 149, 37, 180)\n"
     ]
    }
   ],
   "source": [
    "##input shape (samples, lat, lon) and (#samples, window, features)\n",
    "print(input_u.shape)\n",
    "print(input_ehf.shape)\n",
    "print(input_gph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea129e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62, 149)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##output shape (samples), these will be different from the input\n",
    "temp_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0633c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7852"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frac_ind = round((62*lead)*0.15)\n",
    "frac_end = round((62*lead)-frac_ind)\n",
    "frac_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14cdb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all1 = np.copy(input_u.reshape(62*lead,37*36)) ##u\n",
    "X_all2 = np.copy(input_ehf.reshape(62*lead,37*180)) ##ehf\n",
    "X_all3 = np.copy(input_gph.reshape(62*lead,37*180)) ##gph\n",
    "\n",
    "Y_all = np.copy(temp_n.reshape(62*lead))\n",
    "\n",
    "##training data partition out\n",
    "X_tri1 = X_all1[:frac_end,:] ##u\n",
    "X_tri2 = X_all2[:frac_end,:] ##ehf\n",
    "X_tri3 = X_all3[:frac_end,:] ##gph\n",
    "\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes1 = X_all1[frac_end:,:]\n",
    "X_tes2 = X_all2[frac_end:,:]\n",
    "X_tes3 = X_all3[frac_end:,:]\n",
    "\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "#Convert the Y array into a categorical array. This means we will create one-hot vector labels for all of the inputs.\n",
    "# The one-hot vectors have an index for each possible output category (two in our case)\n",
    "# A \"1\" is put in the index corresponding to the category to which the sample belongs\n",
    "Y_all = keras.utils.to_categorical(Y_all)\n",
    "Y_tri = keras.utils.to_categorical(Y_tri)\n",
    "Y_tes= keras.utils.to_categorical(Y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e167fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(X_all2)) or np.any(np.isinf(X_all2)):\n",
    "    print(\"NaN or Inf values found in X_all!\")\n",
    "\n",
    "if np.any(np.isnan(Y_all)) or np.any(np.isinf(Y_all)):\n",
    "    print(\"NaN or Inf values found in Y_all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666ba63",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cb72c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1386"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##set 15% of data for validation\n",
    "frac_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c0c08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9238, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##checking to make sure shape was properly one hot encoded\n",
    "Y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8a0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##class weight creator for the instance where now I have a four dimensional output array \n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    Y_reshaped = Y.reshape(-1, Y.shape[-1])\n",
    "    weights = np.max(np.sum(Y_reshaped, axis=0)) / np.sum(Y_reshaped, axis=0)\n",
    "    for i in range(Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3bc721",
   "metadata": {},
   "outputs": [],
   "source": [
    "##number of input nodes\n",
    "input_shape1=  X_all1.shape[-1] # u\n",
    "input_shape2= X_all2.shape[-1] ## ehf\n",
    "input_shape3= X_all3.shape[-1] ## gph\n",
    "\n",
    "\n",
    "##fraction of training data\n",
    "X_validation1 = X_tri1[0:frac_ind]\n",
    "X_validation2 = X_tri2[0:frac_ind]\n",
    "X_validation3 = X_tri3[0:frac_ind]\n",
    "Y_validation = Y_tri[0:frac_ind]\n",
    "        \n",
    "X_train1 = X_tri1[frac_ind:len(X_tri1)]\n",
    "X_train2 = X_tri2[frac_ind:len(X_tri2)]\n",
    "X_train3 = X_tri3[frac_ind:len(X_tri3)]\n",
    "Y_train = Y_tri[frac_ind:len(Y_tri)]\n",
    "\n",
    "X_test1 = X_tes1\n",
    "X_test2 = X_tes2\n",
    "X_test3 = X_tes3\n",
    "Y_test = Y_tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff89e4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6466, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd1fcefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ecf1749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6660"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40dc7ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6660"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ab727ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0, 1: 1.5238096}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##do the class_dict weights\n",
    "class_weight = class_weight_creator(Y_train)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffce0d44",
   "metadata": {},
   "source": [
    "### Model Arcitecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35cd64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined(n1, n2, n3, n4, n5, n6, n7, reg1, reg2, reg3, reg4, reg5, reg6, reg7, learn):\n",
    "    ##first layers for U ANN\n",
    "    input_tensor1 = Input(shape=(int(1332),), name='layer1')\n",
    "    layer1 = layers.Dense(n1, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg1))(input_tensor1)\n",
    "   \n",
    "\n",
    "    layer1 = layers.Dense(n2, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg2))(layer1)\n",
    "   \n",
    "    ## second layers for EHF ANN\n",
    "    input_tensor2 = Input(shape=(int(6660),), name='layer2')\n",
    "    layer2 = layers.Dense(n3, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg3))(input_tensor2)\n",
    "    \n",
    "    layer2 = layers.Dense(n4, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg4))(layer2)\n",
    "    \n",
    "    \n",
    "    ## third layers for GPH ANN\n",
    "    input_tensor3 = Input(shape=(int(6660),), name='layer3')\n",
    "    layer3 = layers.Dense(n5, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg5))(input_tensor3)\n",
    "    \n",
    "    layer3 = layers.Dense(n6, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg6))(layer3)\n",
    "    \n",
    "    ##combine two model inputs\n",
    "    merge_layer = Concatenate()([layer1,layer2,layer3])\n",
    "\n",
    "    ##more dense layers\n",
    "    merge = layers.Dense(n7, activation='leaky_relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg7))(merge_layer)\n",
    "\n",
    "    #decay_rate = learn / epochs\n",
    "    output_tensor = layers.Dense(2,activation='softmax')(merge)\n",
    "    model = Model([input_tensor1, input_tensor2, input_tensor3], output_tensor)\n",
    "    model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=learn),\n",
    "              loss = 'categorical_crossentropy', #Our loss function is based on categorical error\n",
    "              metrics=[keras.metrics.categorical_accuracy], #We will print out the categorical accuracy as the network is trained\n",
    "              )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2f7df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "##dictionaries for training and validation\n",
    "X_train = {'layer1': X_train1, 'layer2': X_train2, 'layer3': X_train3}\n",
    "X_all = {'layer1': X_all1, 'layer2': X_all2, 'layer3': X_all3}\n",
    "X_test = {'layer1': X_test1, 'layer2': X_test2, 'layer3': X_test3}\n",
    "X_val = {'layer1': X_validation1, 'layer2': X_validation2, 'layer3': X_validation3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b6fbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "batch_size = 256 #The number of samples the network sees before it backpropagates (batch size)\n",
    "epochs = 50 #The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a84644db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1386, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e65b37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6759fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create model.\n",
    "model = combined(n1 = 128, \n",
    "                 n2 = 16, \n",
    "                 n3 = 128, \n",
    "                 n4 = 16, \n",
    "                 n5 = 128,\n",
    "                 n6 = 16,\n",
    "                 n7 = 24,\n",
    "                 reg1 = 0.05, \n",
    "                 reg2 = 0.01, \n",
    "                 reg3 = 0.05, \n",
    "                 reg4 = 0.01,\n",
    "                 reg5 = 0.05, \n",
    "                 reg6 = 0.01,\n",
    "                 reg7 = 0.05,\n",
    "                 learn =0.11 )\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34aa39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6466 samples, validate on 1386 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:33:24.762005: W tensorflow/c/c_api.cc:305] Operation '{name:'training/Adam/dense_1/bias/v/Assign' id:769 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_1/bias/v, training/Adam/dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6466/6466 [==============================] - 1s 152us/sample - loss: 66947.4412 - categorical_accuracy: 0.5306 - val_loss: 35610.7556 - val_categorical_accuracy: 0.3817\n",
      "Epoch 2/50\n",
      " 256/6466 [>.............................] - ETA: 0s - loss: 36587.2031 - categorical_accuracy: 0.4141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6466/6466 [==============================] - 1s 97us/sample - loss: 15175.1365 - categorical_accuracy: 0.5320 - val_loss: 4437.4499 - val_categorical_accuracy: 0.6760\n",
      "Epoch 3/50\n",
      "6466/6466 [==============================] - 1s 101us/sample - loss: 4174.8329 - categorical_accuracy: 0.5631 - val_loss: 1255.1370 - val_categorical_accuracy: 0.6782\n",
      "Epoch 4/50\n",
      "6466/6466 [==============================] - 1s 96us/sample - loss: 950.7457 - categorical_accuracy: 0.5820 - val_loss: 431.5856 - val_categorical_accuracy: 0.6566\n",
      "Epoch 5/50\n",
      "6466/6466 [==============================] - 1s 106us/sample - loss: 336.4384 - categorical_accuracy: 0.6033 - val_loss: 214.6969 - val_categorical_accuracy: 0.5058\n",
      "Epoch 6/50\n",
      "6466/6466 [==============================] - 1s 96us/sample - loss: 166.1512 - categorical_accuracy: 0.6010 - val_loss: 174.7398 - val_categorical_accuracy: 0.3947\n",
      "Epoch 7/50\n",
      "6466/6466 [==============================] - 1s 106us/sample - loss: 122.4632 - categorical_accuracy: 0.6050 - val_loss: 85.7095 - val_categorical_accuracy: 0.5209\n",
      "Epoch 8/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 71.3883 - categorical_accuracy: 0.6052 - val_loss: 58.6841 - val_categorical_accuracy: 0.6479\n",
      "Epoch 9/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 50.1693 - categorical_accuracy: 0.5985 - val_loss: 41.8302 - val_categorical_accuracy: 0.6797\n",
      "Epoch 10/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 38.1504 - categorical_accuracy: 0.6024 - val_loss: 40.4395 - val_categorical_accuracy: 0.3882\n",
      "Epoch 11/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 32.7515 - categorical_accuracy: 0.5830 - val_loss: 26.9207 - val_categorical_accuracy: 0.5866\n",
      "Epoch 12/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 24.9574 - categorical_accuracy: 0.5943 - val_loss: 22.0117 - val_categorical_accuracy: 0.5765\n",
      "Epoch 13/50\n",
      "6466/6466 [==============================] - 1s 106us/sample - loss: 20.5991 - categorical_accuracy: 0.6466 - val_loss: 18.8805 - val_categorical_accuracy: 0.6479\n",
      "Epoch 14/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 18.1003 - categorical_accuracy: 0.6347 - val_loss: 17.1265 - val_categorical_accuracy: 0.5310\n",
      "Epoch 15/50\n",
      "6466/6466 [==============================] - 1s 106us/sample - loss: 16.2433 - categorical_accuracy: 0.6426 - val_loss: 15.4019 - val_categorical_accuracy: 0.5245\n",
      "Epoch 16/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 14.8045 - categorical_accuracy: 0.6695 - val_loss: 14.1688 - val_categorical_accuracy: 0.5952\n",
      "Epoch 17/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 13.8081 - categorical_accuracy: 0.6714 - val_loss: 13.3009 - val_categorical_accuracy: 0.6544\n",
      "Epoch 18/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 13.0668 - categorical_accuracy: 0.6686 - val_loss: 12.6727 - val_categorical_accuracy: 0.6025\n",
      "Epoch 19/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 12.4835 - categorical_accuracy: 0.6689 - val_loss: 12.1896 - val_categorical_accuracy: 0.6609\n",
      "Epoch 20/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 12.0211 - categorical_accuracy: 0.6791 - val_loss: 11.6880 - val_categorical_accuracy: 0.6609\n",
      "Epoch 21/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 11.6156 - categorical_accuracy: 0.6743 - val_loss: 11.3407 - val_categorical_accuracy: 0.6775\n",
      "Epoch 22/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 11.3001 - categorical_accuracy: 0.6737 - val_loss: 11.0157 - val_categorical_accuracy: 0.6724\n",
      "Epoch 23/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 11.0199 - categorical_accuracy: 0.6766 - val_loss: 10.8062 - val_categorical_accuracy: 0.6169\n",
      "Epoch 24/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 10.7591 - categorical_accuracy: 0.6752 - val_loss: 10.5375 - val_categorical_accuracy: 0.7013\n",
      "Epoch 25/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 10.5962 - categorical_accuracy: 0.6607 - val_loss: 10.3640 - val_categorical_accuracy: 0.6962\n",
      "Epoch 26/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 10.4712 - categorical_accuracy: 0.6573 - val_loss: 10.3456 - val_categorical_accuracy: 0.6198\n",
      "Epoch 27/50\n",
      "6466/6466 [==============================] - 1s 106us/sample - loss: 10.3403 - categorical_accuracy: 0.6417 - val_loss: 10.0746 - val_categorical_accuracy: 0.6573\n",
      "Epoch 28/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 10.1230 - categorical_accuracy: 0.6543 - val_loss: 9.8559 - val_categorical_accuracy: 0.6883\n",
      "Epoch 29/50\n",
      "6466/6466 [==============================] - 1s 107us/sample - loss: 10.0030 - categorical_accuracy: 0.6593 - val_loss: 9.8499 - val_categorical_accuracy: 0.5988\n",
      "Epoch 30/50\n",
      "6466/6466 [==============================] - 1s 104us/sample - loss: 9.8856 - categorical_accuracy: 0.6409 - val_loss: 9.6898 - val_categorical_accuracy: 0.6753\n",
      "Epoch 31/50\n",
      "6466/6466 [==============================] - 1s 105us/sample - loss: 9.7583 - categorical_accuracy: 0.6526 - val_loss: 9.5616 - val_categorical_accuracy: 0.6797\n",
      "Epoch 32/50\n",
      "2560/6466 [==========>...................] - ETA: 0s - loss: 9.6947 - categorical_accuracy: 0.6461"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_validation),\n",
    "          batch_size=batch_size, epochs=epochs, shuffle=shuffle,class_weight = class_weight, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cf0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=model.history.history['loss']\n",
    "val_loss=model.history.history['val_loss']\n",
    "    \n",
    "cat_acc=model.history.history['categorical_accuracy']\n",
    "val_acc=model.history.history['val_categorical_accuracy']\n",
    "        \n",
    "pred = model.predict(X_all)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Look at the optimization history\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_loss, label='Training loss')\n",
    "ax1.plot(val_loss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cat_acc, label='Training ACC')\n",
    "ax2.plot(val_acc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505ee33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_classes = np.argmax(y_val_pred, axis=1)\n",
    "y_val_true = np.argmax(Y_validation, axis=1)\n",
    "\n",
    "print(classification_report(y_val_true, y_val_classes, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b7ec6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert one-hot labels to 1D integer labels (0 or 1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Extract predicted probability for the positive class (class 1)\n",
    "y_prob = pred_test[:,1]\n",
    "\n",
    "# Create climatology baseline: constant probability = mean positive class rate\n",
    "p_climatology = np.full_like(y_true, y_true.mean(), dtype=float)\n",
    "\n",
    "# Compute Brier Scores\n",
    "bs_model = brier_score_loss(y_true, y_prob)\n",
    "bs_climo = brier_score_loss(y_true, p_climatology)\n",
    "\n",
    "# Compute Brier Skill Score\n",
    "bss = 1 - (bs_model / bs_climo)\n",
    "\n",
    "print(f\"Brier Score (Model): {bs_model:.4f}\")\n",
    "print(f\"Brier Score (Climatology): {bs_climo:.4f}\")\n",
    "print(f\"Brier Skill Score: {bss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd35da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 37)  #10 bins from 0.4 to 1\n",
    "##bin the probabilities\n",
    "counts_neg_test, edges_neg_test = np.histogram(pred_test[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(pred_test[:,1], bins=bins)\n",
    "\n",
    "counts_neg_train, edges_neg_train = np.histogram(pred_train[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(pred_train[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(pred_val[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(pred_val[:,1], bins=bins)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative Training Predictions', fontsize=13)\n",
    "ax1.set_xlabel('Probability')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive Training Predictions', fontsize=13)\n",
    "ax2.set_xlabel('Probability')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax3.set_title('Negative Validation Predictions', fontsize=13)\n",
    "ax3.set_xlabel('Probability')\n",
    "ax3.set_ylabel('Count')\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax4.set_title('Positive Validation Predictions', fontsize=13)\n",
    "ax4.set_xlabel('Probability')\n",
    "ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax5.set_title('Negative Testing Predictions', fontsize=13)\n",
    "ax5.set_xlabel('Probability')\n",
    "ax5.set_ylabel('Count')\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax6.set_title('Positive Testing Predictions', fontsize=13)\n",
    "ax6.set_xlabel('Probability')\n",
    "ax6.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61666e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################tuning#############################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a53c6782",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def combined(hp):\n",
    "    ##set ranges for hyperparameters\n",
    "    n1 = hp.Int('n1', min_value=120, max_value=128, step=4)\n",
    "    n2 = hp.Int('n2', min_value=4, max_value=16, step=4)\n",
    "    n3= hp.Int('n3', min_value=120, max_value=128, step=4)\n",
    "    n4 = hp.Int('n4', min_value=4, max_value=16, step=4)\n",
    "    n5 = hp.Int('n5', min_value=4, max_value=32, step=4)\n",
    "   \n",
    "\n",
    "    reg1 = hp.Float('reg1', min_value=0.01, max_value=1, default=0.01)\n",
    "    reg2 = hp.Float('reg2', min_value=0.001, max_value=0.1, default=0.01)\n",
    "    reg3 = hp.Float('reg3', min_value=0.01, max_value=1, default=0.01)\n",
    "    reg4 = hp.Float('reg4', min_value=0.001, max_value=0.1, default=0.01)\n",
    "    reg5 = hp.Float('reg5', min_value=0.001, max_value=0.1, default=0.01)\n",
    "    \n",
    "    ##first layers for ANN\n",
    "    input_tensor1 = Input(shape=(int(1595),), name='layer1')\n",
    "    layer1 = layers.Dense(n1, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg1))(input_tensor1)\n",
    "   \n",
    "    layer1 = layers.Dense(n2, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg2))(input_tensor1)\n",
    "    \n",
    "\n",
    "    ## second layers for ANN\n",
    "    input_tensor2 = Input(shape=(int(777),), name='layer2')\n",
    "    layer2 = layers.Dense(n3, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg3))(input_tensor2)\n",
    "    layer2 = layers.Dense(n4, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg4))(input_tensor2)\n",
    "    ##return_sequence = False\n",
    "    \n",
    "    \n",
    "    ##combine two model inputs\n",
    "    merge_layer = Concatenate()([layer1,layer2])\n",
    "\n",
    "    ##more dense layers\n",
    "    merge = layers.Dense(n5, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg5))(merge_layer)\n",
    "\n",
    "    #decay_rate = learn / epochs\n",
    "    lr = hp.Float('learning_rate', min_value=1e-3, max_value=1, default=1e-3)\n",
    "    output_tensor = layers.Dense(2,activation='softmax')(merge)\n",
    "    model = Model([input_tensor1, input_tensor2], output_tensor)\n",
    "    model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=lr),\n",
    "              loss = 'categorical_crossentropy', #Our loss function is based on categorical error\n",
    "              metrics=[keras.metrics.categorical_accuracy], #We will print out the categorical accuracy as the network is trained\n",
    "              )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66032afc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##intialize/setup the hyperband tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.tuners.RandomSearch(\n",
    "    combined, #model\n",
    "    objective = [kt.Objective('val_loss', 'min'), kt.Objective('loss', 'min'),\n",
    "                kt.Objective('val_categorical_accuracy', 'max')], #target\n",
    "    max_trials=1000, #number of iterations of tuning to run\n",
    "    max_consecutive_failed_trials=3, #number of allowed failed trials\n",
    "    directory = 'tuning',\n",
    "    project_name='joint5'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6ca8968",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d194d75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "##actual tuning process\n",
    "tuner.search(X_train, Y_train, validation_data=(X_val, Y_validation),\n",
    "             batch_size=batch_size, epochs=epochs, shuffle=shuffle,\n",
    "             class_weight = class_weight, callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f412d8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('n1')}, the second is {best_hps.get('n2')},\n",
    "regularizers are {best_hps.get('reg1')} and {best_hps.get('reg2')},\n",
    "and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8ad3cd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "best_hps.values"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b54dcd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(X_train,Y_train, validation_data=(X_val, Y_validation),\n",
    "                    batch_size=batch_size, epochs=epochs, shuffle=shuffle,class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8ed2127",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "train_loss=model.history.history['loss']\n",
    "val_loss=model.history.history['val_loss']\n",
    "    \n",
    "cat_acc=model.history.history['categorical_accuracy']\n",
    "val_acc=model.history.history['val_categorical_accuracy']\n",
    "        \n",
    "pred = model.predict(X_all)\n",
    "pred_val = model.predict(X_val)\n",
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "\n",
    "# Look at the optimization history\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(train_loss, label='Training loss')\n",
    "ax1.plot(val_loss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cat_acc, label='Training ACC')\n",
    "ax2.plot(val_acc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da976f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ba21c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
