{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3816c2b4",
   "metadata": {},
   "source": [
    "### FCNN Model Loop with Skill Score calculation\n",
    "\n",
    "File recreated on 6/26/2025 for the \"updated\" model. \n",
    "\n",
    "I will start with a 14 day model. \n",
    "\n",
    "I added in the LRP too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a2938d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 23:30:07.285762: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-26 23:30:07.287783: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-26 23:30:07.314269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-26 23:30:07.314290: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-26 23:30:07.314310: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-26 23:30:07.319576: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-26 23:30:07.319979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-26 23:30:08.465691: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2211] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "##import cell makes its appearance once again...\n",
    "%matplotlib inline\n",
    "##so-called \"math\" related imports\n",
    "#from netCDF4 import Dataset as ncread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import sample\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import xarray as xr\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "##plotting related imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow/keras related files\n",
    "import tensorflow as tf    \n",
    "#tf.compat.v1.disable_v2_behavior() # <-- HERE !\n",
    "\n",
    "tf.device('/physical_device:GPU:0')\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout, Activation, Reshape, Flatten, LSTM, Dense, Dropout, Embedding, Bidirectional, GRU, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "import shap\n",
    "import pylab\n",
    "#tf.compat.v1.disable_eager_execution()\n",
    "import investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26f4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from StatScores import BSS\n",
    "from StatScores import RAS\n",
    "from StatScores import PAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e132c8",
   "metadata": {},
   "source": [
    "### Inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e928099",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load in solvers for PC analysis to get the PCs for the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56c92020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U Wind shape:  (62, 149, 37, 36)\n",
      "EHF shape:  (62, 149, 37, 180)\n",
      "GPH shape:  (62, 149, 37, 180)\n"
     ]
    }
   ],
   "source": [
    "#load input data\n",
    "infile = open(\"../eof_data/vertanom_u.p\",\"rb\",)\n",
    "u_input = pickle.load(infile)  ##vertical U cross section\n",
    "#flat_u_14 = u_input[:62, 19:168, :, :].reshape((62 * 149, 37, 36))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../eof_data/vertanom_ehf.p\",\"rb\",)\n",
    "ehf_input = pickle.load(infile)  ##ZMehf vertical cross section along longitudes\n",
    "#flat_EHF_14 = ehf_input[:62, 19:168, :, :].reshape((62 * 149, 37, 180))\n",
    "infile.close()\n",
    "\n",
    "infile = open(\"../eof_data/vertanom_gph.p\",\"rb\",)\n",
    "gph_input = pickle.load(infile)  ##GPH vertical cross section along longitudes\n",
    "#flat_GPH_14= gph_input[:62, 19:168, :, :].reshape((62 * 149, 37, 180))\n",
    "infile.close()\n",
    "\n",
    "##19:168 for 14 days and \n",
    "##19:162 for 20 days and \n",
    "##19:152 for 30 days. \n",
    "\n",
    "u_14 = u_input[:62, 19:168, :, :]\n",
    "EHF_14 = ehf_input[:62, 19:168, :, :]\n",
    "GPH_14= gph_input[:62, 19:168, :, :]\n",
    "\n",
    "print(\"U Wind shape: \",u_14.shape)\n",
    "print(\"EHF shape: \",EHF_14.shape)\n",
    "print(\"GPH shape: \",GPH_14.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e268dcb6",
   "metadata": {},
   "source": [
    "### Outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "199f7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load output data\n",
    "# 149 for 14\n",
    "# 143 for 20 and \n",
    "# 133 for 30 \n",
    "\n",
    "infile = open(\"../reduced_data/temps/eur_14.p\",\"rb\",)\n",
    "output = pickle.load(infile) \n",
    "output = output.reshape(62, 149)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c979842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load climo data\n",
    "infile = open(\"../reduced_data/climo/eur_climo14.p\",\"rb\",)\n",
    "climo = pickle.load(infile) \n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "086c1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 149)\n",
      "climo:  (62, 149, 2)\n",
      "output:  (62, 149)\n"
     ]
    }
   ],
   "source": [
    "lead = 149\n",
    "\n",
    "temp = output[:,:]\n",
    "print(temp.shape)\n",
    "\n",
    "climo = climo.reshape(62, 149, 2)\n",
    "climo = climo[:, :, :]\n",
    "#climo = climo.reshape(8370,2)\n",
    "\n",
    "print(\"climo: \",climo.shape)\n",
    "print(\"output: \",temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75e46ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be commented out if not needed. \n",
    "input_u =  u_14\n",
    "input_ehf =  EHF_14\n",
    "input_gph =  GPH_14\n",
    "temp_n = temp\n",
    "new_climo = climo.reshape(62*149,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f69cc5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing and validation size:  1386\n",
      "training size:  7852\n"
     ]
    }
   ],
   "source": [
    "frac_ind = round((62*lead)*0.15)\n",
    "frac_end = round((62*lead)-frac_ind)\n",
    "\n",
    "print(\"testing and validation size: \",frac_ind)\n",
    "print(\"training size: \",frac_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0af74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Set X_all and Y_all datasets\n",
    "X_all1 = np.copy(input_u.reshape(62*lead,37*36)) ##u\n",
    "X_all2 = np.copy(input_ehf.reshape(62*lead,37*180)) ##ehf\n",
    "X_all3 = np.copy(input_gph.reshape(62*lead,37*180)) ##gph\n",
    "\n",
    "Y_all = np.copy(temp_n.reshape(62*lead))\n",
    "\n",
    "##training data partition out\n",
    "X_tri1 = X_all1[:frac_end,:] ##u\n",
    "X_tri2 = X_all2[:frac_end,:] ##ehf\n",
    "X_tri3 = X_all3[:frac_end,:] ##gph\n",
    "\n",
    "Y_tri = Y_all[:frac_end]\n",
    "\n",
    "#testing data partition out\n",
    "X_tes1 = X_all1[frac_end:,:]\n",
    "X_tes2 = X_all2[frac_end:,:]\n",
    "X_tes3 = X_all3[frac_end:,:]\n",
    "\n",
    "Y_tes = Y_all[frac_end:]\n",
    "\n",
    "#Convert the Y array into a categorical array. This means we will create one-hot vector labels for all of the inputs.\n",
    "# The one-hot vectors have an index for each possible output category (two in our case)\n",
    "# A \"1\" is put in the index corresponding to the category to which the sample belongs\n",
    "Y_all = keras.utils.to_categorical(Y_all)\n",
    "Y_tri = keras.utils.to_categorical(Y_tri)\n",
    "Y_tes= keras.utils.to_categorical(Y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9c0bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking my data for NaN of Infs because I need to make sure this doesn't cause\n",
    "#the model to throw back no loss\n",
    "\n",
    "if np.any(np.isnan(X_all1)) or np.any(np.isinf(X_all1)):\n",
    "    print(\"NaN or Inf values found in X_all!\")\n",
    "\n",
    "if np.any(np.isnan(Y_all)) or np.any(np.isinf(Y_all)):\n",
    "    print(\"NaN or Inf values found in Y_all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96473250",
   "metadata": {},
   "source": [
    "### Begin establishing specifics of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c440c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1332\n",
      "6660\n",
      "6660\n"
     ]
    }
   ],
   "source": [
    "##number of input nodes\n",
    "input_shape1=  X_all1.shape[-1] # u\n",
    "input_shape2= X_all2.shape[-1] ## ehf\n",
    "input_shape3= X_all3.shape[-1] ## gph\n",
    "\n",
    "print(input_shape1)\n",
    "print(input_shape2)\n",
    "print(input_shape3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d29bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "#CHANGE BASED ON NUMBER OF FOLDS FOR TIMESERIES SPLIT\n",
    "n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36bda5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##class weight creator for the instance where now I have a four dimensional output array \n",
    "def class_weight_creator(Y):\n",
    "    class_dict = {}\n",
    "    Y_reshaped = Y.reshape(-1, Y.shape[-1])\n",
    "    weights = np.max(np.sum(Y_reshaped, axis=0)) / np.sum(Y_reshaped, axis=0)\n",
    "    for i in range(Y.shape[-1] ):\n",
    "        class_dict[i] = weights[i]\n",
    "        \n",
    "    return class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e10b2a9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##definition statement for ACC\n",
    "def calculate_accuracy(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return np.mean(y_true == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81246283",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##Loss and ACC lists, only relevant w/o early stopping\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "cat_acc = []\n",
    "val_acc = [] \n",
    "\n",
    "##BSS Arrays, all of the skill scores have 200 rows\n",
    "#because that is how many cross-validations I will do for the model\n",
    "BSS_all= np.empty((n,))\n",
    "BSS_val= np.empty((n,))\n",
    "BSS_train= np.empty((n,))\n",
    "BSS_test= np.empty((n,))\n",
    "BSS_90= np.empty((n,))\n",
    "\n",
    "BSS_avg= np.empty((n,))\n",
    "BSS_30= np.empty((n,))\n",
    "BSS_70= np.empty((n,))\n",
    "\n",
    "##RAS and PAS Arrays\n",
    "Prec_all= np.empty((n,2))\n",
    "Rec_all= np.empty((n,2))\n",
    "\n",
    "Prec_val= np.empty((n,2))\n",
    "Rec_val= np.empty((n,2))\n",
    "\n",
    "Prec_train= np.empty((n,2))\n",
    "Rec_train= np.empty((n,2))\n",
    "\n",
    "Prec_test= np.empty((n,2))\n",
    "Rec_test= np.empty((n,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1441c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##empty lists for ACC scores \n",
    "##Testing Accuracy Arrays\n",
    "fulltest_acc = []\n",
    "test90_acc = []\n",
    "\n",
    "##Training Accuracy Arrays\n",
    "fulltrain_acc = []\n",
    "train90_acc = []\n",
    "\n",
    "##Validation Accuracy Arrays\n",
    "fullval_acc = []\n",
    "val90_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8788005c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##probabilities\n",
    "p_test = []\n",
    "p_train = []\n",
    "p_val = []\n",
    "\n",
    "p_mid = []\n",
    "p_90 = []\n",
    "p_30 = []\n",
    "p_70 = []\n",
    "\n",
    "##correct positive\n",
    "posXtest = []\n",
    "#false positive\n",
    "FposXtest = []\n",
    "#correct negative\n",
    "negXtest = []\n",
    "#false negative\n",
    "FnegXtest = []\n",
    "\n",
    "index90 = []\n",
    "indexmin = []\n",
    "indexmid = []\n",
    "indexmax = []\n",
    "\n",
    "##correct positive\n",
    "percpos = []\n",
    "#false positive\n",
    "percFpos = []\n",
    "#correct negative\n",
    "percneg = []\n",
    "#false negative\n",
    "percFneg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b28f73",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##LRP\n",
    "U_comp_pos  = np.zeros((n, 1332))\n",
    "U_comp_neg  = np.zeros((n, 1332))\n",
    "U_comp_Fpos = np.zeros((n, 1332))\n",
    "U_comp_Fneg = np.zeros((n, 1332))\n",
    "\n",
    "EHF_comp_pos  = np.zeros((n, 6660))\n",
    "EHF_comp_neg  = np.zeros((n, 6660))\n",
    "EHF_comp_Fpos = np.zeros((n, 6660))\n",
    "EHF_comp_Fneg = np.zeros((n, 6660))\n",
    "\n",
    "GPH_comp_pos  = np.zeros((n, 6660))\n",
    "GPH_comp_neg  = np.zeros((n, 6660))\n",
    "GPH_comp_Fpos = np.zeros((n, 6660))\n",
    "GPH_comp_Fneg = np.zeros((n, 6660))\n",
    "\n",
    "U_pos90_LRP   = np.zeros((n, 37, 36))\n",
    "EHF_pos90_LRP = np.zeros((n, 37, 180))\n",
    "GPH_pos90_LRP = np.zeros((n, 37, 180))\n",
    "\n",
    "U_Fpos90_LRP   = np.zeros((n, 37, 36))\n",
    "EHF_Fpos90_LRP = np.zeros((n, 37, 180))\n",
    "GPH_Fpos90_LRP = np.zeros((n, 37, 180))\n",
    "\n",
    "U_Fneg90_LRP   = np.zeros((n, 37, 36))\n",
    "EHF_Fneg90_LRP = np.zeros((n, 37, 180))\n",
    "GPH_Fneg90_LRP = np.zeros((n, 37, 180))\n",
    "\n",
    "U_neg90_LRP   = np.zeros((n, 37, 36))\n",
    "EHF_neg90_LRP = np.zeros((n, 37, 180))\n",
    "GPH_neg90_LRP = np.zeros((n, 37, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e79bd",
   "metadata": {},
   "source": [
    "### Model Architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d42c1454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_datasubset(data,start,end):\n",
    "    train1 = data[0:start]\n",
    "    train2 = data[end:]\n",
    "    train = np.concatenate((train1,train2))\n",
    "    return train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eccafd97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##giant LRP loop lol\n",
    "def run_lrp_analysis_3features(\n",
    "    model, X_test,\n",
    "    correct_pos, correct_neg, false_pos, false_neg,\n",
    "    X_tes1, X_tes2, X_tes3,  # Uwind, EHF, GPH (all flattened)\n",
    "    l,  #loop index\n",
    "\n",
    "    #arrays for composites (flattened)\n",
    "    U_comp_pos,   U_comp_neg,   U_comp_Fpos,   U_comp_Fneg,\n",
    "    EHF_comp_pos, EHF_comp_neg, EHF_comp_Fpos, EHF_comp_Fneg,\n",
    "    GPH_comp_pos, GPH_comp_neg, GPH_comp_Fpos, GPH_comp_Fneg,\n",
    "\n",
    "    #arrays for LRP heatmaps (reshaped)\n",
    "    U_pos90_LRP,   U_neg90_LRP,   U_Fpos90_LRP,   U_Fneg90_LRP,\n",
    "    EHF_pos90_LRP, EHF_neg90_LRP, EHF_Fpos90_LRP, EHF_Fneg90_LRP,\n",
    "    GPH_pos90_LRP, GPH_neg90_LRP, GPH_Fpos90_LRP, GPH_Fneg90_LRP\n",
    "):\n",
    "    def process_subset(analysis_data, mask, X_tensor, comp_arr, lrp_arr, shape_2d):\n",
    "        subset = np.array(analysis_data[mask])                      #shape: (samples, flattened)\n",
    "        comp_arr[l, :] = np.nanmean(X_tensor[mask], axis=0)         #save composite of inputs\n",
    "        reshaped = subset.reshape(subset.shape[0], *shape_2d)       #reshape to 2D map\n",
    "        scaled = reshaped / np.nanmax(reshaped, axis=(-2, -1), keepdims=True)  # normalize\n",
    "        lrp_arr[l, :, :] = np.nanmean(scaled, axis=0)               #average over samples\n",
    "\n",
    "    #analyzer\n",
    "    analyzer = innvestigate.create_analyzer(\"deep_taylor\", innvestigate.model_wo_softmax(model))\n",
    "    uwind_analysis, ehf_analysis, gph_analysis = analyzer.analyze(X_test)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Define reshapes for each feature\n",
    "    U_shape = (37, 36)   \n",
    "    EHF_shape = (37, 180)  \n",
    "    GPH_shape = (37, 180)  \n",
    "\n",
    "    #correct pos\n",
    "    process_subset(uwind_analysis, correct_pos, X_tes1, U_comp_pos,   U_pos90_LRP,   U_shape)\n",
    "    process_subset(ehf_analysis,   correct_pos, X_tes2, EHF_comp_pos, EHF_pos90_LRP, EHF_shape)\n",
    "    process_subset(gph_analysis,   correct_pos, X_tes3, GPH_comp_pos, GPH_pos90_LRP, GPH_shape)\n",
    "\n",
    "    #correct neg\n",
    "    process_subset(uwind_analysis, correct_neg, X_tes1, U_comp_neg,   U_neg90_LRP,   U_shape)\n",
    "    process_subset(ehf_analysis,   correct_neg, X_tes2, EHF_comp_neg, EHF_neg90_LRP, EHF_shape)\n",
    "    process_subset(gph_analysis,   correct_neg, X_tes3, GPH_comp_neg, GPH_neg90_LRP, GPH_shape)\n",
    "\n",
    "    #false pos\n",
    "    process_subset(uwind_analysis, false_pos, X_tes1, U_comp_Fpos,   U_Fpos90_LRP,   U_shape)\n",
    "    process_subset(ehf_analysis,   false_pos, X_tes2, EHF_comp_Fpos, EHF_Fpos90_LRP, EHF_shape)\n",
    "    process_subset(gph_analysis,   false_pos, X_tes3, GPH_comp_Fpos, GPH_Fpos90_LRP, GPH_shape)\n",
    "\n",
    "    #false neg\n",
    "    process_subset(uwind_analysis, false_neg, X_tes1, U_comp_Fneg,   U_Fneg90_LRP,   U_shape)\n",
    "    process_subset(ehf_analysis,   false_neg, X_tes2, EHF_comp_Fneg, EHF_Fneg90_LRP, EHF_shape)\n",
    "    process_subset(gph_analysis,   false_neg, X_tes3, GPH_comp_Fneg, GPH_Fneg90_LRP, GPH_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78e46f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "shuffle = True\n",
    "verbose = 0 #0 returns no apoch info, 2 does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4a5d91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combined(n1, n2, n3, n4, n5, n6, n7, reg1, reg2, reg3, reg4, reg5, reg6, reg7, learn):\n",
    "    ##first layers for U ANN\n",
    "    input_tensor1 = Input(shape=(int(1332),), name='layer1')\n",
    "    layer1 = layers.Dense(n1, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg1))(input_tensor1)\n",
    "   \n",
    "\n",
    "    layer1 = layers.Dense(n2, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg2))(layer1)\n",
    "   \n",
    "    ## second layers for EHF ANN\n",
    "    input_tensor2 = Input(shape=(int(6660),), name='layer2')\n",
    "    layer2 = layers.Dense(n3, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg3))(input_tensor2)\n",
    "    \n",
    "    layer2 = layers.Dense(n4, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg4))(layer2)\n",
    "    \n",
    "    \n",
    "    ## third layers for GPH ANN\n",
    "    input_tensor3 = Input(shape=(int(6660),), name='layer3')\n",
    "    layer3 = layers.Dense(n5, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg5))(input_tensor3)\n",
    "    \n",
    "    layer3 = layers.Dense(n6, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg6))(layer3)\n",
    "    \n",
    "    ##combine two model inputs\n",
    "    merge_layer = Concatenate()([layer1,layer2,layer3])\n",
    "\n",
    "    ##more dense layers\n",
    "    merge = layers.Dense(n7, activation='relu', use_bias=True,\n",
    "                          kernel_initializer='he_normal',bias_initializer='he_normal',\n",
    "                          kernel_regularizer=keras.regularizers.l2(l2= reg7))(merge_layer)\n",
    "\n",
    "    #decay_rate = learn / epochs\n",
    "    output_tensor = layers.Dense(2,activation='softmax')(merge)\n",
    "    model = Model([input_tensor1, input_tensor2, input_tensor3], output_tensor)\n",
    "    model.compile(optimizer=keras.optimizers.legacy.Adam(learning_rate=learn),\n",
    "              loss = 'categorical_crossentropy', #Our loss function is based on categorical error\n",
    "              metrics=[keras.metrics.categorical_accuracy], #We will print out the categorical accuracy as the network is trained\n",
    "              )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdbcbb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Early Stopping Condition, if desired\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',patience=4,restore_best_weights=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9edc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "CV  0\n"
     ]
    }
   ],
   "source": [
    "## create loop for cross validation\n",
    "anomalous=0\n",
    "for l in range(0,n):\n",
    "    print(\"#######################################\")\n",
    "    print(\"CV \",l)\n",
    "    tf.keras.backend.clear_session()\n",
    "    ##randomly choose a fraction of events for validation and training\n",
    "    start = random.randrange(len(X_tri1[:,0])-frac_ind)\n",
    "    end = start+(frac_ind)\n",
    "\n",
    "    #choose random 15% window of training data to be used for validation\n",
    "    X_val1 = X_tri1[start:end]\n",
    "    X_val2 = X_tri2[start:end]\n",
    "    X_val3 = X_tri3[start:end]\n",
    "    \n",
    "    Y_val = Y_tri[start:end]\n",
    "\n",
    "    #remaining training data\n",
    "    X_tr1 = train_datasubset(X_tri1,start,end)\n",
    "    X_tr2 = train_datasubset(X_tri2,start,end)\n",
    "    X_tr3 = train_datasubset(X_tri3,start,end)\n",
    "    \n",
    "    Y_tr = train_datasubset(Y_tri,start,end)\n",
    "    \n",
    "    ##dictionaries for training and validation\n",
    "    X_tr = {'layer1': X_tr1, 'layer2': X_tr2, 'layer3': X_tr3}\n",
    "    X_all = {'layer1': X_all1, 'layer2': X_all2, 'layer3': X_all3}\n",
    "    X_tes = {'layer1': X_tes1, 'layer2': X_tes2, 'layer3': X_tes3}\n",
    "    X_val = {'layer1': X_val1, 'layer2': X_val2, 'layer3': X_val3}\n",
    "    \n",
    "    ##calculate class weights on remaining training data\n",
    "    class_weight = class_weight_creator(Y_tr)\n",
    "\n",
    "    ##same partitioning for climo data\n",
    "    climo_train1 = new_climo[0:start,:]\n",
    "    climo_train2 = new_climo[end:frac_end,:]\n",
    "    \n",
    "    climo_train = np.concatenate((climo_train1,climo_train2))\n",
    "    \n",
    "    ##pause, calculate climo probabilities\n",
    "    climo_val = new_climo[start:end,:]\n",
    "    climo_test = new_climo[frac_end:,:]\n",
    "    \n",
    "    #initalize model\n",
    "    model = combined(n1 = 124,\n",
    "                 n2 = 24,\n",
    "                 n3 = 128,\n",
    "                 n4 = 8,\n",
    "                 n5 = 128, \n",
    "                 n6 = 8,\n",
    "                 n7 = 20,\n",
    "                 reg1 = 0.0896,\n",
    "                 reg2 = 0.0083, \n",
    "                 reg3 = 0.0342,\n",
    "                 reg4 = 0.0655,\n",
    "                 reg5 = 0.0344,\n",
    "                 reg6 = 0.0145,\n",
    "                 reg7 = 0.0169,\n",
    "                 learn = 0.0175)\n",
    "\n",
    "    model.fit(X_tr, Y_tr, validation_data=(X_val, Y_val), verbose=verbose,\n",
    "          batch_size=batch_size, epochs=epochs, shuffle=shuffle,class_weight = class_weight, callbacks=[early_stopping])\n",
    "    \n",
    "    train_loss.append(model.history.history['loss'])\n",
    "    val_loss.append(model.history.history['val_loss'])\n",
    "    \n",
    "    cat_acc.append(model.history.history['categorical_accuracy'])\n",
    "    val_acc.append(model.history.history['val_categorical_accuracy'])\n",
    "    \n",
    "    pred = model.predict(X_all)\n",
    "    pred_val = model.predict(X_val)\n",
    "    pred_train = model.predict(X_tr)\n",
    "    pred_test = model.predict(X_tes)\n",
    "    \n",
    "    valid_rows = np.all((pred_test == [1, 0]) | (pred_test == [0, 1]), axis=1)\n",
    "    if np.all(valid_rows):\n",
    "        print(f\"Skipping fold {l} due to degenerate predictions: only [1, 0] or [0, 1]\")\n",
    "        anomalous += 1\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        p_train.extend(pred_train)\n",
    "        p_val.extend(pred_val)\n",
    "        p_test.extend(pred_test)\n",
    "        ########################################################################################################\n",
    "        ##STATISTICS CALCULATINNG\n",
    "        pred_class = []\n",
    "        predval_class = []\n",
    "        predtr_class = []\n",
    "        predtest_class = []\n",
    "\n",
    "        ##BRIER SKILL SCORE\n",
    "        BSS_all[l] = BSS(Y_all,pred)\n",
    "        BSS_val[l] = BSS(Y_val,pred_val)\n",
    "        BSS_train[l] = BSS(Y_tr,pred_train)\n",
    "        BSS_test[l] = BSS(Y_tes,pred_test) \n",
    "\n",
    "        ##RECALL ACCURACY SCORE    \n",
    "        RAS(l, Rec_all, new_climo, Y_all, pred,\n",
    "            climo_val, Rec_val, Y_val, pred_val,\n",
    "            climo_train, Rec_train, Y_tr, pred_train,\n",
    "            climo_test, Rec_test, Y_tes, pred_test)\n",
    "        ##PRECISION ACCURACY SCORE     \n",
    "        PAS(l, Prec_all, new_climo, Y_all, pred,\n",
    "            climo_val, Prec_val, Y_val, pred_val,\n",
    "            climo_train, Prec_train, Y_tr, pred_train,\n",
    "            climo_test, Prec_test, Y_tes, pred_test)\n",
    "        ########################################################################################################\n",
    "        ##ACC\n",
    "        ##TESTING\n",
    "        q90 = np.percentile(pred_test,90,axis=0) ##90th percentile of test\n",
    "        ##90th percentile acc\n",
    "        great90 = [i for i, row in enumerate(pred_test) if (row[0] > q90[0]) or (row[1] > q90[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test90 = pred_test[great90]\n",
    "        test90_norm = Y_tes[great90]\n",
    "        test90_acc.append(calculate_accuracy(test90_norm, test90, threshold=0.5))\n",
    "        ##full model\n",
    "        fulltest_acc.append(calculate_accuracy(Y_tes, pred_test, threshold=0.5))\n",
    "        ##Greater than 90 BSS\n",
    "        climo_90 = climo_test[great90]\n",
    "\n",
    "        BSS_90[l] = BSS(test90_norm,test90)\n",
    "        p_90.extend(test90)\n",
    "\n",
    "        ##classify the accuracy of predicitons\n",
    "        correct_pos = [] #correct positive anomaly\n",
    "        correct_neg = [] #correct negative anomaly\n",
    "\n",
    "        false_pos = [] #false positive\n",
    "        false_neg = [] #false negative\n",
    "\n",
    "        for j in range(len(great90)):\n",
    "            #print(j)\n",
    "            #print(great90[j])\n",
    "            index = great90[j]\n",
    "            if pred_test[index,0] < pred_test[index,1] and Y_tes[index,0] == 0:\n",
    "                correct_pos.append(index)\n",
    "                #print('###########')\n",
    "            elif pred_test[index,0] > pred_test[index,1] and Y_tes[index,0] == 1:\n",
    "                correct_neg.append(index)\n",
    "                #print('###########')\n",
    "            elif pred_test[index,0] < pred_test[index,1] and Y_tes[index,0] == 1:\n",
    "                false_neg.append(index)\n",
    "                #print('###########')\n",
    "            elif pred_test[index,0] > pred_test[index,1] and Y_tes[index,0] == 0:\n",
    "                false_pos.append(index)\n",
    "                #print('###########')\n",
    "\n",
    "        ##correct positive\n",
    "        posXtest.extend(correct_pos)\n",
    "        #false positive\n",
    "        FposXtest.extend(false_pos)\n",
    "        #correct negative\n",
    "        negXtest.extend(correct_neg)\n",
    "        #false negative\n",
    "        FnegXtest.extend(false_neg)\n",
    "\n",
    "        percpos.append(len(correct_pos)/len(great90))\n",
    "        #false positive\n",
    "        percFpos.append(len(false_pos)/len(great90))\n",
    "        #correct negative\n",
    "        percneg.append(len(correct_neg)/len(great90))\n",
    "        #false negative\n",
    "        percFneg.append(len(false_neg)/len(great90))\n",
    "        \n",
    "        ############################\n",
    "        #LRP Call\n",
    "        run_lrp_analysis_3features(model, X_tes,\n",
    "                                   correct_pos, correct_neg, false_pos, false_neg,\n",
    "                                   X_tes1, X_tes2, X_tes3,\n",
    "                                   l,\n",
    "                                   U_comp_pos, U_comp_neg, U_comp_Fpos, U_comp_Fneg,\n",
    "                                   EHF_comp_pos, EHF_comp_neg, EHF_comp_Fpos, EHF_comp_Fneg,\n",
    "                                   GPH_comp_pos, GPH_comp_neg, GPH_comp_Fpos, GPH_comp_Fneg,\n",
    "                                   U_pos90_LRP, U_neg90_LRP, U_Fpos90_LRP, U_Fneg90_LRP,\n",
    "                                   EHF_pos90_LRP, EHF_neg90_LRP, EHF_Fpos90_LRP, EHF_Fneg90_LRP,\n",
    "                                   GPH_pos90_LRP, GPH_neg90_LRP, GPH_Fpos90_LRP, GPH_Fneg90_LRP)\n",
    "\n",
    "        ##############################\n",
    "\n",
    "        #AVERAGE QUANT\n",
    "        qmin = np.min(pred_test,axis=0)\n",
    "        q30 = np.percentile(pred_test,30,axis=0) ##30th percentile of test\n",
    "        q70 = np.percentile(pred_test,70,axis=0) ##70th percentile of test\n",
    "        qmax = np.max(pred_test,axis=0)\n",
    "\n",
    "        ##mid percentile acc\n",
    "        mid = [i for i, row in enumerate(pred_test) if (row[0] >= q30[0] and row[0] <= q70[0] ) or (row[1] >= q30[1] and row[1] <= q70[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_mid = pred_test[mid]\n",
    "        test_mid_norm = Y_tes[mid]\n",
    "\n",
    "        BSS_avg[l] = BSS(test_mid_norm,test_mid)\n",
    "        p_mid.extend(test_mid)\n",
    "\n",
    "        #0-30\n",
    "        minimum = [i for i, row in enumerate(pred_test) if (row[0] >= qmin[0] and row[0] < q30[0] ) or (row[1] >= qmin[1] and row[1] < q30[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_minimum = pred_test[minimum]\n",
    "        test_minimum_norm = Y_tes[minimum]\n",
    "\n",
    "        BSS_30[l] = BSS(test_minimum_norm,test_minimum)\n",
    "        p_30.extend(test_minimum)\n",
    "\n",
    "        #70-100\n",
    "        maximum = [i for i, row in enumerate(pred_test) if (row[0] > q70[0] and row[0] <= qmax[0] ) or (row[1] > q70[1] and row[1] <= qmax[1])]\n",
    "        # Create the arrays of probabilities and actual values that exceed the 90th percentile\n",
    "        test_maximum = pred_test[maximum]\n",
    "        test_maximum_norm = Y_tes[maximum]\n",
    "\n",
    "        BSS_70[l] = BSS(test_maximum_norm,test_maximum)\n",
    "        p_70.extend(test_maximum)\n",
    "\n",
    "        ####_______SAVE INDICES _______\n",
    "        index90.extend(great90)\n",
    "        indexmin.extend(minimum)\n",
    "        indexmid.extend(mid)\n",
    "        indexmax.extend(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##correct positive\n",
    "posXtest = np.array(posXtest)\n",
    "#false positive\n",
    "FposXtest = np.array(FposXtest)\n",
    "#correct negative\n",
    "negXtest = np.array(negXtest)\n",
    "#false negative\n",
    "FnegXtest = np.array(FnegXtest)\n",
    "\n",
    "print(f'Average Num. of 10% Confident and Correct Postive Predictions: {np.mean(percpos)* 100:.2f}%')\n",
    "print(f'Average Num. of 10% Confident and Correct Negative Predictions: {np.mean(percneg)* 100:.2f}%')\n",
    "print(f'Average Num. of 10% Confident and FALSE Postive Predictions: {np.mean(percFpos)* 100:.2f}%')\n",
    "print(f'Average Num. of 10% Confident and FALSE Negative Predictions: {np.mean(percFneg)* 100:.2f}%')\n",
    "print('#######################################################################')\n",
    "print(f'Average Num. of 10% Confident and Correct Predictions: {np.mean(percpos)* 100 + np.mean(percneg)* 100:.2f}%')\n",
    "print(f'Average Num. of 10% Confident and FALSE Predictions: {np.mean(percFpos)* 100 +np.mean(percFneg)* 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036963f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num of anomalous models: {anomalous}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3839c68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(-1, 1,13)\n",
    "counts, edges, = np.histogram(BSS_90[:], bins=bins)\n",
    "\n",
    "fig, (ax1) = plt.subplots(1,1, figsize=(9,7))\n",
    "\n",
    "ax1.bar(edges[:-1], counts, width=np.diff(edges), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS of 90th Percentile',fontsize =13)\n",
    "ax1.set_ylabel('Number of Models',fontsize =13)\n",
    "plt.tick_params(labelsize = 12)\n",
    "ax1.set_title('BSS for 90th Percentile Predictions of Testing Data in 100 CVs',fontsize =15)\n",
    "plt.savefig(\"BSS90_eur14_FCNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe50ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(-1, 1,13)\n",
    "fs = 11\n",
    "\n",
    "counts1, edges1, = np.histogram(BSS_30[:], bins=bins)\n",
    "counts2, edges2, = np.histogram(BSS_avg[:], bins=bins)\n",
    "counts3, edges3, = np.histogram(BSS_70[:], bins=bins)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "ax1.bar(edges1[:-1], counts1, width=np.diff(edges1), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS 0-30th Percentiles', fontsize=fs+1)\n",
    "ax1.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges2[:-1], counts2, width=np.diff(edges2), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax2.set_xlabel('BSS Middle Percentiles', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax3.bar(edges3[:-1], counts3, width=np.diff(edges3), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax3.set_xlabel('BSS 70-100th Percentiles', fontsize=fs+1)\n",
    "#ax3.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "plt.suptitle('BSS of Predictions in Various Percentiles of Testing Data in 100 CVs for Predicting +14 day European Temp Anomalies', fontsize=fs+3, x=0.525,y=0.95)\n",
    "plt.savefig(\"BSSpercentile_eur14_FCNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdcff4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(-1, 1,13)\n",
    "fs = 11\n",
    "\n",
    "counts1, edges1, = np.histogram(BSS_train[:], bins=bins)\n",
    "counts2, edges2, = np.histogram(BSS_val[:], bins=bins)\n",
    "counts3, edges3, = np.histogram(BSS_test[:], bins=bins)\n",
    "\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(14,4))\n",
    "\n",
    "ax1.bar(edges1[:-1], counts1, width=np.diff(edges1), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax1.set_xlabel('BSS Training Data', fontsize=fs+1)\n",
    "ax1.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges2[:-1], counts2, width=np.diff(edges2), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax2.set_xlabel('BSS Validation Data', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "ax3.bar(edges3[:-1], counts3, width=np.diff(edges3), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='lightblue')\n",
    "ax3.set_xlabel('BSS Testing Data', fontsize=fs+1)\n",
    "#ax3.set_ylabel('Number of Models', fontsize=fs+1)\n",
    "\n",
    "plt.suptitle('BSS of Predictions Across All Datasets in 100 CVs for Predicting +14 day European Temp Anomalies', fontsize=fs+3, x=0.525,y=0.95)\n",
    "plt.savefig(\"BSSdatasets_eur14_FCNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f996690",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p_test = np.array(p_test)\n",
    "p_test = p_test.reshape(((len(p_test),2)))\n",
    "\n",
    "p_train = np.array(p_train)\n",
    "p_train = p_train.reshape(((len(p_train),2)))\n",
    "\n",
    "p_val= np.array(p_val)\n",
    "p_val = p_val.reshape(((len(p_val),2)))\n",
    "\n",
    "p_mid= np.array(p_mid)\n",
    "p_mid = p_mid.reshape(((len(p_mid),2)))\n",
    "\n",
    "p_30= np.array(p_30)\n",
    "p_30 = p_30.reshape(((len(p_30),2)))\n",
    "\n",
    "p_70= np.array(p_70)\n",
    "p_70 = p_70.reshape(((len(p_70),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226cf4a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 37)  #10 bins from 0.4 to 1\n",
    "fs = 11\n",
    "##bin the probabilities\n",
    "counts_neg_train, edges_neg_train = np.histogram(p_train[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(p_train[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(p_val[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(p_val[:,1], bins=bins)\n",
    "\n",
    "counts_neg_test, edges_neg_test = np.histogram(p_test[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(p_test[:,1], bins=bins)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(11,11))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative', fontsize=fs+2)\n",
    "ax1.set_xlabel('Probability in Training Data', fontsize=fs+1)\n",
    "ax1.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive', fontsize=fs+2)\n",
    "ax2.set_xlabel('Probability in Training Data', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax3.set_title('Negative Validation Predictions', fontsize=fs)\n",
    "ax3.set_xlabel('Probability in Validation Data', fontsize=fs+1)\n",
    "ax3.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax4.set_title('Positive Validation Predictions', fontsize=fs)\n",
    "ax4.set_xlabel('Probability in Validation Data', fontsize=fs+1)\n",
    "#ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax5.set_title('Negative Testing Predictions', fontsize=fs)\n",
    "ax5.set_xlabel('Probability in Testing Data', fontsize=fs+1)\n",
    "ax5.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax6.set_title('Positive Testing Predictions', fontsize=fs)\n",
    "ax6.set_xlabel('Probability in Testing Data', fontsize=fs+1)\n",
    "#ax6.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle(\"Probability Distributions of Various Datasets Across 100 CVs for Predicting +14 day European Temp Anomalies\", fontsize=fs+4, x=0.525,y=0.99)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"probability_distrib_eur14_FCNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2480d33",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 1, 37)  #10 bins from 0.4 to 1\n",
    "fs = 11\n",
    "##bin the probabilities\n",
    "counts_neg_train, edges_neg_train = np.histogram(p_30[:,0], bins=bins)\n",
    "counts_pos_train, edges_pos_train = np.histogram(p_30[:,1], bins=bins)\n",
    "\n",
    "counts_neg_val, edges_neg_val = np.histogram(p_mid[:,0], bins=bins)\n",
    "counts_pos_val, edges_pos_val = np.histogram(p_mid[:,1], bins=bins)\n",
    "\n",
    "counts_neg_test, edges_neg_test = np.histogram(p_70[:,0], bins=bins)\n",
    "counts_pos_test, edges_pos_test = np.histogram(p_70[:,1], bins=bins)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(11,11))\n",
    "ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()\n",
    "\n",
    "ax1.bar(edges_neg_train[:-1], counts_neg_train, width=np.diff(edges_neg_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "ax1.set_title('Negative', fontsize=fs+2)\n",
    "ax1.set_xlabel('Probability in 0-30th Percentiles', fontsize=fs+1)\n",
    "ax1.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax2.bar(edges_pos_train[:-1], counts_pos_train, width=np.diff(edges_pos_train), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "ax2.set_title('Positive', fontsize=fs+2)\n",
    "ax2.set_xlabel('Probability in 0-30th Percentiles', fontsize=fs+1)\n",
    "#ax2.set_ylabel('Count')\n",
    "\n",
    "ax3.bar(edges_neg_val[:-1], counts_neg_val, width=np.diff(edges_neg_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax3.set_title('Negative Validation Predictions', fontsize=fs)\n",
    "ax3.set_xlabel('Probability in Middle Percentiles', fontsize=fs+1)\n",
    "ax3.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax4.bar(edges_pos_val[:-1], counts_pos_val, width=np.diff(edges_pos_val), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax4.set_title('Positive Validation Predictions', fontsize=fs)\n",
    "ax4.set_xlabel('Probability in Middle Percentiles', fontsize=fs+1)\n",
    "#ax4.set_ylabel('Count')\n",
    "\n",
    "ax5.bar(edges_neg_test[:-1], counts_neg_test, width=np.diff(edges_neg_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='blue')\n",
    "#ax5.set_title('Negative Testing Predictions', fontsize=fs)\n",
    "ax5.set_xlabel('Probability in 70-100th Percentiles', fontsize=fs+1)\n",
    "ax5.set_ylabel('Count', fontsize=fs+1)\n",
    "\n",
    "ax6.bar(edges_pos_test[:-1], counts_pos_test, width=np.diff(edges_pos_test), \n",
    "        edgecolor='black', alpha=0.5, align='edge', color='orange')\n",
    "#ax6.set_title('Positive Testing Predictions', fontsize=fs)\n",
    "ax6.set_xlabel('Probability in 70-100th Percentiles', fontsize=fs+1)\n",
    "#ax6.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle(\"Probability Distributions for Percentiles of Testing Data Across 100 CVs, +14 day European Temp Anomalies\", fontsize=fs+4, x=0.525,y=0.99)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"percentile_distrib_eur14_FCNN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aaf4d8",
   "metadata": {},
   "source": [
    "### Model Curves. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "94599168",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "tloss = np.mean(train_loss, axis=0)\n",
    "vloss = np.mean(val_loss, axis=0)\n",
    "    \n",
    "cacc= np.mean(cat_acc, axis=0)\n",
    "vacc= np.mean(val_acc, axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58bf7588",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=plt.figaspect(0.25))\n",
    "ax1.plot(tloss, label='Training loss')\n",
    "ax1.plot(vloss, label='Validation loss')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title('loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(cacc, label='Training ACC')\n",
    "ax2.plot(vacc, label='Validation ACC')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Acc')\n",
    "ax2.legend\n",
    "\n",
    "#plt.savefig(\"EUR_ACC_14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b82423",
   "metadata": {},
   "source": [
    "### Skill Score Distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6235c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "font = 12\n",
    "#loop through each member\n",
    "ax2 = plt.subplot(2,1,1)\n",
    "ax3 = plt.subplot(2,1,2)\n",
    "\n",
    "ticks = ['Full', 'Train', 'Val', 'Test'] #set tick numbers for dataset\n",
    "colors = ['lightcyan','peachpuff']\n",
    "ind = [2, 4, 6, 8]  # the x locations for the groups\n",
    "w = 0.25 #box-plot width\n",
    "labels = ['- Anom', '+ Anom '] #labels of quantiles\n",
    "\n",
    "\n",
    "##begin to go plot by plot ...\n",
    "#each plot has a separate plot function for each lead time. In these, the plots get each quantile plotted. \n",
    "ax2.set_title(\"Stat Scores for Predicting Europe Temp Anoms at 14-days Lead with Top 20 PCs\",fontsize = 14) \n",
    "\n",
    "##repeat the process\n",
    "a2_0 = ax2.boxplot([Rec_all[:,0],Rec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a2_5 = ax2.boxplot([Rec_train[:,0],Rec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a2_10 = ax2.boxplot([Rec_val[:,0],Rec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a2_14 = ax2.boxplot([Rec_test[:,0],Rec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax2.axhline(0.5, c='k', ls ='-.')\n",
    "ax2.set_xticks(ind, ticks)\n",
    "for bplot in (a2_0, a2_5, a2_10, a2_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a2_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax2.set_title('RAS for Temperature Region',fontsize = 14)\n",
    "ax2.set_ylim(-0.1,1.1)\n",
    "ax2.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax2.set_ylabel('RAS',fontsize = font)\n",
    "#ax2.set_xlabel('Data Set',fontsize = 14)\n",
    "#ax2.set_ylabel('Brier Skill Score')\n",
    "ax2.legend(loc = 'lower right', fontsize = 11)\n",
    "ax2.set_aspect('auto') ;\n",
    "\n",
    "a3_0 = ax3.boxplot([Prec_all[:,0],Prec_all[:,1]], positions= [1.8,2.2], widths=w, patch_artist=True)\n",
    "a3_5 = ax3.boxplot([Prec_train[:,0],Prec_train[:,1]], positions=[3.8,4.2], widths=w, patch_artist=True)\n",
    "a3_10 = ax3.boxplot([Prec_val[:,0],Prec_val[:,1]], positions=[5.8,6.2], widths=w, patch_artist=True)\n",
    "a3_14 = ax3.boxplot([Prec_test[:,0],Prec_test[:,1]], positions=[7.8,8.2], widths=w, patch_artist=True)\n",
    "ax3.axhline(0.5, c='k', ls ='-.')\n",
    "ax3.set_xticks(ind, ticks)\n",
    "for bplot in (a3_0, a3_5, a3_10, a3_14):\n",
    "    for patch, color in zip(bplot['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "for patch, label in zip(a3_0['boxes'], labels):\n",
    "    patch.set_label(label)\n",
    "#ax3.set_title('RAS Well')\n",
    "#ax3.set_title('PAS for Temperature Region',fontsize = 14)\n",
    "ax3.set_ylim(-0.1,1.1)\n",
    "ax3.set_ylabel('PAS',fontsize = font)\n",
    "ax3.set_xlabel('Data Set',fontsize = font)\n",
    "#ax3.set_ylabel('Recall Accuracy Score',fontsize = 14)\n",
    "ax3.tick_params(axis='both', which='major', labelsize= font)\n",
    "ax3.legend(loc = 'lower right', fontsize = 11)\n",
    "ax3.set_aspect('auto') ;\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig(\"EUR_StatScore_14_FCNN.png\", bbox_inches='tight',dpi =150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe2f1e",
   "metadata": {},
   "source": [
    "### Percentile ACC Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e565d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "bins = np.linspace(0.4, 1,13)  # 10 bins from 0.4 to 1\n",
    "fig = plt.figure(figsize=(9,7))\n",
    "##bin the testing 90th percentile accuracy data\n",
    "counts90, edges90 = np.histogram(test90_acc, bins=bins)\n",
    "countsfull, edgesfull = np.histogram(fulltest_acc, bins=bins)\n",
    "# Plot the full dataset\n",
    "#offset = 0.02  # Adjust this value if needed for better visibility\n",
    "plt.bar(edgesfull[:-1], countsfull, width=np.diff(edgesfull), \n",
    "        edgecolor='black', alpha=0.5, label='All Predictions', align='edge', color='blue')\n",
    "# Plot the 90th percentile\n",
    "plt.bar(edges90[:-1], counts90, width=np.diff(edges90), \n",
    "        edgecolor='black', alpha=0.5, label='>90th Percentile Predictions', align='edge', color = 'lightblue')\n",
    "plt.xlabel('Accuracy', fontsize =13)\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.ylabel('Number of Models', fontsize =13)\n",
    "plt.legend(fontsize = 11)\n",
    "plt.title('Testing Prediction Accuracy Across 100 Models for +14 day Europe Temp Anomalies', fontsize =15)\n",
    "plt.savefig(\"EUR14_ACCtesting_FCNN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab63729",
   "metadata": {},
   "source": [
    "### Pickle out LRP stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eeddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##pickle out the stuff for making heat maps because I want to run it on a smaller code\n",
    "##true values\n",
    "pickle.dump(U_neg90_LRP, open(\"./LRP_stuff/U_neg90_LRP.p\", 'wb'))\n",
    "pickle.dump(U_pos90_LRP, open(\"./LRP_stuff/U_pos90_LRP.p\",'wb'))\n",
    "pickle.dump(EHF_neg90_LRP, open(\"./LRP_stuff/EHF_neg90_LRP.p\", 'wb'))\n",
    "pickle.dump(EHF_pos90_LRP, open(\"./LRP_stuff/EHF_pos90_LRP.p\",'wb'))\n",
    "pickle.dump(GPH_neg90_LRP, open(\"./LRP_stuff/GPH_neg90_LRP.p\", 'wb'))\n",
    "pickle.dump(GPH_pos90_LRP, open(\"./LRP_stuff/GPH_pos90_LRP.p\",'wb'))\n",
    "\n",
    "##false values\n",
    "pickle.dump(U_Fneg90_LRP, open(\"./LRP_stuff/U_Fneg90_LRP.p\",'wb'))\n",
    "pickle.dump(U_Fpos90_LRP, open(\"./LRP_stuff/U_Fpos90_LRP.p\",'wb'))\n",
    "pickle.dump(EHF_Fneg90_LRP, open(\"./LRP_stuff/EHF_Fneg90_LRP.p\",'wb'))\n",
    "pickle.dump(EHF_Fpos90_LRP, open(\"./LRP_stuff/EHF_Fpos90_LRP.p\",'wb'))\n",
    "pickle.dump(GPH_Fneg90_LRP, open(\"./LRP_stuff/GPH_Fneg90_LRP.p\",'wb'))\n",
    "pickle.dump(GPH_Fpos90_LRP, open(\"./LRP_stuff/GPH_Fpos90_LRP.p\",'wb'))\n",
    "\n",
    "##for composites\n",
    "pickle.dump(U_comp_pos, open(\"./LRP_stuff/U_comp_pos.p\",'wb'))\n",
    "pickle.dump(U_comp_Fpos, open(\"./LRP_stuff/U_compFpos.p\",'wb'))\n",
    "pickle.dump(U_comp_neg, open(\"./LRP_stuff/U_comp_neg.p\",'wb'))\n",
    "pickle.dump(U_comp_Fneg, open(\"./LRP_stuff/U_compFneg.p\",'wb'))\n",
    "\n",
    "pickle.dump(EHF_comp_pos, open(\"./LRP_stuff/EHF_comp_pos.p\",'wb'))\n",
    "pickle.dump(EHF_comp_Fpos, open(\"./LRP_stuff/EHF_compFpos.p\",'wb'))\n",
    "pickle.dump(EHF_comp_neg, open(\"./LRP_stuff/EHF_comp_neg.p\",'wb'))\n",
    "pickle.dump(EHF_comp_Fneg, open(\"./LRP_stuff/EHF_compFneg.p\",'wb'))\n",
    "\n",
    "pickle.dump(GPH_comp_pos, open(\"./LRP_stuff/GPH_comp_pos.p\",'wb'))\n",
    "pickle.dump(GPH_comp_Fpos, open(\"./LRP_stuff/GPH_compFpos.p\",'wb'))\n",
    "pickle.dump(GPH_comp_neg, open(\"./LRP_stuff/GPH_comp_neg.p\",'wb'))\n",
    "pickle.dump(GPH_comp_Fneg, open(\"./LRP_stuff/GPH_compFneg.p\",'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
